"""
å…è²» LLM æ¨¡å‹ç¤ºç¯„æ‡‰ç”¨
å±•ç¤ºå¦‚ä½•ä½¿ç”¨ Ollama å’Œ Hugging Face çš„å…è²»æ¨¡å‹
"""

import streamlit as st
import os
from typing import Optional, List
import time

# å‹•æ…‹å°å…¥ï¼Œé¿å…åœ¨æ²’æœ‰å®‰è£ç›¸ä¾å¥—ä»¶æ™‚å‡ºéŒ¯
try:
    from langchain_ollama import OllamaLLM
    OLLAMA_AVAILABLE = True
except ImportError:
    OLLAMA_AVAILABLE = False

try:
    from langchain_huggingface import HuggingFaceEndpoint
    HF_AVAILABLE = True
except ImportError:
    HF_AVAILABLE = False

try:
    from langchain_core.prompts import PromptTemplate
    from langchain_core.output_parsers import StrOutputParser
    LANGCHAIN_AVAILABLE = True
except ImportError:
    LANGCHAIN_AVAILABLE = False

def check_dependencies():
    """æª¢æŸ¥å¿…è¦çš„ä¾è³´æ˜¯å¦å·²å®‰è£"""
    missing_deps = []
    
    if not OLLAMA_AVAILABLE:
        missing_deps.append("langchain-ollama")
    if not HF_AVAILABLE:
        missing_deps.append("langchain-huggingface")
    if not LANGCHAIN_AVAILABLE:
        missing_deps.append("langchain-core")
    
    return missing_deps

def show_installation_guide(missing_deps: List[str]):
    """é¡¯ç¤ºå®‰è£æŒ‡å—"""
    st.error("âš ï¸ ç¼ºå°‘å¿…è¦çš„ä¾è³´å¥—ä»¶")
    
    with st.expander("ğŸ“¦ å®‰è£æŒ‡å—"):
        st.write("è«‹åœ¨çµ‚ç«¯æ©Ÿä¸­åŸ·è¡Œä»¥ä¸‹å‘½ä»¤å®‰è£ç¼ºå°‘çš„å¥—ä»¶ï¼š")
        
        install_cmd = "pip install " + " ".join(missing_deps)
        st.code(install_cmd, language="bash")
        
        st.write("æˆ–è€…å®‰è£å®Œæ•´çš„ä¾è³´ï¼š")
        st.code("pip install langchain-ollama langchain-huggingface langchain-core", language="bash")

def test_ollama_connection() -> bool:
    """æ¸¬è©¦ Ollama é€£æ¥"""
    try:
        import requests
        response = requests.get("http://localhost:11434/api/tags", timeout=5)
        return response.status_code == 200
    except:
        return False

def get_available_ollama_models() -> List[str]:
    """ç²å–å¯ç”¨çš„ Ollama æ¨¡å‹"""
    try:
        import requests
        response = requests.get("http://localhost:11434/api/tags", timeout=5)
        if response.status_code == 200:
            data = response.json()
            return [model["name"] for model in data.get("models", [])]
    except:
        pass
    return []

def create_ollama_llm(model_name: str) -> Optional[OllamaLLM]:
    """å‰µå»º Ollama LLM å¯¦ä¾‹"""
    try:
        return OllamaLLM(model=model_name, temperature=0.7)
    except Exception as e:
        st.error(f"å‰µå»º Ollama æ¨¡å‹æ™‚ç™¼ç”ŸéŒ¯èª¤: {str(e)}")
        return None

def create_hf_llm(model_name: str, api_token: str = None) -> Optional[HuggingFaceEndpoint]:
    """å‰µå»º Hugging Face LLM å¯¦ä¾‹"""
    try:
        kwargs = {
            "repo_id": model_name,
            "model_kwargs": {"temperature": 0.7, "max_length": 200}
        }
        
        if api_token:
            kwargs["huggingfacehub_api_token"] = api_token
            
        return HuggingFaceEndpoint(**kwargs)
    except Exception as e:
        st.error(f"å‰µå»º Hugging Face æ¨¡å‹æ™‚ç™¼ç”ŸéŒ¯èª¤: {str(e)}")
        return None

def main():
    st.set_page_config(
        page_title="å…è²» LLM æ¨¡å‹ç¤ºç¯„",
        page_icon="ğŸ¤–",
        layout="wide"
    )
    
    st.title("ğŸ¤– å…è²» LLM æ¨¡å‹ç¤ºç¯„")
    st.write("é«”é©—ä¸åŒçš„å…è²»é–‹æºèªè¨€æ¨¡å‹")
    
    # æª¢æŸ¥ä¾è³´
    missing_deps = check_dependencies()
    if missing_deps:
        show_installation_guide(missing_deps)
        return
    
    # å´é‚Šæ¬„é…ç½®
    st.sidebar.header("âš™ï¸ æ¨¡å‹è¨­å®š")
    
    model_type = st.sidebar.selectbox(
        "é¸æ“‡æ¨¡å‹é¡å‹",
        ["Ollama (æœ¬åœ°æ¨¡å‹)", "Hugging Face (è¨—ç®¡æ¨¡å‹)"]
    )
    
    if model_type == "Ollama (æœ¬åœ°æ¨¡å‹)":
        st.sidebar.subheader("Ollama è¨­å®š")
        
        # æª¢æŸ¥ Ollama é€£æ¥
        if test_ollama_connection():
            st.sidebar.success("âœ… Ollama æœå‹™é‹è¡Œä¸­")
            
            # ç²å–å¯ç”¨æ¨¡å‹
            available_models = get_available_ollama_models()
            
            if available_models:
                selected_model = st.sidebar.selectbox(
                    "é¸æ“‡æ¨¡å‹",
                    available_models
                )
            else:
                st.sidebar.warning("æ²’æœ‰æ‰¾åˆ°å·²ä¸‹è¼‰çš„æ¨¡å‹")
                st.sidebar.write("è«‹å…ˆä¸‹è¼‰æ¨¡å‹ï¼š")
                st.sidebar.code("ollama pull llama3.1", language="bash")
                selected_model = st.sidebar.text_input(
                    "æˆ–æ‰‹å‹•è¼¸å…¥æ¨¡å‹åç¨±",
                    "llama3.1"
                )
        else:
            st.sidebar.error("âŒ ç„¡æ³•é€£æ¥åˆ° Ollama æœå‹™")
            with st.sidebar.expander("å®‰è£ Ollama"):
                st.write("è«‹å…ˆå®‰è£ä¸¦å•Ÿå‹• Ollamaï¼š")
                st.code("curl -fsSL https://ollama.com/install.sh | sh", language="bash")
                st.write("ç„¶å¾Œä¸‹è¼‰æ¨¡å‹ï¼š")
                st.code("ollama pull llama3.1", language="bash")
            
            selected_model = st.sidebar.text_input(
                "æ¨¡å‹åç¨±",
                "llama3.1"
            )
    
    else:  # Hugging Face
        st.sidebar.subheader("Hugging Face è¨­å®š")
        
        hf_models = [
            "google/flan-t5-small",
            "google/flan-t5-base", 
            "google/flan-t5-large",
            "microsoft/DialoGPT-small",
            "microsoft/DialoGPT-medium",
            "distilgpt2"
        ]
        
        selected_model = st.sidebar.selectbox(
            "é¸æ“‡æ¨¡å‹",
            hf_models
        )
        
        hf_token = st.sidebar.text_input(
            "Hugging Face API Token (é¸ç”¨)",
            type="password",
            help="æä¾› token å¯ä»¥æé«˜ API é™åˆ¶"
        )
    
    # ä¸»è¦å…§å®¹å€åŸŸ
    col1, col2 = st.columns([2, 1])
    
    with col1:
        st.subheader("ğŸ’¬ å°è©±æ¸¬è©¦")
        
        # é è¨­å•é¡Œ
        example_questions = [
            "ä½ å¥½ï¼Œè«‹ä»‹ç´¹ä¸€ä¸‹è‡ªå·±",
            "è§£é‡‹ä»€éº¼æ˜¯äººå·¥æ™ºæ…§",
            "å¯«ä¸€å€‹ç°¡å–®çš„ Python å‡½æ•¸ä¾†è¨ˆç®—æ–æ³¢é‚£å¥‘æ•¸åˆ—",
            "çµ¦æˆ‘ä¸€äº›å­¸ç¿’ç¨‹å¼è¨­è¨ˆçš„å»ºè­°",
            "ä»€éº¼æ˜¯æ©Ÿå™¨å­¸ç¿’ï¼Ÿ",
        ]
        
        # å•é¡Œé¸æ“‡å™¨
        question_type = st.radio(
            "é¸æ“‡å•é¡Œé¡å‹",
            ["é è¨­å•é¡Œ", "è‡ªè¨‚å•é¡Œ"],
            horizontal=True
        )
        
        if question_type == "é è¨­å•é¡Œ":
            user_question = st.selectbox(
                "é¸æ“‡ä¸€å€‹å•é¡Œ",
                example_questions
            )
        else:
            user_question = st.text_area(
                "è¼¸å…¥ä½ çš„å•é¡Œ",
                height=100,
                placeholder="åœ¨é€™è£¡è¼¸å…¥ä½ çš„å•é¡Œ..."
            )
        
        # åŸ·è¡ŒæŒ‰éˆ•
        if st.button("ğŸš€ åŸ·è¡Œ", type="primary"):
            if not user_question.strip():
                st.warning("è«‹è¼¸å…¥å•é¡Œ")
                return
            
            # å‰µå»ºæ¨¡å‹å¯¦ä¾‹
            if model_type == "Ollama (æœ¬åœ°æ¨¡å‹)":
                if not test_ollama_connection():
                    st.error("ç„¡æ³•é€£æ¥åˆ° Ollama æœå‹™ï¼Œè«‹ç¢ºä¿ Ollama æ­£åœ¨é‹è¡Œ")
                    return
                
                llm = create_ollama_llm(selected_model)
            else:
                llm = create_hf_llm(selected_model, hf_token)
            
            if llm is None:
                return
            
            # é¡¯ç¤ºè™•ç†ä¸­
            with st.spinner(f"ğŸ¤” {selected_model} æ­£åœ¨æ€è€ƒ..."):
                start_time = time.time()
                
                try:
                    # åŸ·è¡Œæ¨ç†
                    response = llm.invoke(user_question)
                    
                    end_time = time.time()
                    processing_time = end_time - start_time
                    
                    # é¡¯ç¤ºçµæœ
                    st.subheader("ğŸ¤– æ¨¡å‹å›æ‡‰")
                    st.write(response)
                    
                    # é¡¯ç¤ºæ€§èƒ½æŒ‡æ¨™
                    st.info(f"â±ï¸ è™•ç†æ™‚é–“: {processing_time:.2f} ç§’")
                    
                except Exception as e:
                    st.error(f"åŸ·è¡Œæ™‚ç™¼ç”ŸéŒ¯èª¤: {str(e)}")
    
    with col2:
        st.subheader("ğŸ“Š æ¨¡å‹è³‡è¨Š")
        
        if model_type == "Ollama (æœ¬åœ°æ¨¡å‹)":
            st.write("**æ¨¡å‹é¡å‹:** æœ¬åœ°é‹è¡Œ")
            st.write(f"**æ¨¡å‹åç¨±:** {selected_model}")
            st.write("**å„ªé»:**")
            st.write("- å®Œå…¨å…è²»")
            st.write("- éš±ç§å®‰å…¨") 
            st.write("- é›¢ç·šä½¿ç”¨")
            st.write("**éœ€æ±‚:**")
            st.write("- è¶³å¤ çš„ RAM")
            st.write("- æœ¬åœ°å„²å­˜ç©ºé–“")
            
            if test_ollama_connection():
                st.success("âœ… æœå‹™ç‹€æ…‹: é‹è¡Œä¸­")
            else:
                st.error("âŒ æœå‹™ç‹€æ…‹: æœªé‹è¡Œ")
        else:
            st.write("**æ¨¡å‹é¡å‹:** è¨—ç®¡æœå‹™")
            st.write(f"**æ¨¡å‹åç¨±:** {selected_model}")
            st.write("**å„ªé»:**")
            st.write("- ç„¡éœ€æœ¬åœ°è³‡æº")
            st.write("- å¿«é€Ÿé–‹å§‹")
            st.write("- å¤šç¨®æ¨¡å‹é¸æ“‡")
            st.write("**é™åˆ¶:**")
            st.write("- éœ€è¦ç¶²è·¯é€£æ¥")
            st.write("- API ä½¿ç”¨é™åˆ¶")
            
            if hf_token:
                st.success("âœ… API Token: å·²è¨­å®š")
            else:
                st.warning("âš ï¸ API Token: æœªè¨­å®š")
        
        # ä½¿ç”¨èªªæ˜
        with st.expander("ğŸ“– ä½¿ç”¨èªªæ˜"):
            if model_type == "Ollama (æœ¬åœ°æ¨¡å‹)":
                st.write("**å®‰è£ Ollama:**")
                st.code("curl -fsSL https://ollama.com/install.sh | sh")
                
                st.write("**ä¸‹è¼‰æ¨¡å‹:**")
                st.code("ollama pull llama3.1")
                
                st.write("**å•Ÿå‹•æœå‹™:**")
                st.code("ollama serve")
            else:
                st.write("**ç²å– API Token:**")
                st.write("1. è¨»å†Š [Hugging Face](https://huggingface.co)")
                st.write("2. å‰å¾€ Settings > Access Tokens")
                st.write("3. å‰µå»ºæ–°çš„ token")
                st.write("4. åœ¨å·¦å´è¼¸å…¥ token")
    
    # åº•éƒ¨è³‡è¨Š
    st.divider()
    
    col1, col2, col3 = st.columns(3)
    
    with col1:
        st.subheader("ğŸ”— ç›¸é—œé€£çµ")
        st.write("- [Ollama å®˜ç¶²](https://ollama.com)")
        st.write("- [Hugging Face](https://huggingface.co)")
        st.write("- [LangChain æ–‡æª”](https://python.langchain.com)")
    
    with col2:
        st.subheader("ğŸ’¡ ä½¿ç”¨å»ºè­°")
        st.write("- æœ¬åœ°æ¨¡å‹é©åˆéš±ç§æ•æ„Ÿæ‡‰ç”¨")
        st.write("- è¨—ç®¡æ¨¡å‹é©åˆå¿«é€ŸåŸå‹é–‹ç™¼")
        st.write("- æ ¹æ“šç¡¬é«”è³‡æºé¸æ“‡æ¨¡å‹å¤§å°")
    
    with col3:
        st.subheader("âš¡ æ•ˆèƒ½å„ªåŒ–")
        st.write("- ä½¿ç”¨é‡åŒ–æ¨¡å‹æ¸›å°‘è¨˜æ†¶é«”")
        st.write("- èª¿æ•´ temperature åƒæ•¸")
        st.write("- æ‰¹æ¬¡è™•ç†æé«˜æ•ˆç‡")

if __name__ == "__main__":
    main()