# è¨˜æ†¶æ©Ÿåˆ¶èˆ‡å°è©±ç®¡ç†

## LangChain v0.2+ è¨˜æ†¶æ¶æ§‹è®Šé©

å‚³çµ±çš„è¨˜æ†¶æ©Ÿåˆ¶å·²è¢«æ›´å¼·å¤§ä¸”éˆæ´»çš„ `RunnableWithMessageHistory` å–ä»£ï¼Œæä¾›æ›´å¥½çš„æŒä¹…åŒ–å’Œå¤šæœƒè©±æ”¯æŒã€‚

### æ ¸å¿ƒæ¶æ§‹è®ŠåŒ–

```mermaid
graph TB
    subgraph "v0.2+ æ–°ç‰ˆè¨˜æ†¶æ¶æ§‹"
        A[Message History Store] --> B[RunnableWithMessageHistory]
        B --> C[Session Management]
        C --> D[Persistent Storage]
        
        subgraph "æ”¯æ´çš„å„²å­˜å¾Œç«¯"
            D --> E[ChatMessageHistory]
            D --> F[RedisChatMessageHistory]
            D --> G[PostgresChatMessageHistory]
            D --> H[Custom Storage]
        end
        
        subgraph "è¨˜æ†¶é¡å‹"
            I[ConversationBufferMemory]
            J[ConversationSummaryMemory]
            K[ConversationBufferWindowMemory]
            L[VectorStoreRetrieverMemory]
        end
        
        B --> I
        B --> J
        B --> K
        B --> L
    end
```

## æ–°ç‰ˆè¨˜æ†¶å¯¦ç¾

### 1. åŸºæœ¬å°è©±è¨˜æ†¶

```python
from langchain_core.runnables.history import RunnableWithMessageHistory
from langchain_community.chat_message_histories import ChatMessageHistory
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_openai import ChatOpenAI

# å»ºç«‹å°è©±æ¨¡æ¿
prompt = ChatPromptTemplate.from_messages([
    ("system", "ä½ æ˜¯ä¸€å€‹æœ‰ç”¨çš„åŠ©æ‰‹ï¼Œèƒ½è¨˜ä½å°è©±æ­·å²ã€‚"),
    MessagesPlaceholder(variable_name="chat_history"),
    ("human", "{input}"),
])

# å»ºç«‹åŸºæœ¬å°è©±éˆ
llm = ChatOpenAI(model="gpt-4o-mini", temperature=0.7)
chain = prompt | llm

# è¨˜æ†¶å„²å­˜
store = {}
def get_session_history(session_id: str) -> ChatMessageHistory:
    if session_id not in store:
        store[session_id] = ChatMessageHistory()
    return store[session_id]

# å¸¶è¨˜æ†¶çš„å°è©±éˆ
conversation = RunnableWithMessageHistory(
    chain,
    get_session_history,
    input_messages_key="input",
    history_messages_key="chat_history",
)

# å¤šè¼ªå°è©±
config = {"configurable": {"session_id": "user_123"}}

# ç¬¬ä¸€è¼ª
response1 = conversation.invoke(
    {"input": "æˆ‘å«å°æ˜ï¼Œä»Šå¹´ 25 æ­²"},
    config=config
)
print(response1.content)

# ç¬¬äºŒè¼ª - AI æœƒè¨˜ä½å‰é¢çš„è³‡è¨Š
response2 = conversation.invoke(
    {"input": "æˆ‘å‰›æ‰èªªæˆ‘å¹¾æ­²ï¼Ÿ"},
    config=config
)
print(response2.content)  # æœƒå›ç­”ã€Œä½ èªªä½ ä»Šå¹´ 25 æ­²ã€
```

### 2. æŒä¹…åŒ–è¨˜æ†¶å„²å­˜

```python
# Redis æŒä¹…åŒ–è¨˜æ†¶
from langchain_community.chat_message_histories import RedisChatMessageHistory

def get_redis_history(session_id: str) -> RedisChatMessageHistory:
    return RedisChatMessageHistory(
        session_id=session_id,
        url="redis://localhost:6379/0",
        key_prefix="chat_history:",
        ttl=86400  # 24 å°æ™‚éæœŸ
    )

# PostgreSQL æŒä¹…åŒ–è¨˜æ†¶
from langchain_community.chat_message_histories import PostgresChatMessageHistory

def get_postgres_history(session_id: str) -> PostgresChatMessageHistory:
    return PostgresChatMessageHistory(
        connection_string="postgresql://user:pass@localhost/chatdb",
        session_id=session_id,
        table_name="chat_histories"
    )

# ä½¿ç”¨æŒä¹…åŒ–å„²å­˜
persistent_conversation = RunnableWithMessageHistory(
    chain,
    get_redis_history,  # æˆ– get_postgres_history
    input_messages_key="input",
    history_messages_key="chat_history",
)
```

### 3. æ™ºèƒ½è¨˜æ†¶ç®¡ç†

```python
from langchain.memory import ConversationSummaryBufferMemory

class IntelligentMemoryManager:
    def __init__(self, max_token_limit=4000):
        self.max_token_limit = max_token_limit
        self.memory = ConversationSummaryBufferMemory(
            llm=ChatOpenAI(model="gpt-4o-mini"),
            max_token_limit=max_token_limit,
            return_messages=True
        )
    
    def get_relevant_history(self, current_input: str, session_id: str):
        """ç²å–èˆ‡ç•¶å‰è¼¸å…¥æœ€ç›¸é—œçš„æ­·å²è¨˜éŒ„"""
        # ä½¿ç”¨å‘é‡æœç´¢æ‰¾åˆ°ç›¸é—œå°è©±
        history = self.memory.chat_memory.messages
        
        # ç°¡åŒ–çš„ç›¸é—œæ€§è¨ˆç®—ï¼ˆå¯¦éš›æ‡‰ç”¨ä¸­å¯ä½¿ç”¨ embeddingï¼‰
        relevant_messages = []
        for message in history[-10:]:  # å–æœ€è¿‘ 10 æ¢
            if any(word in message.content.lower() 
                  for word in current_input.lower().split()):
                relevant_messages.append(message)
        
        return relevant_messages

# æ™ºèƒ½è¨˜æ†¶å°è©±ç³»çµ±
class SmartConversationSystem:
    def __init__(self):
        self.memory_manager = IntelligentMemoryManager()
        self.llm = ChatOpenAI(model="gpt-4o-mini")
    
    def chat(self, user_input: str, session_id: str):
        # ç²å–ç›¸é—œæ­·å²
        relevant_history = self.memory_manager.get_relevant_history(
            user_input, session_id
        )
        
        # æ§‹å»ºå‹•æ…‹ prompt
        prompt = ChatPromptTemplate.from_messages([
            ("system", "åŸºæ–¼ä»¥ä¸‹ç›¸é—œå°è©±æ­·å²å›ç­”å•é¡Œ"),
            *[(msg.type, msg.content) for msg in relevant_history],
            ("human", user_input)
        ])
        
        chain = prompt | self.llm
        response = chain.invoke({"input": user_input})
        
        # æ›´æ–°è¨˜æ†¶
        self.memory_manager.memory.save_context(
            {"input": user_input},
            {"output": response.content}
        )
        
        return response.content
```

### 4. å¤šé¡å‹è¨˜æ†¶æ•´åˆ

```python
class MultiModalMemorySystem:
    """å¤šæ¨¡å¼è¨˜æ†¶ç³»çµ±"""
    
    def __init__(self):
        # çŸ­æœŸè¨˜æ†¶ï¼šæœ€è¿‘çš„å°è©±
        self.short_term = ChatMessageHistory()
        
        # é•·æœŸè¨˜æ†¶ï¼šé‡è¦è³‡è¨Šæ‘˜è¦
        self.long_term = ConversationSummaryMemory(
            llm=ChatOpenAI(model="gpt-4o-mini")
        )
        
        # å‘é‡è¨˜æ†¶ï¼šèªç¾©æœç´¢
        from langchain.memory import VectorStoreRetrieverMemory
        from langchain_community.vectorstores import Chroma
        from langchain_openai import OpenAIEmbeddings
        
        vectorstore = Chroma(embedding_function=OpenAIEmbeddings())
        self.vector_memory = VectorStoreRetrieverMemory(
            retriever=vectorstore.as_retriever(search_kwargs={"k": 3})
        )
    
    def process_conversation(self, user_input: str, ai_response: str):
        """è™•ç†å°è©±ä¸¦æ›´æ–°æ‰€æœ‰è¨˜æ†¶é¡å‹"""
        
        # æ›´æ–°çŸ­æœŸè¨˜æ†¶
        self.short_term.add_user_message(user_input)
        self.short_term.add_ai_message(ai_response)
        
        # æ›´æ–°é•·æœŸè¨˜æ†¶ï¼ˆå®šæœŸç¸½çµï¼‰
        if len(self.short_term.messages) % 10 == 0:
            summary = self.long_term.predict_new_summary(
                self.short_term.messages[-10:],
                self.long_term.moving_summary_buffer
            )
            self.long_term.moving_summary_buffer = summary
        
        # æ›´æ–°å‘é‡è¨˜æ†¶ï¼ˆé‡è¦å°è©±ï¼‰
        if self._is_important_conversation(user_input, ai_response):
            self.vector_memory.save_context(
                {"input": user_input},
                {"output": ai_response}
            )
    
    def _is_important_conversation(self, user_input: str, ai_response: str) -> bool:
        """åˆ¤æ–·æ˜¯å¦ç‚ºé‡è¦å°è©±"""
        important_keywords = ["é‡è¦", "è¨˜ä½", "è¨˜éŒ„", "é—œéµ", "å•é¡Œ"]
        return any(keyword in user_input for keyword in important_keywords)
    
    def retrieve_context(self, query: str) -> str:
        """æª¢ç´¢ç›¸é—œä¸Šä¸‹æ–‡"""
        contexts = []
        
        # çŸ­æœŸè¨˜æ†¶
        recent_context = "\n".join([
            f"{msg.type}: {msg.content}" 
            for msg in self.short_term.messages[-6:]
        ])
        contexts.append(f"æœ€è¿‘å°è©±ï¼š\n{recent_context}")
        
        # é•·æœŸè¨˜æ†¶
        if self.long_term.moving_summary_buffer:
            contexts.append(f"å°è©±æ‘˜è¦ï¼š\n{self.long_term.moving_summary_buffer}")
        
        # å‘é‡è¨˜æ†¶
        relevant_docs = self.vector_memory.retriever.get_relevant_documents(query)
        if relevant_docs:
            vector_context = "\n".join([doc.page_content for doc in relevant_docs])
            contexts.append(f"ç›¸é—œæ­·å²ï¼š\n{vector_context}")
        
        return "\n\n".join(contexts)
```

## è¨˜æ†¶æ©Ÿåˆ¶æœ€ä½³å¯¦è¸

### 1. è¨˜æ†¶é¡å‹é¸æ“‡æŒ‡å—

| ä½¿ç”¨å ´æ™¯ | æ¨è–¦è¨˜æ†¶é¡å‹ | åŸå›  |
|----------|------------|------|
| **çŸ­æœŸå®¢æœå°è©±** | ConversationBufferWindowMemory | åªéœ€è¨˜ä½æœ€è¿‘å¹¾è¼ªå°è©± |
| **é•·æœŸå€‹äººåŠ©æ‰‹** | ConversationSummaryBufferMemory | å¹³è¡¡è©³ç´°åº¦èˆ‡é•·åº¦ |
| **çŸ¥è­˜å•ç­”ç³»çµ±** | VectorStoreRetrieverMemory | èªç¾©æœç´¢æ­·å²ç›¸é—œå•é¡Œ |
| **å¤šç”¨æˆ¶ç³»çµ±** | RunnableWithMessageHistory + Redis | å¯æ“´å±•æ€§èˆ‡æŒä¹…åŒ– |
| **ä¼æ¥­ç´šæ‡‰ç”¨** | å¤šæ¨¡å¼è¨˜æ†¶æ•´åˆ | å…¨é¢çš„ä¸Šä¸‹æ–‡ç®¡ç† |

### 2. æ€§èƒ½å„ªåŒ–ç­–ç•¥

```python
# è¨˜æ†¶å£“ç¸®èˆ‡æ¸…ç†
class OptimizedMemoryManager:
    def __init__(self, max_messages=100):
        self.max_messages = max_messages
        self.compression_threshold = 50
    
    def compress_memory_if_needed(self, memory):
        """æ™ºèƒ½è¨˜æ†¶å£“ç¸®"""
        if len(memory.messages) > self.compression_threshold:
            # ä¿ç•™æœ€è¿‘çš„é‡è¦å°è©±
            important_messages = self._extract_important_messages(
                memory.messages
            )
            # ç¸½çµèˆŠå°è©±
            summary = self._summarize_old_messages(
                memory.messages[:-20]
            )
            # é‡æ§‹è¨˜æ†¶
            memory.clear()
            memory.add_message(summary)
            memory.messages.extend(important_messages)
    
    def _extract_important_messages(self, messages):
        """æå–é‡è¦è¨Šæ¯"""
        # å¯¦ç¾é‡è¦æ€§è©•åˆ†é‚è¼¯
        pass
    
    def _summarize_old_messages(self, old_messages):
        """ç¸½çµèˆŠè¨Šæ¯"""
        # å¯¦ç¾ç¸½çµé‚è¼¯
        pass
```

### 3. å¤šæœƒè©±ç®¡ç†

```python
class SessionManager:
    def __init__(self):
        self.active_sessions = {}
        self.session_timeout = 3600  # 1å°æ™‚
    
    def get_or_create_session(self, user_id: str, session_type: str = "default"):
        """ç²å–æˆ–å‰µå»ºæœƒè©±"""
        session_key = f"{user_id}_{session_type}"
        
        if session_key not in self.active_sessions:
            self.active_sessions[session_key] = {
                "memory": ChatMessageHistory(),
                "created_at": time.time(),
                "last_activity": time.time()
            }
        
        # æ›´æ–°æ´»å‹•æ™‚é–“
        self.active_sessions[session_key]["last_activity"] = time.time()
        return self.active_sessions[session_key]["memory"]
    
    def cleanup_expired_sessions(self):
        """æ¸…ç†éæœŸæœƒè©±"""
        current_time = time.time()
        expired_sessions = [
            key for key, session in self.active_sessions.items()
            if current_time - session["last_activity"] > self.session_timeout
        ]
        
        for key in expired_sessions:
            del self.active_sessions[key]
```

## å¯¦éš›æ‡‰ç”¨ç¯„ä¾‹

### æ™ºèƒ½å®¢æœè¨˜æ†¶ç³»çµ±

```python
class CustomerServiceMemory:
    def __init__(self):
        self.customer_profiles = {}  # å®¢æˆ¶æª”æ¡ˆ
        self.conversation_history = {}  # å°è©±æ­·å²
        self.issue_tracking = {}  # å•é¡Œè¿½è¹¤
    
    def get_customer_context(self, customer_id: str, current_query: str):
        """ç²å–å®¢æˆ¶ä¸Šä¸‹æ–‡"""
        # å®¢æˆ¶æª”æ¡ˆ
        profile = self.customer_profiles.get(customer_id, {})
        
        # æ­·å²å°è©±
        history = self.conversation_history.get(customer_id, [])
        
        # ç›¸é—œå•é¡Œ
        similar_issues = self._find_similar_issues(current_query)
        
        context = {
            "customer_profile": profile,
            "conversation_history": history[-5:],  # æœ€è¿‘5æ¬¡å°è©±
            "similar_issues": similar_issues,
            "current_query": current_query
        }
        
        return context
    
    def update_customer_context(self, customer_id: str, 
                              interaction_data: dict):
        """æ›´æ–°å®¢æˆ¶ä¸Šä¸‹æ–‡"""
        # æ›´æ–°å®¢æˆ¶æª”æ¡ˆ
        if customer_id not in self.customer_profiles:
            self.customer_profiles[customer_id] = {}
        
        # æå–å®¢æˆ¶è³‡è¨Š
        self._extract_customer_info(customer_id, interaction_data)
        
        # æ›´æ–°å°è©±æ­·å²
        if customer_id not in self.conversation_history:
            self.conversation_history[customer_id] = []
        
        self.conversation_history[customer_id].append({
            "timestamp": datetime.now(),
            "query": interaction_data.get("query"),
            "response": interaction_data.get("response"),
            "satisfaction": interaction_data.get("satisfaction")
        })
```

### æ•™è‚²å°è©±ç³»çµ±

```python
class EducationalChatMemory:
    """æ•™è‚²å°è©±ç³»çµ±è¨˜æ†¶"""
    
    def __init__(self):
        self.student_progress = {}
        self.learning_patterns = {}
        self.difficulty_adjustments = {}
    
    def track_learning_progress(self, student_id: str, 
                              topic: str, performance: dict):
        """è¿½è¹¤å­¸ç¿’é€²åº¦"""
        if student_id not in self.student_progress:
            self.student_progress[student_id] = {}
        
        if topic not in self.student_progress[student_id]:
            self.student_progress[student_id][topic] = {
                "attempts": 0,
                "correct_answers": 0,
                "difficulty_level": 1,
                "last_session": None
            }
        
        progress = self.student_progress[student_id][topic]
        progress["attempts"] += 1
        
        if performance.get("correct"):
            progress["correct_answers"] += 1
        
        progress["last_session"] = datetime.now()
        
        # èª¿æ•´é›£åº¦
        self._adjust_difficulty(student_id, topic)
    
    def get_personalized_context(self, student_id: str, topic: str):
        """ç²å–å€‹äººåŒ–ä¸Šä¸‹æ–‡"""
        progress = self.student_progress.get(student_id, {}).get(topic, {})
        
        context = {
            "current_level": progress.get("difficulty_level", 1),
            "success_rate": self._calculate_success_rate(progress),
            "weak_areas": self._identify_weak_areas(student_id),
            "learning_style": self._detect_learning_style(student_id)
        }
        
        return context
```

## ç¸½çµ

LangChain v0.2+ çš„è¨˜æ†¶æ©Ÿåˆ¶æä¾›äº†ï¼š

- ğŸ”„ **çµ±ä¸€çš„è¨˜æ†¶ä»‹é¢** - RunnableWithMessageHistory çµ±ä¸€ç®¡ç†
- ğŸ’¾ **å¤šç¨®æŒä¹…åŒ–é¸é …** - Redisã€PostgreSQLã€è‡ªå®šç¾©å„²å­˜
- ğŸ§  **æ™ºèƒ½è¨˜æ†¶ç®¡ç†** - è‡ªå‹•å£“ç¸®ã€ç›¸é—œæ€§æœç´¢
- ğŸ‘¥ **å¤šæœƒè©±æ”¯æŒ** - ä¸¦è¡Œè™•ç†å¤šå€‹ç”¨æˆ¶æœƒè©±
- ğŸ¯ **ä¸Šä¸‹æ–‡æ„ŸçŸ¥** - åŸºæ–¼æ­·å²çš„æ™ºèƒ½å›æ‡‰
- âš¡ **æ€§èƒ½å„ªåŒ–** - è¨˜æ†¶å£“ç¸®å’ŒéæœŸæ¸…ç†

é¸æ“‡åˆé©çš„è¨˜æ†¶ç­–ç•¥ï¼Œçµåˆè‰¯å¥½çš„æœƒè©±ç®¡ç†ï¼Œèƒ½å¤ å»ºæ§‹å‡ºçœŸæ­£æ™ºèƒ½çš„å°è©±ç³»çµ±ã€‚

---

::: tip ä¸‹ä¸€æ­¥
ç¾åœ¨ä½ å·²ç¶“æŒæ¡äº†è¨˜æ†¶æ©Ÿåˆ¶ï¼Œæ¥ä¸‹ä¾†å¯ä»¥ï¼š
1. [ç›£æ§èˆ‡å¯è§€æ¸¬æ€§](/tutorials/monitoring) - ç›£æ§è¨˜æ†¶ç³»çµ±çš„æ€§èƒ½
2. [é€²éšæ‡‰ç”¨æ¡ˆä¾‹](/tutorials/advanced-examples) - æŸ¥çœ‹ä¼æ¥­ç´šè¨˜æ†¶ç³»çµ±å¯¦ç¾
3. çµåˆ [LangGraph](/tutorials/langgraph) å»ºæ§‹æœ‰ç‹€æ…‹çš„æ™ºèƒ½å·¥ä½œæµ
:::

::: warning å¯¦è¸å»ºè­°
- **åˆç†é¸æ“‡è¨˜æ†¶é¡å‹**ï¼šæ ¹æ“šæ‡‰ç”¨å ´æ™¯é¸æ“‡æœ€é©åˆçš„è¨˜æ†¶ç­–ç•¥
- **å®šæœŸæ¸…ç†è¨˜æ†¶**ï¼šé¿å…è¨˜æ†¶éåº¦è†¨è„¹å½±éŸ¿æ€§èƒ½
- **ä¿è­·éš±ç§è³‡æ–™**ï¼šæ•æ„Ÿè³‡è¨Šæ‡‰åŠ å¯†å­˜å„²æˆ–å®šæœŸæ¸…é™¤
- **ç›£æ§è¨˜æ†¶ä½¿ç”¨**ï¼šè¿½è¹¤è¨˜æ†¶å¤§å°å’Œæª¢ç´¢æ•ˆç‡
:::