# ç›£æ§èˆ‡å¯è§€æ¸¬æ€§ (LangSmith Integration)

## LangSmith æ¦‚è¿°

LangSmith æ˜¯ LangChain å®˜æ–¹çš„å¯è§€æ¸¬æ€§å¹³å°ï¼Œæä¾›å®Œæ•´çš„ AI æ‡‰ç”¨ç¨‹å¼ç›£æ§ã€èª¿è©¦å’Œå„ªåŒ–åŠŸèƒ½ã€‚

```mermaid
graph TB
    subgraph "LangSmith å¯è§€æ¸¬æ€§æ¶æ§‹"
        A[LangChain Application] --> B[LangSmith SDK]
        B --> C[Trace Collection]
        C --> D[LangSmith Platform]
        
        subgraph "ç›£æ§åŠŸèƒ½"
            D --> E[åŸ·è¡Œè¿½è¹¤]
            D --> F[æ€§èƒ½åˆ†æ]
            D --> G[éŒ¯èª¤ç›£æ§]
            D --> H[æˆæœ¬è¿½è¹¤]
        end
        
        subgraph "å„ªåŒ–åŠŸèƒ½"
            D --> I[Prompt æ¸¬è©¦]
            D --> J[A/B æ¸¬è©¦]
            D --> K[æ•¸æ“šé›†ç®¡ç†]
            D --> L[æ¨¡å‹æ¯”è¼ƒ]
        end
        
        subgraph "å”ä½œåŠŸèƒ½"
            D --> M[åœ˜éšŠå„€è¡¨æ¿]
            D --> N[å ±å‘Šç”Ÿæˆ]
            D --> O[å‘Šè­¦é€šçŸ¥]
        end
    end
```

## LangSmith åŸºæœ¬è¨­ç½®

### ç’°å¢ƒé…ç½®

```python
import os
from langchain_openai import ChatOpenAI
from langchain_core.prompts import ChatPromptTemplate

# è¨­ç½® LangSmith è¿½è¹¤
os.environ["LANGCHAIN_TRACING_V2"] = "true"
os.environ["LANGCHAIN_ENDPOINT"] = "https://api.smith.langchain.com"
os.environ["LANGCHAIN_API_KEY"] = "your_langsmith_api_key"  # å¾ç’°å¢ƒè®Šæ•¸ç²å–
os.environ["LANGCHAIN_PROJECT"] = "your_project_name"  # å°ˆæ¡ˆåç¨±

# æ‰€æœ‰å¾ŒçºŒçš„ LangChain åŸ·è¡Œéƒ½æœƒè¢«è‡ªå‹•è¿½è¹¤
llm = ChatOpenAI(model="gpt-4o-mini")
prompt = ChatPromptTemplate.from_template("å›ç­”å•é¡Œï¼š{question}")
chain = prompt | llm

# é€™å€‹åŸ·è¡Œæœƒè¢«è¿½è¹¤åˆ° LangSmith
result = chain.invoke({"question": "ä»€éº¼æ˜¯æ©Ÿå™¨å­¸ç¿’ï¼Ÿ"})
```

### è‡ªå®šç¾©è¿½è¹¤æ¨™ç±¤

```python
from langsmith import traceable
from langchain.callbacks import LangChainTracer

# æ–¹æ³•ä¸€ï¼šä½¿ç”¨ @traceable è£é£¾å™¨
@traceable(name="custom_analysis")
def analyze_sentiment(text: str) -> dict:
    """è‡ªå®šç¾©å‡½æ•¸è¿½è¹¤"""
    chain = sentiment_prompt | llm | sentiment_parser
    result = chain.invoke({"text": text})
    
    return {
        "sentiment": result.sentiment,
        "confidence": result.confidence,
        "processing_time": time.time()  # è‡ªå®šç¾©æŒ‡æ¨™
    }

# æ–¹æ³•äºŒï¼šä½¿ç”¨ LangChainTracer
tracer = LangChainTracer(
    project_name="sentiment_analysis",
    tags=["production", "v2.0"]  # è‡ªå®šç¾©æ¨™ç±¤
)

chain_with_tracer = chain.with_config({
    "callbacks": [tracer],
    "tags": ["sentiment", "batch_processing"]
})

result = chain_with_tracer.invoke({"question": "åˆ†æé€™æ®µæ–‡å­—çš„æƒ…æ„Ÿ"})
```

## é€²éšç›£æ§åŠŸèƒ½

### 1. æ€§èƒ½ç›£æ§èˆ‡åˆ†æ

```python
from langsmith import Client
import time
from typing import Dict, Any

class PerformanceMonitor:
    def __init__(self, project_name: str):
        self.client = Client()
        self.project_name = project_name
    
    @traceable(name="monitored_chain_execution")
    def execute_with_monitoring(self, chain, inputs: Dict[str, Any]):
        """å¸¶æ€§èƒ½ç›£æ§çš„éˆåŸ·è¡Œ"""
        start_time = time.time()
        
        try:
            # åŸ·è¡Œéˆ
            result = chain.invoke(inputs)
            
            # è¨˜éŒ„æˆåŠŸæŒ‡æ¨™
            execution_time = time.time() - start_time
            self._log_performance_metrics({
                "execution_time": execution_time,
                "status": "success",
                "input_length": len(str(inputs)),
                "output_length": len(str(result))
            })
            
            return result
            
        except Exception as e:
            # è¨˜éŒ„éŒ¯èª¤æŒ‡æ¨™
            self._log_error_metrics({
                "error_type": type(e).__name__,
                "error_message": str(e),
                "execution_time": time.time() - start_time
            })
            raise
    
    def _log_performance_metrics(self, metrics: dict):
        """è¨˜éŒ„æ€§èƒ½æŒ‡æ¨™"""
        # ç™¼é€è‡ªå®šç¾©æŒ‡æ¨™åˆ° LangSmith
        pass
    
    def _log_error_metrics(self, metrics: dict):
        """è¨˜éŒ„éŒ¯èª¤æŒ‡æ¨™"""
        # ç™¼é€éŒ¯èª¤æŒ‡æ¨™åˆ° LangSmith
        pass

# ä½¿ç”¨ç¯„ä¾‹
monitor = PerformanceMonitor("production_analysis")
result = monitor.execute_with_monitoring(analysis_chain, {"data": input_data})
```

### 2. è‡ªå‹•åŒ–æ¸¬è©¦èˆ‡è©•ä¼°

```python
from langsmith.evaluation import evaluate
from langsmith.schemas import Dataset, Example

# å»ºç«‹æ¸¬è©¦è³‡æ–™é›†
def create_evaluation_dataset():
    examples = [
        Example(
            inputs={"question": "ä»€éº¼æ˜¯äººå·¥æ™ºæ…§ï¼Ÿ"},
            outputs={"answer": "äººå·¥æ™ºæ…§æ˜¯ä¸€é–€è®“æ©Ÿå™¨èƒ½å¤ æ¨¡æ“¬äººé¡æ™ºæ…§çš„æŠ€è¡“..."},
            metadata={"category": "definition", "difficulty": "easy"}
        ),
        Example(
            inputs={"question": "è§£é‡‹æ·±åº¦å­¸ç¿’çš„åŸç†"},
            outputs={"answer": "æ·±åº¦å­¸ç¿’æ˜¯ä¸€ç¨®æ©Ÿå™¨å­¸ç¿’æ–¹æ³•..."},
            metadata={"category": "technical", "difficulty": "hard"}
        )
    ]
    
    return Dataset(
        name="ai_qa_evaluation",
        description="AI å•ç­”ç³»çµ±è©•ä¼°æ•¸æ“šé›†",
        examples=examples
    )

# å®šç¾©è©•ä¼°å‡½æ•¸
def accuracy_evaluator(run, example) -> dict:
    """æº–ç¢ºæ€§è©•ä¼°å™¨"""
    expected = example.outputs["answer"]
    actual = run.outputs["answer"]
    
    # ä½¿ç”¨ LLM è©•ä¼°ç­”æ¡ˆç›¸ä¼¼æ€§
    evaluation_prompt = f"""
    è©•ä¼°ä»¥ä¸‹å…©å€‹ç­”æ¡ˆçš„ç›¸ä¼¼æ€§ï¼ˆ0-1 åˆ†ï¼‰ï¼š
    æœŸæœ›ç­”æ¡ˆï¼š{expected}
    å¯¦éš›ç­”æ¡ˆï¼š{actual}
    """
    
    eval_llm = ChatOpenAI(model="gpt-4o-mini")
    score = eval_llm.invoke(evaluation_prompt)
    
    return {"score": float(score.content), "key": "accuracy"}

# åŸ·è¡Œè©•ä¼°
dataset = create_evaluation_dataset()
results = evaluate(
    lambda inputs: qa_chain.invoke(inputs),  # è¦æ¸¬è©¦çš„å‡½æ•¸
    data=dataset,
    evaluators=[accuracy_evaluator],
    experiment_prefix="qa_system_v2",
    metadata={"version": "2.0", "model": "gpt-4o-mini"}
)

print(f"å¹³å‡æº–ç¢ºæ€§ï¼š{results['accuracy']}")
```

### 3. A/B æ¸¬è©¦æ¡†æ¶

```python
import random
from enum import Enum

class ModelVariant(Enum):
    VARIANT_A = "gpt-4o-mini"
    VARIANT_B = "gpt-4o"

class ABTestFramework:
    def __init__(self, split_ratio=0.5):
        self.split_ratio = split_ratio
        self.results = {"A": [], "B": []}
    
    @traceable(name="ab_test_execution")
    def execute_ab_test(self, user_input: str, session_id: str):
        """åŸ·è¡Œ A/B æ¸¬è©¦"""
        
        # æ±ºå®šä½¿ç”¨å“ªå€‹è®Šé«”
        variant = self._assign_variant(session_id)
        
        # æ ¹æ“šè®Šé«”å»ºç«‹ä¸åŒçš„éˆ
        if variant == "A":
            llm = ChatOpenAI(model=ModelVariant.VARIANT_A.value)
            chain = prompt | llm
        else:
            llm = ChatOpenAI(model=ModelVariant.VARIANT_B.value)
            chain = prompt | llm
        
        # åŸ·è¡Œä¸¦è¨˜éŒ„çµæœ
        result = chain.invoke({"input": user_input})
        
        # è¨˜éŒ„åˆ° LangSmithï¼ˆåŒ…å«è®Šé«”è³‡è¨Šï¼‰
        self._log_ab_result(variant, user_input, result, session_id)
        
        return result
    
    def _assign_variant(self, session_id: str) -> str:
        """åŸºæ–¼ session_id ç©©å®šåœ°åˆ†é…è®Šé«”"""
        hash_value = hash(session_id)
        return "A" if hash_value % 2 == 0 else "B"
    
    @traceable(name="ab_test_logging")
    def _log_ab_result(self, variant: str, input_data: str, 
                      result: Any, session_id: str):
        """è¨˜éŒ„ A/B æ¸¬è©¦çµæœ"""
        log_data = {
            "variant": variant,
            "session_id": session_id,
            "input_length": len(input_data),
            "output_length": len(str(result)),
            "timestamp": time.time()
        }
        
        # ç™¼é€åˆ° LangSmith é€²è¡Œåˆ†æ
        self.results[variant].append(log_data)

# ä½¿ç”¨ A/B æ¸¬è©¦
ab_tester = ABTestFramework()
result = ab_tester.execute_ab_test(
    "è§£é‡‹é‡å­è¨ˆç®—çš„åŸºæœ¬åŸç†", 
    "user_12345"
)
```

## ç”Ÿç”¢ç’°å¢ƒç›£æ§æœ€ä½³å¯¦è¸

### 1. å‘Šè­¦ç³»çµ±è¨­ç½®

```python
from langsmith import Client
from typing import Callable
import logging

class ProductionMonitor:
    def __init__(self, alert_thresholds: dict):
        self.client = Client()
        self.thresholds = alert_thresholds
        self.logger = logging.getLogger(__name__)
    
    @traceable(name="production_execution_monitor")
    def monitor_execution(self, chain_func: Callable, inputs: dict):
        """ç”Ÿç”¢ç’°å¢ƒåŸ·è¡Œç›£æ§"""
        start_time = time.time()
        
        try:
            result = chain_func(inputs)
            execution_time = time.time() - start_time
            
            # æª¢æŸ¥æ€§èƒ½å‘Šè­¦
            self._check_performance_alerts(execution_time)
            
            # æª¢æŸ¥è¼¸å‡ºå“è³ª
            self._check_output_quality(result)
            
            return result
            
        except Exception as e:
            self._handle_error_alert(e)
            raise
    
    def _check_performance_alerts(self, execution_time: float):
        """æª¢æŸ¥æ€§èƒ½å‘Šè­¦"""
        if execution_time > self.thresholds.get("max_execution_time", 30):
            self._send_alert(
                "PERFORMANCE",
                f"åŸ·è¡Œæ™‚é–“éé•·ï¼š{execution_time:.2f}ç§’"
            )
    
    def _check_output_quality(self, result: Any):
        """æª¢æŸ¥è¼¸å‡ºå“è³ª"""
        output_text = str(result)
        
        # æª¢æŸ¥ç©ºè¼¸å‡º
        if len(output_text.strip()) == 0:
            self._send_alert("QUALITY", "æª¢æ¸¬åˆ°ç©ºè¼¸å‡º")
        
        # æª¢æŸ¥éŒ¯èª¤é—œéµå­—
        error_keywords = ["éŒ¯èª¤", "å¤±æ•—", "ç„¡æ³•", "ä¸çŸ¥é“"]
        if any(keyword in output_text for keyword in error_keywords):
            self._send_alert("QUALITY", f"æª¢æ¸¬åˆ°å•é¡Œè¼¸å‡ºï¼š{output_text[:100]}...")
    
    def _handle_error_alert(self, error: Exception):
        """è™•ç†éŒ¯èª¤å‘Šè­¦"""
        self._send_alert(
            "ERROR",
            f"ç³»çµ±éŒ¯èª¤ï¼š{type(error).__name__} - {str(error)}"
        )
    
    def _send_alert(self, alert_type: str, message: str):
        """ç™¼é€å‘Šè­¦"""
        self.logger.error(f"[{alert_type}] {message}")
        # é€™è£¡å¯ä»¥æ•´åˆ Slackã€Emailã€webhook ç­‰å‘Šè­¦é€šé“
        # ä¾‹å¦‚ï¼šsend_slack_alert(alert_type, message)

# ç”Ÿç”¢ç’°å¢ƒä½¿ç”¨
monitor = ProductionMonitor({
    "max_execution_time": 10.0,
    "min_output_length": 10
})

result = monitor.monitor_execution(
    lambda inputs: production_chain.invoke(inputs),
    {"user_query": "å¹«æˆ‘åˆ†æå¸‚å ´è¶¨å‹¢"}
)
```

### 2. æˆæœ¬æ§åˆ¶èˆ‡ç›£æ§

```python
class CostMonitor:
    def __init__(self, budget_limits: dict):
        self.budget_limits = budget_limits
        self.current_costs = {"daily": 0.0, "monthly": 0.0}
    
    @traceable(name="cost_aware_execution")
    def execute_with_cost_control(self, chain, inputs: dict, 
                                user_tier: str = "basic"):
        """å¸¶æˆæœ¬æ§åˆ¶çš„åŸ·è¡Œ"""
        
        # æª¢æŸ¥æˆæœ¬é™åˆ¶
        if not self._check_budget_limits(user_tier):
            raise Exception(f"é ç®—è¶…é™ï¼Œç„¡æ³•åŸ·è¡Œè«‹æ±‚")
        
        # ä¼°ç®—åŸ·è¡Œæˆæœ¬
        estimated_cost = self._estimate_execution_cost(inputs, user_tier)
        
        # åŸ·è¡Œä¸¦è¿½è¹¤å¯¦éš›æˆæœ¬
        result = chain.invoke(inputs)
        actual_cost = self._calculate_actual_cost(inputs, result)
        
        # æ›´æ–°æˆæœ¬è¨ˆæ•¸å™¨
        self._update_cost_counters(actual_cost)
        
        # è¨˜éŒ„æˆæœ¬è³‡è¨Šåˆ° LangSmith
        self._log_cost_metrics({
            "estimated_cost": estimated_cost,
            "actual_cost": actual_cost,
            "user_tier": user_tier,
            "input_tokens": self._count_tokens(str(inputs)),
            "output_tokens": self._count_tokens(str(result))
        })
        
        return result
    
    def _check_budget_limits(self, user_tier: str) -> bool:
        """æª¢æŸ¥é ç®—é™åˆ¶"""
        daily_limit = self.budget_limits.get(f"{user_tier}_daily", float('inf'))
        return self.current_costs["daily"] < daily_limit
    
    def _estimate_execution_cost(self, inputs: dict, user_tier: str) -> float:
        """ä¼°ç®—åŸ·è¡Œæˆæœ¬"""
        input_tokens = self._count_tokens(str(inputs))
        # åŸºæ–¼æ­·å²æ•¸æ“šä¼°ç®—è¼¸å‡º tokens
        estimated_output_tokens = input_tokens * 2  # ç°¡åŒ–ä¼°ç®—
        
        # æ ¹æ“šç”¨æˆ¶ç­‰ç´šæ‡‰ç”¨ä¸åŒçš„å®šåƒ¹
        pricing = {
            "basic": {"input": 0.0001, "output": 0.0002},
            "premium": {"input": 0.00005, "output": 0.0001}
        }
        
        rate = pricing.get(user_tier, pricing["basic"])
        return (input_tokens * rate["input"] + 
                estimated_output_tokens * rate["output"])
    
    def _count_tokens(self, text: str) -> int:
        """è¨ˆç®— token æ•¸é‡ï¼ˆç°¡åŒ–å¯¦ç¾ï¼‰"""
        return len(text.split()) * 1.3  # ç²—ç•¥ä¼°ç®—
    
    def _calculate_actual_cost(self, inputs: dict, result: Any) -> float:
        """è¨ˆç®—å¯¦éš›æˆæœ¬"""
        # å¯¦éš›æ‡‰ç”¨ä¸­ï¼Œå¯ä»¥é€šé LLM provider çš„ API ç²å¾—ç²¾ç¢ºçš„ token ä½¿ç”¨é‡
        input_tokens = self._count_tokens(str(inputs))
        output_tokens = self._count_tokens(str(result))
        
        return input_tokens * 0.0001 + output_tokens * 0.0002
    
    def _update_cost_counters(self, cost: float):
        """æ›´æ–°æˆæœ¬è¨ˆæ•¸å™¨"""
        self.current_costs["daily"] += cost
        self.current_costs["monthly"] += cost
    
    @traceable(name="cost_logging")
    def _log_cost_metrics(self, metrics: dict):
        """è¨˜éŒ„æˆæœ¬æŒ‡æ¨™åˆ° LangSmith"""
        # LangSmith æœƒè‡ªå‹•è¿½è¹¤é€™äº›è‡ªå®šç¾©æŒ‡æ¨™
        pass

# ä½¿ç”¨æˆæœ¬ç›£æ§
cost_monitor = CostMonitor({
    "basic_daily": 10.0,    # åŸºç¤ç”¨æˆ¶æ¯æ—¥ $10
    "premium_daily": 50.0   # é«˜ç´šç”¨æˆ¶æ¯æ—¥ $50
})

result = cost_monitor.execute_with_cost_control(
    expensive_analysis_chain,
    {"data": large_dataset},
    user_tier="premium"
)
```

## ç¸½çµ

LangSmith å¯è§€æ¸¬æ€§å¹³å°æä¾›äº†ï¼š

- ğŸ“Š **å…¨é¢ç›£æ§** - åŸ·è¡Œè¿½è¹¤ã€æ€§èƒ½åˆ†æã€éŒ¯èª¤ç›£æ§
- ğŸ§ª **è‡ªå‹•åŒ–æ¸¬è©¦** - æ•¸æ“šé›†ç®¡ç†ã€è©•ä¼°æ¡†æ¶ã€A/B æ¸¬è©¦
- ğŸ’° **æˆæœ¬æ§åˆ¶** - å¯¦æ™‚æˆæœ¬è¿½è¹¤ã€é ç®—ç®¡ç†ã€ä½¿ç”¨å„ªåŒ–
- ğŸš¨ **æ™ºèƒ½å‘Šè­¦** - è‡ªå®šç¾©é–¾å€¼ã€å¤šé€šé“é€šçŸ¥ã€ç•°å¸¸æª¢æ¸¬
- ğŸ‘¥ **åœ˜éšŠå”ä½œ** - å…±äº«å„€è¡¨æ¿ã€å ±å‘Šç”Ÿæˆã€æ¬Šé™ç®¡ç†
- ğŸ” **æ·±åº¦æ´å¯Ÿ** - èª¿ç”¨éˆåˆ†æã€ç“¶é ¸è­˜åˆ¥ã€å„ªåŒ–å»ºè­°

åœ¨ç”Ÿç”¢ç’°å¢ƒä¸­æ•´åˆ LangSmithï¼Œèƒ½å¤ é¡¯è‘—æå‡ AI æ‡‰ç”¨çš„å¯é æ€§ã€æ€§èƒ½å’Œå¯ç¶­è­·æ€§ã€‚

---

::: tip ä¸‹ä¸€æ­¥
ç¾åœ¨ä½ å·²ç¶“æŒæ¡äº†ç›£æ§èˆ‡å¯è§€æ¸¬æ€§ï¼Œæ¥ä¸‹ä¾†å¯ä»¥ï¼š
1. [é€²éšæ‡‰ç”¨æ¡ˆä¾‹](/tutorials/advanced-examples) - æŸ¥çœ‹ä¼æ¥­ç´šç›£æ§å¯¦ç¾
2. çµåˆå…¶ä»–ç« ç¯€å»ºæ§‹å®Œæ•´çš„ç›£æ§ç­–ç•¥
3. æ¢ç´¢ LangSmith å¹³å°çš„æ›´å¤šé«˜ç´šåŠŸèƒ½
:::

::: warning ç”Ÿç”¢éƒ¨ç½²å»ºè­°
- **è¨­ç½®é©ç•¶çš„å‘Šè­¦é–¾å€¼**ï¼šé¿å…å‘Šè­¦ç–²å‹ï¼Œé—œæ³¨çœŸæ­£é‡è¦çš„æŒ‡æ¨™
- **å®šæœŸå¯©æŸ¥ç›£æ§æ•¸æ“š**ï¼šå¾ç›£æ§æ•¸æ“šä¸­ç²å¾—æŒçºŒæ”¹é€²çš„æ´å¯Ÿ
- **ä¿è­·æ•æ„Ÿè³‡è¨Š**ï¼šç¢ºä¿ç›£æ§æ•¸æ“šä¸åŒ…å«æ•æ„Ÿçš„ç”¨æˆ¶è³‡è¨Š
- **æˆæœ¬æ„è­˜**ï¼šå¯†åˆ‡é—œæ³¨ API èª¿ç”¨æˆæœ¬ï¼Œè¨­ç½®åˆç†çš„é ç®—é™åˆ¶
:::