# LCEL (LangChain Expression Language) è¡¨é”å¼èªè¨€

## ä»€éº¼æ˜¯ LCELï¼Ÿ

LCEL æ˜¯ LangChain v0.2+ çš„æ ¸å¿ƒåŸ·è¡Œå¼•æ“ï¼Œä½¿ç”¨ `|` ç®¡é“æ“ä½œç¬¦ä¾†çµ„åˆå„ç¨®çµ„ä»¶ã€‚å®ƒæä¾›äº†ä¸€ç¨®è²æ˜å¼çš„æ–¹å¼ä¾†æ§‹å»ºè¤‡é›œçš„ AI æ‡‰ç”¨ç¨‹å¼æµç¨‹ã€‚

```mermaid
graph LR
    A[Input] -->|"| æ“ä½œç¬¦"| B[Prompt]
    B -->|"| æ“ä½œç¬¦"| C[LLM]
    C -->|"| æ“ä½œç¬¦"| D[Output Parser]
    D --> E[Structured Output]
    
    style A fill:#e1f5fe
    style E fill:#e8f5e8
    style B fill:#fff3e0
    style C fill:#fce4ec
    style D fill:#f3e5f5
```

## LCEL æ ¸å¿ƒæ¦‚å¿µ

### Runnables ä»‹é¢

æ‰€æœ‰ LangChain çµ„ä»¶éƒ½å¯¦ç¾äº† `Runnable` ä»‹é¢ï¼Œæä¾›çµ±ä¸€çš„åŸ·è¡Œæ–¹æ³•ï¼š

```python
from langchain_core.runnables import Runnable
from langchain_openai import ChatOpenAI
from langchain_core.prompts import PromptTemplate
from langchain_core.output_parsers import StrOutputParser

# æ‰€æœ‰çµ„ä»¶éƒ½æ˜¯ Runnable
llm = ChatOpenAI(model="gpt-4o-mini")
prompt = PromptTemplate.from_template("ç¿»è­¯æˆä¸­æ–‡: {text}")
parser = StrOutputParser()

# ä½¿ç”¨ | æ“ä½œç¬¦çµ„åˆ
chain = prompt | llm | parser

# çµ±ä¸€çš„åŸ·è¡Œä»‹é¢
result = chain.invoke({"text": "Hello, world!"})
print(result)  # ä½ å¥½ï¼Œä¸–ç•Œï¼
```

### ç®¡é“æ“ä½œç¬¦çš„å¨åŠ›

**å‚³çµ±åšæ³• vs LCEL åšæ³•å°æ¯”ï¼š**

```python
# âŒ å‚³çµ±åšæ³•ï¼šæ‰‹å‹•ç®¡ç†ä¸­é–“æ­¥é©Ÿ
def traditional_approach(input_text):
    prompt_result = prompt.format(text=input_text)
    llm_result = llm.invoke(prompt_result)
    final_result = parser.parse(llm_result.content)
    return final_result

# âœ… LCEL åšæ³•ï¼šè²æ˜å¼ç®¡é“
chain = prompt | llm | parser
result = chain.invoke({"text": input_text})
```

## é€²éš LCEL æ¨¡å¼

### 1. å¹³è¡Œè™•ç† (RunnableParallel)

```python
from langchain_core.runnables import RunnableParallel, RunnablePassthrough

# åŒæ™‚åŸ·è¡Œå¤šå€‹åˆ†æ
analysis_chain = RunnableParallel({
    "sentiment": PromptTemplate.from_template("åˆ†ææƒ…æ„Ÿ: {text}") | llm | parser,
    "summary": PromptTemplate.from_template("ç¸½çµå…§å®¹: {text}") | llm | parser,
    "keywords": PromptTemplate.from_template("æå–é—œéµå­—: {text}") | llm | parser,
    "original": RunnablePassthrough()  # ä¿ç•™åŸå§‹è¼¸å…¥
})

# åŒæ™‚ç²å¾—ä¸‰ç¨®åˆ†æçµæœ
results = analysis_chain.invoke({"text": "ä»Šå¤©å¤©æ°£çœŸå¥½ï¼Œå¿ƒæƒ…å¾ˆæ„‰å¿«ï¼"})
print(results)  # {"sentiment": "æ­£é¢", "summary": "...", "keywords": "...", "original": "..."}
```

### 2. æ¢ä»¶åˆ†æ”¯ (RunnableBranch)

```python
from langchain_core.runnables import RunnableBranch

# æ ¹æ“šè¼¸å…¥å…§å®¹é¸æ“‡ä¸åŒè™•ç†è·¯å¾‘
branch = RunnableBranch(
    # æ¢ä»¶ï¼šå¦‚æœæ˜¯å•é¡Œï¼Œèµ° QA è·¯å¾‘
    (lambda x: "?" in x.get("text", ""), 
     PromptTemplate.from_template("å›ç­”å•é¡Œ: {text}") | llm | parser),
    
    # æ¢ä»¶ï¼šå¦‚æœæ˜¯ç¿»è­¯è«‹æ±‚ï¼Œèµ°ç¿»è­¯è·¯å¾‘  
    (lambda x: "translate" in x.get("text", "").lower(),
     PromptTemplate.from_template("ç¿»è­¯: {text}") | llm | parser),
    
    # é»˜èªè·¯å¾‘ï¼šæ™®é€šå°è©±
    PromptTemplate.from_template("èŠå¤©: {text}") | llm | parser
)

result = branch.invoke({"text": "What is AI?"})
```

### 3. å‹•æ…‹è·¯ç”± (RunnableLambda)

```python
from langchain_core.runnables import RunnableLambda

def route_by_language(input_dict):
    text = input_dict["text"]
    if any(ord(char) > 127 for char in text):  # åŒ…å«ä¸­æ–‡
        return "chinese_chain"
    else:
        return "english_chain"

router = RunnableLambda(route_by_language)

# å®Œæ•´çš„è·¯ç”±éˆ
routing_chain = {
    "route": router,
    "input": RunnablePassthrough()
} | RunnableLambda(lambda x: chains[x["route"]].invoke(x["input"]))
```

## LCEL æœ€ä½³å¯¦è¸

### 1. çµ„åˆå¼è¨­è¨ˆ

```python
# æ§‹å»ºå¯é‡è¤‡ä½¿ç”¨çš„çµ„ä»¶
sentiment_analyzer = (
    PromptTemplate.from_template("åˆ†æä»¥ä¸‹æ–‡æœ¬çš„æƒ…æ„Ÿå‚¾å‘ï¼š{text}")
    | llm 
    | StrOutputParser()
)

summarizer = (
    PromptTemplate.from_template("ç¸½çµä»¥ä¸‹å…§å®¹ï¼š{text}")
    | llm 
    | StrOutputParser()
)

# çµ„åˆæˆå®Œæ•´æµç¨‹
content_processor = RunnableParallel({
    "sentiment": sentiment_analyzer,
    "summary": summarizer,
    "original": RunnablePassthrough()
})
```

### 2. éŒ¯èª¤è™•ç†èˆ‡é‡è©¦

```python
from langchain_core.runnables import RunnableRetry

# æ·»åŠ é‡è©¦æ©Ÿåˆ¶
resilient_chain = (
    prompt 
    | RunnableRetry(llm, max_attempt_number=3) 
    | parser
)
```

### 3. ç•°æ­¥åŸ·è¡Œæ”¯æŒ

```python
import asyncio

# æ‰€æœ‰ LCEL éˆéƒ½æ”¯æŒç•°æ­¥åŸ·è¡Œ
async def async_processing():
    result = await chain.ainvoke({"text": "Hello"})
    return result

# æ‰¹é‡è™•ç†
async def batch_processing(inputs):
    results = await chain.abatch(inputs)
    return results
```

## LCEL vs å‚³çµ± Chains å°æ¯”

| ç‰¹æ€§ | å‚³çµ± Chains | LCEL |
|------|------------|------|
| **èªæ³•** | é¡åˆ¥å¯¦ä¾‹åŒ– | ç®¡é“æ“ä½œç¬¦ `\|` |
| **çµ„åˆæ€§** | å›ºå®šçµæ§‹ | éˆæ´»çµ„åˆ |
| **å¹³è¡Œè™•ç†** | éœ€è¦é¡å¤–è¨­å®š | åŸç”Ÿæ”¯æŒ |
| **ç•°æ­¥æ”¯æŒ** | éƒ¨åˆ†æ”¯æŒ | å®Œæ•´æ”¯æŒ |
| **éŒ¯èª¤è™•ç†** | æ‰‹å‹•è™•ç† | å…§å»ºæ©Ÿåˆ¶ |
| **å¯è®€æ€§** | è¤‡é›œé…ç½® | ç›´è§€æµç¨‹ |
| **ç¶­è­·æ€§** | è¼ƒä½ | è¼ƒé«˜ |

## å¯¦éš›æ‡‰ç”¨ç¯„ä¾‹

### åŸºæœ¬æ–‡æœ¬è™•ç†éˆ

```python
from langchain_core.prompts import ChatPromptTemplate
from langchain_openai import ChatOpenAI
from langchain_core.output_parsers import StrOutputParser

# å»ºç«‹åŸºæœ¬è™•ç†éˆ
llm = ChatOpenAI(model="gpt-4o-mini")
prompt = ChatPromptTemplate.from_template("æ”¹å¯«ä»¥ä¸‹æ–‡æœ¬ï¼Œä½¿å…¶æ›´å°ˆæ¥­ï¼š\n{text}")
parser = StrOutputParser()

# LCEL éˆ
professional_writer = prompt | llm | parser

# ä½¿ç”¨
result = professional_writer.invoke({"text": "é€™å€‹ç”¢å“é‚„ä¸éŒ¯å•¦"})
print(result)  # "æ­¤ç”¢å“å…·æœ‰è‰¯å¥½çš„å“è³ªè¡¨ç¾"
```

### å¤šæ­¥é©Ÿåˆ†æéˆ

```python
# å¤šæ­¥é©Ÿå¥åº·åˆ†ææµç¨‹
health_prompt_1 = PromptTemplate.from_template("åˆ†æå¥åº·æ•¸æ“šï¼š{health_data}")
health_prompt_2 = PromptTemplate.from_template("åŸºæ–¼åˆ†æ {analysis} æä¾›å»ºè­°")
health_prompt_3 = PromptTemplate.from_template("æ ¼å¼åŒ–å»ºè­° {recommendation} ç‚ºå ±å‘Š")

# ä½¿ç”¨ RunnablePassthrough å‚³éæ•¸æ“š
from langchain_core.runnables import RunnablePassthrough

health_pipeline = (
    {"health_data": RunnablePassthrough()}
    | health_prompt_1
    | llm
    | {"analysis": StrOutputParser()}
    | health_prompt_2
    | llm 
    | {"recommendation": StrOutputParser()}
    | health_prompt_3
    | llm
    | StrOutputParser()
)

# ä½¿ç”¨ç¯„ä¾‹
result = health_pipeline.invoke("è¡€ç³–åé«˜ 130mg/dL")
print(result)
```

### æ¢ä»¶é‚è¼¯è™•ç†éˆ

```python
from langchain_core.runnables import RunnableBranch

# æ™ºèƒ½å®¢æœè·¯ç”±éˆ
def classify_intent(input_dict):
    text = input_dict["text"].lower()
    if any(word in text for word in ["é€€è²¨", "é€€æ¬¾", "æ›è²¨"]):
        return "refund"
    elif any(word in text for word in ["é€è²¨", "ç‰©æµ", "é…é€"]):
        return "shipping"
    elif any(word in text for word in ["æŠ€è¡“", "æ•…éšœ", "å•é¡Œ"]):
        return "technical"
    else:
        return "general"

intent_router = RunnableLambda(classify_intent)

# ä¸åŒæ„åœ–çš„è™•ç†éˆ
refund_chain = PromptTemplate.from_template(
    "è™•ç†é€€æ¬¾è«‹æ±‚ï¼š{text}\nè«‹æä¾›é€€æ¬¾æµç¨‹å’Œæ‰€éœ€è³‡æ–™"
) | llm | parser

shipping_chain = PromptTemplate.from_template(
    "è™•ç†ç‰©æµæŸ¥è©¢ï¼š{text}\nè«‹æä¾›ç‰©æµç‹€æ…‹å’Œé è¨ˆé€é”æ™‚é–“"
) | llm | parser

technical_chain = PromptTemplate.from_template(
    "è™•ç†æŠ€è¡“å•é¡Œï¼š{text}\nè«‹æä¾›æŠ€è¡“æ”¯æ´å’Œè§£æ±ºæ–¹æ¡ˆ"
) | llm | parser

general_chain = PromptTemplate.from_template(
    "ä¸€èˆ¬å®¢æœå›æ‡‰ï¼š{text}\nè«‹æä¾›å‹å–„ä¸”æœ‰å¹«åŠ©çš„å›ç­”"
) | llm | parser

# çµ„åˆæ™ºèƒ½å®¢æœç³»çµ±
customer_service = RunnableBranch(
    (lambda x: classify_intent(x) == "refund", refund_chain),
    (lambda x: classify_intent(x) == "shipping", shipping_chain),
    (lambda x: classify_intent(x) == "technical", technical_chain),
    general_chain  # é»˜èªè™•ç†
)

# æ¸¬è©¦ä¸åŒé¡å‹çš„å®¢æœè«‹æ±‚
queries = [
    {"text": "æˆ‘è¦é€€è²¨ï¼Œå•†å“æœ‰å•é¡Œ"},
    {"text": "æˆ‘çš„åŒ…è£¹ä»€éº¼æ™‚å€™æœƒåˆ°ï¼Ÿ"},
    {"text": "ç”¢å“ç„¡æ³•é–‹æ©Ÿï¼Œæ€éº¼è¾¦ï¼Ÿ"},
    {"text": "ä½ å€‘çš„ç‡Ÿæ¥­æ™‚é–“æ˜¯ï¼Ÿ"}
]

for query in queries:
    response = customer_service.invoke(query)
    print(f"å®¢æˆ¶ï¼š{query['text']}")
    print(f"å®¢æœï¼š{response}\n")
```

## æ€§èƒ½å„ªåŒ–æŠ€å·§

### 1. æ‰¹é‡è™•ç†å„ªåŒ–

```python
# å–®å€‹è™•ç† vs æ‰¹é‡è™•ç†
texts = ["æ–‡æœ¬1", "æ–‡æœ¬2", "æ–‡æœ¬3", "æ–‡æœ¬4", "æ–‡æœ¬5"]

# âŒ æ•ˆç‡è¼ƒä½ï¼šé€å€‹è™•ç†
results = []
for text in texts:
    result = chain.invoke({"text": text})
    results.append(result)

# âœ… é«˜æ•ˆï¼šæ‰¹é‡è™•ç†
results = chain.batch([{"text": text} for text in texts])
```

### 2. ç•°æ­¥ä¸¦è¡Œè™•ç†

```python
import asyncio

async def parallel_processing():
    tasks = [
        chain.ainvoke({"text": f"è™•ç†æ–‡æœ¬ {i}"}) 
        for i in range(10)
    ]
    
    # ä¸¦è¡ŒåŸ·è¡Œæ‰€æœ‰ä»»å‹™
    results = await asyncio.gather(*tasks)
    return results

# é‹è¡Œç•°æ­¥è™•ç†
results = asyncio.run(parallel_processing())
```

### 3. æµå¼è™•ç†

```python
# æµå¼è¼¸å‡ºï¼Œé©åˆé•·æ–‡æœ¬ç”Ÿæˆ
for chunk in chain.stream({"text": "å¯«ä¸€ç¯‡é•·æ–‡ç« é—œæ–¼ AI çš„æœªä¾†"}):
    print(chunk, end="", flush=True)
```

## èª¿è©¦å’Œç›£æ§

### 1. éˆçš„çµæ§‹æª¢æŸ¥

```python
# æª¢æŸ¥éˆçš„çµæ§‹
print("è¼¸å…¥ Schema:", chain.input_schema)
print("è¼¸å‡º Schema:", chain.output_schema)

# ç²å–éˆçš„é…ç½®è³‡è¨Š
print("éˆé…ç½®:", chain.config)
```

### 2. ä¸­é–“çµæœç›£æ§

```python
# æ·»åŠ ä¸­é–“æ­¥é©Ÿçš„è¼¸å‡ºç›£æ§
def debug_step(step_name):
    def _debug(x):
        print(f"[DEBUG] {step_name}: {x}")
        return x
    return RunnableLambda(_debug)

# åœ¨éˆä¸­æ’å…¥ç›£æ§é»
debug_chain = (
    prompt 
    | debug_step("Prompt è¼¸å‡º")
    | llm 
    | debug_step("LLM è¼¸å‡º")
    | parser
    | debug_step("æœ€çµ‚çµæœ")
)
```

## ç¸½çµ

LCEL è¡¨é”å¼èªè¨€æ˜¯ LangChain v0.2+ çš„æ ¸å¿ƒå‰µæ–°ï¼Œå®ƒæä¾›äº†ï¼š

- ğŸ”— **ç›´è§€çš„ç®¡é“èªæ³•** - ç”¨ `|` æ“ä½œç¬¦è¼•é¬†çµ„åˆçµ„ä»¶
- ğŸš€ **åŸç”Ÿç•°æ­¥æ”¯æŒ** - é«˜æ€§èƒ½çš„ä¸¦è¡Œè™•ç†èƒ½åŠ›
- ğŸ”„ **éˆæ´»çš„æ¢ä»¶é‚è¼¯** - æ”¯æŒè¤‡é›œçš„åˆ†æ”¯å’Œè·¯ç”±
- ğŸ›¡ï¸ **å…§å»ºéŒ¯èª¤è™•ç†** - è‡ªå‹•é‡è©¦å’Œç•°å¸¸æ¢å¾©
- ğŸ“Š **çµ±ä¸€çš„ä»‹é¢æŠ½è±¡** - æ‰€æœ‰çµ„ä»¶éƒ½éµå¾ªç›¸åŒçš„ Runnable å”è­°
- ğŸ” **æ˜“æ–¼èª¿è©¦ç›£æ§** - æ¸…æ™°çš„ä¸­é–“ç‹€æ…‹å’ŒåŸ·è¡Œè¿½è¹¤

æŒæ¡ LCEL æ˜¯ä½¿ç”¨ç¾ä»£ LangChain çš„é—œéµï¼Œå®ƒè®“ AI æ‡‰ç”¨çš„é–‹ç™¼è®Šå¾—æ›´åŠ ç°¡æ½”ã€å¼·å¤§å’Œå¯ç¶­è­·ã€‚

---

::: tip ä¸‹ä¸€æ­¥
ç¾åœ¨ä½ å·²ç¶“æŒæ¡äº† LCEL çš„æ ¸å¿ƒæ¦‚å¿µï¼Œæ¥ä¸‹ä¾†å¯ä»¥ï¼š
1. [LangGraph å·¥ä½œæµ](/tutorials/langgraph) - å­¸ç¿’æ›´è¤‡é›œçš„ç‹€æ…‹æ©Ÿå’Œå¤šä»£ç†å”ä½œ
2. [çµæ§‹åŒ–è¼¸å‡ºè§£æ](/tutorials/output-parsers) - åœ¨ LCEL ä¸­ä½¿ç”¨çµæ§‹åŒ–è¼¸å‡º
3. [è¨˜æ†¶æ©Ÿåˆ¶èˆ‡å°è©±ç®¡ç†](/tutorials/memory-systems) - çµåˆè¨˜æ†¶ç³»çµ±å»ºæ§‹å°è©±æ‡‰ç”¨
:::

::: warning å¯¦è¸å»ºè­°
- **å¾ç°¡å–®é–‹å§‹**ï¼šå…ˆæŒæ¡åŸºæœ¬çš„ `prompt | llm | parser` æ¨¡å¼
- **å–„ç”¨çµ„åˆ**ï¼šå°‡è¤‡é›œé‚è¼¯åˆ†è§£ç‚ºå¯é‡ç”¨çš„å°çµ„ä»¶
- **ç•°æ­¥å„ªå…ˆ**ï¼šåœ¨è™•ç†å¤§é‡æ•¸æ“šæ™‚å„ªå…ˆä½¿ç”¨ç•°æ­¥æ–¹æ³•
- **ç›£æ§èª¿è©¦**ï¼šåœ¨é–‹ç™¼éšæ®µå……åˆ†åˆ©ç”¨èª¿è©¦å’Œç›£æ§åŠŸèƒ½
:::