# ç¬¬ä¸€å€‹ LangChain æ‡‰ç”¨

æ­¡è¿ä¾†åˆ° LangChain çš„å¯¦éš›æ‡‰ç”¨æ•™å­¸ï¼åœ¨é€™å€‹ç« ç¯€ä¸­ï¼Œæˆ‘å€‘å°‡å»ºæ§‹ä¸€å€‹**æ™ºèƒ½è§’è‰²å•ç­”åŠ©æ‰‹**ï¼Œå±•ç¤º LangChain çš„æ ¸å¿ƒåƒ¹å€¼ï¼šå°‡è¤‡é›œçš„ AI äº’å‹•ç°¡åŒ–ç‚ºæ˜“æ–¼ç†è§£å’Œç¶­è­·çš„ç¨‹å¼ç¢¼ã€‚

## ğŸš€ å­¸ç¿’ç›®æ¨™

å®Œæˆæœ¬æ•™å­¸å¾Œï¼Œæ‚¨å°‡å­¸æœƒï¼š

- ğŸ”— **LangChain æ ¸å¿ƒæ¦‚å¿µ**ï¼šç†è§£ LLMã€Prompt Templates å’Œ Chains çš„æ•´åˆ
- ğŸ¤– **æœ¬åœ°æ¨¡å‹æ•´åˆ**ï¼šä½¿ç”¨ Ollama å»ºæ§‹å…è²»çš„ AI æ‡‰ç”¨
- ğŸ­ **è§’è‰²æ‰®æ¼”ç³»çµ±**ï¼šå¯¦ä½œå¯åˆ‡æ›è§’è‰²çš„æ™ºèƒ½åŠ©æ‰‹
- ğŸ› ï¸ **éŒ¯èª¤è™•ç†**ï¼šå»ºç«‹ç©©å¥çš„æ‡‰ç”¨ç¨‹å¼
- ğŸ§ª **æ¸¬è©¦å’Œé™¤éŒ¯**ï¼šç¢ºä¿ç¨‹å¼ç¢¼å“è³ª

## ğŸ“‹ å…ˆæ±ºæ¢ä»¶

åœ¨é–‹å§‹ä¹‹å‰ï¼Œè«‹ç¢ºä¿æ‚¨å·²ç¶“ï¼š

- âœ… é–±è®€ [LangChain ä»‹ç´¹](/tutorials/introduction)
- âœ… å®Œæˆ [ç’°å¢ƒè¨­ç½®](/tutorials/setup)
- âœ… å®‰è£ä¸¦ç†Ÿæ‚‰ [å…è²» LLM æ¨¡å‹æŒ‡å—](/tutorials/free-llm-models)
- âœ… Ollama æ­£åœ¨é‹è¡Œï¼Œä¸¦å·²ä¸‹è¼‰ `llama3.2:1b` æ¨¡å‹

::: tip å¿«é€Ÿæª¢æŸ¥
åœ¨çµ‚ç«¯åŸ·è¡Œ `ollama list` ç¢ºèªæ‚¨æœ‰å¯ç”¨çš„æ¨¡å‹ã€‚å¦‚æœæ²’æœ‰ï¼Œè«‹åŸ·è¡Œï¼š
```bash
ollama pull llama3.2:1b
```
:::

## ğŸ¯ æˆ‘å€‘è¦å»ºæ§‹ä»€éº¼ï¼Ÿ

æˆ‘å€‘å°‡å»ºæ§‹ä¸€å€‹**æ™ºèƒ½è§’è‰²å•ç­”åŠ©æ‰‹**ï¼Œç‰¹è‰²åŒ…æ‹¬ï¼š

- ğŸ­ **å¤šé‡è§’è‰²**ï¼šç¨‹å¼è€å¸«ã€ç¿»è­¯å“¡ã€ç”Ÿæ´»é¡§å•ç­‰
- ğŸ’¬ **è‡ªç„¶å°è©±**ï¼šä½¿ç”¨ LangChain çš„ Prompt Templates
- ğŸ  **æœ¬åœ°é‹è¡Œ**ï¼šå®Œå…¨å…è²»ï¼Œä¿è­·éš±ç§
- ğŸ”§ **æ˜“æ–¼æ“´å±•**ï¼šç‚ºå¾ŒçºŒæ•™å­¸ï¼ˆè¨˜æ†¶ã€RAGç­‰ï¼‰å¥ å®šåŸºç¤

## ğŸ“¦ ç’°å¢ƒæº–å‚™

### 1. å‰µå»ºå°ˆæ¡ˆç›®éŒ„

```bash
mkdir my-first-langchain-app
cd my-first-langchain-app
```

### 2. å‰µå»ºè™›æ“¬ç’°å¢ƒ

```bash
# å‰µå»ºè™›æ“¬ç’°å¢ƒ
python -m venv venv

# å•Ÿå‹•è™›æ“¬ç’°å¢ƒ
# Linux/macOS:
source venv/bin/activate
# Windows:
# venv\Scripts\activate
```

### 3. å®‰è£å¿…è¦å¥—ä»¶

```bash
# åŸºç¤ LangChain å¥—ä»¶
pip install langchain-ollama langchain-core

# Streamlitï¼ˆç”¨æ–¼ Web ä»‹é¢ï¼‰
pip install streamlit
```

### 4. å®‰è£å’Œè¨­ç½® Ollama

::: warning é‡è¦æ­¥é©Ÿ
å¦‚æœæ‚¨é‚„æœªå®‰è£ Ollamaï¼Œè«‹å…ˆå®Œæˆæ­¤æ­¥é©Ÿã€‚
:::

**å®‰è£ Ollama:**

### Linux/Ubuntu
```bash
curl -fsSL https://ollama.com/install.sh | sh
```

### WSL2 (æ¨è–¦)
```bash
# 1. æ›´æ–°å¥—ä»¶ç®¡ç†å™¨
sudo apt update

# 2. ä¸‹è¼‰ä¸¦å®‰è£ Ollama
curl -fsSL https://ollama.com/install.sh | sh

# 3. å°‡ Ollama æ·»åŠ åˆ°ç³»çµ±è·¯å¾‘ï¼ˆå¦‚æœéœ€è¦ï¼‰
export PATH=$PATH:/usr/local/bin

# 4. é©—è­‰å®‰è£
ollama --version
```

::: tip WSL2 ç‰¹åˆ¥æ³¨æ„
å¦‚æœé‡åˆ°æ¬Šé™å•é¡Œï¼Œå¯èƒ½éœ€è¦ï¼š
```bash
# é‡æ–°è¼‰å…¥ shell é…ç½®
source ~/.bashrc

# æˆ–é‡æ–°å•Ÿå‹• WSL2
# åœ¨ Windows PowerShell ä¸­åŸ·è¡Œï¼šwsl --shutdown
# ç„¶å¾Œé‡æ–°é–‹å•Ÿ WSL2
```
:::

### macOS
```bash
# ä½¿ç”¨ Homebrew
brew install ollama

# æˆ–ä¸‹è¼‰å®‰è£ç¨‹å¼
# å‰å¾€ https://ollama.com/download
```

### Windows
```powershell
# å‰å¾€ https://ollama.com/download ä¸‹è¼‰å®‰è£ç¨‹å¼
# åŸ·è¡Œ .exe æª”æ¡ˆé€²è¡Œå®‰è£

# æˆ–ä½¿ç”¨ PowerShell (ç®¡ç†å“¡æ¬Šé™)
winget install Ollama.Ollama
```

**å•Ÿå‹• Ollama æœå‹™:**

### Linux/WSL2
```bash
# æ–¹æ³• 1: å‰å°å•Ÿå‹•ï¼ˆé–‹ç™¼æ¸¬è©¦ç”¨ï¼‰
ollama serve

# æ–¹æ³• 2: èƒŒæ™¯å•Ÿå‹•ï¼ˆå»ºè­°ï¼‰
nohup ollama serve > ollama.log 2>&1 &

# æª¢æŸ¥æœå‹™ç‹€æ…‹
ps aux | grep ollama
```

::: warning WSL2 å¸¸è¦‹å•é¡Œ
å¦‚æœåœ¨ WSL2 ä¸­é‡åˆ°å•Ÿå‹•å•é¡Œï¼š

**å•é¡Œ 1: æœå‹™ç„¡æ³•å•Ÿå‹•**
```bash
# æª¢æŸ¥æ˜¯å¦æœ‰ç«¯å£ä½”ç”¨
sudo netstat -tulpn | grep 11434

# æ®ºæ­»ä½”ç”¨çš„é€²ç¨‹
sudo pkill -f ollama
```

**å•é¡Œ 2: æ¬Šé™å•é¡Œ**
```bash
# ç¢ºä¿ç”¨æˆ¶æœ‰åŸ·è¡Œæ¬Šé™
sudo chmod +x $(which ollama)

# æˆ–é‡æ–°å®‰è£
curl -fsSL https://ollama.com/install.sh | sh
```

**å•é¡Œ 3: è·¯å¾‘å•é¡Œ**
```bash
# ç¢ºèª Ollama å·²å®‰è£
which ollama

# å¦‚æœæ‰¾ä¸åˆ°ï¼Œæ·»åŠ åˆ° PATH
echo 'export PATH=$PATH:/usr/local/bin' >> ~/.bashrc
source ~/.bashrc
```
:::

### macOS
```bash
# å•Ÿå‹•æœå‹™
ollama serve

# æˆ–ä½¿ç”¨ brew services
brew services start ollama
```

### Windows
```powershell
# Windows é€šå¸¸æœƒè‡ªå‹•å•Ÿå‹•æœå‹™
# å¦‚éœ€æ‰‹å‹•å•Ÿå‹•ï¼Œåœ¨å‘½ä»¤æç¤ºå­—å…ƒä¸­åŸ·è¡Œï¼š
ollama serve
```

**ä¸‹è¼‰ llama3.2:1b æ¨¡å‹:**

```bash
# ä¸‹è¼‰æ¨¡å‹ï¼ˆé¦–æ¬¡ä½¿ç”¨éœ€è¦ä¸‹è¼‰ï¼Œç´„ 1.3GBï¼‰
ollama pull llama3.2:1b

# é©—è­‰æ¨¡å‹å·²ä¸‹è¼‰
ollama list
```

::: tip WSL2 ä¸‹è¼‰æ³¨æ„äº‹é …
åœ¨ WSL2 ä¸­ä¸‹è¼‰æ¨¡å‹æ™‚ï¼š

**ç¶²è·¯å•é¡Œæ’é™¤:**
```bash
# å¦‚æœä¸‹è¼‰ç·©æ…¢æˆ–å¤±æ•—ï¼Œå¯ä»¥è¨­å®šä»£ç†ï¼ˆå¦‚æœæœ‰ï¼‰
export https_proxy=http://proxy:port
export http_proxy=http://proxy:port

# æª¢æŸ¥ç¶²è·¯é€£æ¥
curl -I https://ollama.com
```

**å„²å­˜ç©ºé–“æª¢æŸ¥:**
```bash
# æª¢æŸ¥å¯ç”¨ç©ºé–“ï¼ˆæ¨¡å‹éœ€è¦ç´„ 5GBï¼‰
df -h

# æª¢æŸ¥ Ollama æ¨¡å‹å­˜æ”¾ä½ç½®
ls -la ~/.ollama/models/
```

**ä¸‹è¼‰é€²åº¦ç›£æ§:**
```bash
# åœ¨å¦ä¸€å€‹çµ‚ç«¯ç›£æ§ä¸‹è¼‰é€²åº¦
watch -n 2 "du -sh ~/.ollama/models/"
```
:::

::: tip æ¨¡å‹é¸æ“‡
å¦‚æœæ‚¨çš„é›»è…¦è¨˜æ†¶é«”è¼ƒå°ï¼Œå¯ä»¥é¸æ“‡è¼ƒå°çš„æ¨¡å‹ï¼š
- `llama3.2:1b` - ç´„ 1.3GBï¼Œé©åˆ 4GB RAMï¼ˆæ¨è–¦æ•™å­¸ä½¿ç”¨ï¼‰
- `llama3.2:3b` - ç´„ 2GBï¼Œé©åˆ 8GB RAM  
- `llama3.1:8b` - ç´„ 4.7GBï¼Œé©åˆ 16GB+ RAM
:::

### 5. é©—è­‰ Ollama é€£æ¥

å‰µå»º `test_ollama.py` ä¾†æ¸¬è©¦é€£æ¥ï¼š

```python
from langchain_ollama import OllamaLLM

def test_ollama_connection():
    try:
        llm = OllamaLLM(model="llama3.2:1b")
        response = llm.invoke("Hello, è«‹ç”¨ç¹é«”ä¸­æ–‡å›ç­”ï¼šä½ æ˜¯èª°ï¼Ÿ")
        print("âœ… Ollama é€£æ¥æˆåŠŸï¼")
        print(f"å›æ‡‰ï¼š{response}")
        return True
    except Exception as e:
        print(f"âŒ Ollama é€£æ¥å¤±æ•—ï¼š{e}")
        return False

if __name__ == "__main__":
    test_ollama_connection()
```

åŸ·è¡Œæ¸¬è©¦ï¼š
```bash
python test_ollama.py
```

## ğŸ—ï¸ æ­¥é©Ÿä¸€ï¼šåŸºç¤ LLM èª¿ç”¨

è®“æˆ‘å€‘å¾æœ€ç°¡å–®çš„ LLM èª¿ç”¨é–‹å§‹ï¼š

```python
# basic_llm.py
from langchain_ollama import OllamaLLM

def basic_llm_example():
    # åˆå§‹åŒ– Ollama LLM
    llm = OllamaLLM(model="llama3.2:1b")
    
    # ç°¡å–®èª¿ç”¨
    response = llm.invoke("è«‹ç”¨ç¹é«”ä¸­æ–‡è§£é‡‹ä»€éº¼æ˜¯äººå·¥æ™ºæ…§")
    print(response)

if __name__ == "__main__":
    basic_llm_example()
```

é€™å€‹ç¯„ä¾‹å±•ç¤ºäº†æœ€åŸºæœ¬çš„ LLM ä½¿ç”¨æ–¹å¼ï¼Œä½†é‚„ä¸ç®—æ˜¯çœŸæ­£çš„ LangChain æ‡‰ç”¨ã€‚

## ğŸ¨ æ­¥é©ŸäºŒï¼šåŠ å…¥ Prompt Template

ç¾åœ¨è®“æˆ‘å€‘ä½¿ç”¨ LangChain çš„ Prompt Template ä¾†å¢åŠ å½ˆæ€§ï¼š

```python
# prompt_template_example.py
from langchain_ollama import OllamaLLM
from langchain_core.prompts import PromptTemplate

def prompt_template_example():
    # åˆå§‹åŒ– LLM
    llm = OllamaLLM(model="llama3.2:1b")
    
    # å‰µå»º Prompt Template
    template = PromptTemplate.from_template(
        """ä½ æ˜¯ä¸€å€‹{role}ã€‚è«‹{task}ï¼š

å•é¡Œï¼š{question}

è«‹ç”¨ç¹é«”ä¸­æ–‡å›ç­”ï¼Œä¸¦ä¸”{style}ã€‚"""
    )
    
    # ä½¿ç”¨æ¨¡æ¿ç”Ÿæˆ prompt
    prompt = template.format(
        role="Python ç¨‹å¼è¨­è¨ˆè€å¸«",
        task="ç”¨ç°¡å–®æ˜“æ‡‚çš„æ–¹å¼è§£é‡‹",
        question="ä»€éº¼æ˜¯å‡½æ•¸ï¼Ÿ",
        style="åŒ…å«å¯¦éš›çš„ç¨‹å¼ç¢¼ç¯„ä¾‹"
    )
    
    print("ğŸ“ ç”Ÿæˆçš„ Promptï¼š")
    print(prompt)
    print("\n" + "="*50 + "\n")
    
    # èª¿ç”¨ LLM
    response = llm.invoke(prompt)
    print("ğŸ¤– AI å›æ‡‰ï¼š")
    print(response)

if __name__ == "__main__":
    prompt_template_example()
```

## â›“ï¸ æ­¥é©Ÿä¸‰ï¼šå»ºæ§‹ LangChain éˆ

é€™æ˜¯ LangChain çš„æ ¸å¿ƒå¨åŠ› - å°‡çµ„ä»¶éˆæ¥èµ·ä¾†ï¼š

```python
# langchain_chain_example.py
from langchain_ollama import OllamaLLM
from langchain_core.prompts import PromptTemplate

def langchain_chain_example():
    # åˆå§‹åŒ–çµ„ä»¶
    llm = OllamaLLM(model="llama3.2:1b")
    
    # å‰µå»º Prompt Template
    template = PromptTemplate.from_template(
        """ä½ æ˜¯ä¸€å€‹{role}ã€‚è«‹{task}ï¼š

å•é¡Œï¼š{question}

è«‹ç”¨ç¹é«”ä¸­æ–‡å›ç­”ï¼Œä¸¦ä¸”{style}ã€‚"""
    )
    
    # ğŸ”— å»ºæ§‹ LangChain éˆ
    chain = template | llm
    
    # ä½¿ç”¨éˆ
    response = chain.invoke({
        "role": "æ­·å²è€å¸«",
        "task": "è©³ç´°èªªæ˜",
        "question": "ç§¦å§‹çš‡çµ±ä¸€ä¸­åœ‹çš„é‡è¦æ€§",
        "style": "èˆ‰å‡ºå…·é«”çš„æ­·å²äº‹ä»¶å’Œå½±éŸ¿"
    })
    
    print("ğŸ”— ä½¿ç”¨ LangChain éˆçš„å›æ‡‰ï¼š")
    print(response)

if __name__ == "__main__":
    langchain_chain_example()
```

## ğŸ­ æ­¥é©Ÿå››ï¼šå®Œæ•´çš„è§’è‰²å•ç­”åŠ©æ‰‹

ç¾åœ¨è®“æˆ‘å€‘å»ºæ§‹å®Œæ•´çš„æ‡‰ç”¨ç¨‹å¼ï¼š

```python
# smart_assistant.py
from langchain_ollama import OllamaLLM
from langchain_core.prompts import PromptTemplate
from typing import Dict, Any
import time

class SmartAssistant:
    def __init__(self, model_name: str = "llama3.2:1b"):
        """åˆå§‹åŒ–æ™ºèƒ½åŠ©æ‰‹"""
        self.llm = OllamaLLM(model=model_name)
        self.setup_templates()
        
    def setup_templates(self):
        """è¨­ç½®ä¸åŒè§’è‰²çš„æ¨¡æ¿"""
        self.templates = {
            "ç¨‹å¼è€å¸«": PromptTemplate.from_template(
                """ä½ æ˜¯ä¸€å€‹ç¶“é©—è±å¯Œçš„ç¨‹å¼è¨­è¨ˆè€å¸«ã€‚è«‹ç”¨æ¸…æ¥šæ˜“æ‡‚çš„æ–¹å¼å›ç­”å­¸ç”Ÿçš„ç¨‹å¼å•é¡Œã€‚

å•é¡Œï¼š{question}

è«‹ç”¨ç¹é«”ä¸­æ–‡å›ç­”ï¼Œä¸¦ä¸”ï¼š
1. å…ˆè§£é‡‹æ¦‚å¿µ
2. æä¾›å¯¦éš›çš„ç¨‹å¼ç¢¼ç¯„ä¾‹
3. æŒ‡å‡ºå¸¸è¦‹çš„éŒ¯èª¤æˆ–æ³¨æ„äº‹é …
4. å»ºè­°é€²ä¸€æ­¥å­¸ç¿’çš„æ–¹å‘"""
            ),
            
            "ç¿»è­¯å“¡": PromptTemplate.from_template(
                """ä½ æ˜¯ä¸€å€‹å°ˆæ¥­çš„ç¿»è­¯å“¡ã€‚è«‹å”åŠ©ç¿»è­¯ä»¥ä¸‹å…§å®¹ã€‚

è¦ç¿»è­¯çš„å…§å®¹ï¼š{question}

è«‹æä¾›ï¼š
1. æº–ç¢ºçš„ç¿»è­¯
2. æ–‡æ„èªªæ˜ï¼ˆå¦‚æœéœ€è¦ï¼‰
3. ä½¿ç”¨æƒ…å¢ƒï¼ˆå¦‚æœæ˜¯ä¿šèªæˆ–å°ˆæ¥­è¡“èªï¼‰"""
            ),
            
            "ç”Ÿæ´»é¡§å•": PromptTemplate.from_template(
                """ä½ æ˜¯ä¸€å€‹æº«æš–é«”è²¼çš„ç”Ÿæ´»é¡§å•ã€‚è«‹é‡å°ä½¿ç”¨è€…çš„ç”Ÿæ´»å•é¡Œæä¾›å»ºè­°ã€‚

å•é¡Œï¼š{question}

è«‹ç”¨ç¹é«”ä¸­æ–‡å›ç­”ï¼Œä¸¦ä¸”ï¼š
1. è¡¨é”åŒç†å¿ƒ
2. æä¾›å¯¦ç”¨çš„å»ºè­°
3. åˆ†äº«ç›¸é—œçš„ç¶“é©—æˆ–çŸ¥è­˜
4. çµ¦äºˆæ­£é¢çš„é¼“å‹µ"""
            ),
            
            "å­¸ç¿’å¤¥ä¼´": PromptTemplate.from_template(
                """ä½ æ˜¯ä¸€å€‹å‹å–„çš„å­¸ç¿’å¤¥ä¼´ã€‚è«‹å”åŠ©è§£ç­”å­¸ç¿’ç›¸é—œçš„å•é¡Œã€‚

å•é¡Œï¼š{question}

è«‹ç”¨ç¹é«”ä¸­æ–‡å›ç­”ï¼Œä¸¦ä¸”ï¼š
1. åˆ†è§£è¤‡é›œçš„æ¦‚å¿µ
2. æä¾›è¨˜æ†¶æŠ€å·§æˆ–å­¸ç¿’æ–¹æ³•
3. èˆ‰å‡ºå¯¦éš›çš„ä¾‹å­
4. å»ºè­°ç·´ç¿’æ–¹å¼æˆ–è³‡æº"""
            )
        }
        
        # å»ºæ§‹éˆ
        self.chains = {
            role: template | self.llm 
            for role, template in self.templates.items()
        }
    
    def get_available_roles(self) -> list:
        """å–å¾—å¯ç”¨è§’è‰²åˆ—è¡¨"""
        return list(self.templates.keys())
    
    def ask(self, role: str, question: str) -> Dict[str, Any]:
        """å‘æŒ‡å®šè§’è‰²æå•"""
        if role not in self.chains:
            return {
                "success": False,
                "error": f"è§’è‰² '{role}' ä¸å­˜åœ¨ã€‚å¯ç”¨è§’è‰²ï¼š{', '.join(self.get_available_roles())}"
            }
        
        try:
            # è¨˜éŒ„é–‹å§‹æ™‚é–“
            start_time = time.time()
            
            # èª¿ç”¨å°æ‡‰çš„éˆ
            response = self.chains[role].invoke({"question": question})
            
            # è¨ˆç®—å›æ‡‰æ™‚é–“
            response_time = time.time() - start_time
            
            return {
                "success": True,
                "role": role,
                "question": question,
                "response": response,
                "response_time": f"{response_time:.2f}ç§’"
            }
            
        except Exception as e:
            return {
                "success": False,
                "error": f"ç™¼ç”ŸéŒ¯èª¤ï¼š{str(e)}"
            }

def main():
    """ä¸»ç¨‹å¼"""
    print("ğŸ­ æ™ºèƒ½è§’è‰²å•ç­”åŠ©æ‰‹")
    print("=" * 40)
    
    # åˆå§‹åŒ–åŠ©æ‰‹
    try:
        assistant = SmartAssistant()
        print("âœ… åŠ©æ‰‹åˆå§‹åŒ–æˆåŠŸï¼")
    except Exception as e:
        print(f"âŒ åˆå§‹åŒ–å¤±æ•—ï¼š{e}")
        return
    
    # é¡¯ç¤ºå¯ç”¨è§’è‰²
    roles = assistant.get_available_roles()
    print(f"\nğŸ“‹ å¯ç”¨è§’è‰²ï¼š{', '.join(roles)}")
    
    print("\nğŸ’¡ ä½¿ç”¨èªªæ˜ï¼š")
    print("- è¼¸å…¥ 'quit' çµæŸç¨‹å¼")
    print("- è¼¸å…¥ 'roles' æŸ¥çœ‹å¯ç”¨è§’è‰²")
    print("- æ ¼å¼ï¼š[è§’è‰²] æ‚¨çš„å•é¡Œ")
    print("- ç¯„ä¾‹ï¼šç¨‹å¼è€å¸« ä»€éº¼æ˜¯éè¿´ï¼Ÿ")
    
    while True:
        print("\n" + "-" * 40)
        user_input = input("ğŸ‘¤ è«‹è¼¸å…¥ï¼ˆè§’è‰² å•é¡Œï¼‰ï¼š").strip()
        
        # è™•ç†ç‰¹æ®ŠæŒ‡ä»¤
        if user_input.lower() == 'quit':
            print("ğŸ‘‹ å†è¦‹ï¼")
            break
        elif user_input.lower() == 'roles':
            print(f"ğŸ“‹ å¯ç”¨è§’è‰²ï¼š{', '.join(roles)}")
            continue
        elif not user_input:
            print("â“ è«‹è¼¸å…¥å•é¡Œ")
            continue
        
        # è§£æè¼¸å…¥
        parts = user_input.split(' ', 1)
        if len(parts) < 2:
            print("â“ è«‹ä½¿ç”¨æ ¼å¼ï¼š[è§’è‰²] [å•é¡Œ]")
            continue
        
        role, question = parts[0], parts[1]
        
        # æå•
        print(f"\nğŸ¤” å‘ {role} æå•ï¼š{question}")
        print("â³ æ€è€ƒä¸­...")
        
        result = assistant.ask(role, question)
        
        if result["success"]:
            print(f"\nğŸ­ {result['role']} çš„å›ç­”ï¼š")
            print("-" * 30)
            print(result["response"])
            print(f"\nâ±ï¸  å›æ‡‰æ™‚é–“ï¼š{result['response_time']}")
        else:
            print(f"\nâŒ éŒ¯èª¤ï¼š{result['error']}")

if __name__ == "__main__":
    main()
```

## ğŸ› ï¸ æ­¥é©Ÿäº”ï¼šå¢å¼·éŒ¯èª¤è™•ç†

è®“æˆ‘å€‘ç‚ºæ‡‰ç”¨ç¨‹å¼åŠ å…¥æ›´å®Œå–„çš„éŒ¯èª¤è™•ç†ï¼š

```python
# enhanced_assistant.py
from langchain_ollama import OllamaLLM
from langchain_core.prompts import PromptTemplate
from typing import Dict, Any, Optional
import time
import logging
from pathlib import Path

# è¨­ç½®æ—¥èªŒ
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s',
    handlers=[
        logging.FileHandler('assistant.log'),
        logging.StreamHandler()
    ]
)

class EnhancedSmartAssistant:
    def __init__(self, model_name: str = "llama3.2:1b", timeout: int = 60):
        """åˆå§‹åŒ–å¢å¼·ç‰ˆæ™ºèƒ½åŠ©æ‰‹"""
        self.model_name = model_name
        self.timeout = timeout
        self.llm = None
        self.chains = {}
        
        # åˆå§‹åŒ–
        if not self._check_ollama_availability():
            raise ConnectionError("Ollama æœå‹™ä¸å¯ç”¨ï¼Œè«‹ç¢ºèªå·²å®‰è£ä¸¦å•Ÿå‹• Ollama")
        
        self._initialize_llm()
        self._setup_templates()
        
        logging.info(f"åŠ©æ‰‹åˆå§‹åŒ–å®Œæˆï¼Œä½¿ç”¨æ¨¡å‹ï¼š{model_name}")
    
    def _check_ollama_availability(self) -> bool:
        """æª¢æŸ¥ Ollama æ˜¯å¦å¯ç”¨"""
        try:
            import subprocess
            result = subprocess.run(['ollama', 'list'], 
                                  capture_output=True, text=True, timeout=10)
            return result.returncode == 0
        except (subprocess.TimeoutExpired, FileNotFoundError, Exception) as e:
            logging.error(f"æª¢æŸ¥ Ollama å¯ç”¨æ€§æ™‚ç™¼ç”ŸéŒ¯èª¤ï¼š{e}")
            return False
    
    def _initialize_llm(self):
        """åˆå§‹åŒ– LLM"""
        try:
            self.llm = OllamaLLM(
                model=self.model_name,
                timeout=self.timeout
            )
            
            # æ¸¬è©¦é€£æ¥
            test_response = self.llm.invoke("Hello", timeout=10)
            logging.info("LLM é€£æ¥æ¸¬è©¦æˆåŠŸ")
            
        except Exception as e:
            logging.error(f"LLM åˆå§‹åŒ–å¤±æ•—ï¼š{e}")
            raise ConnectionError(f"ç„¡æ³•åˆå§‹åŒ– LLMï¼š{e}")
    
    def _setup_templates(self):
        """è¨­ç½®æ¨¡æ¿å’Œéˆ"""
        templates = {
            "ç¨‹å¼è€å¸«": """ä½ æ˜¯ä¸€å€‹ç¶“é©—è±å¯Œçš„ç¨‹å¼è¨­è¨ˆè€å¸«ã€‚è«‹ç”¨æ¸…æ¥šæ˜“æ‡‚çš„æ–¹å¼å›ç­”å­¸ç”Ÿçš„ç¨‹å¼å•é¡Œã€‚

å•é¡Œï¼š{question}

è«‹ç”¨ç¹é«”ä¸­æ–‡å›ç­”ï¼Œä¸¦ä¸”ï¼š
1. å…ˆè§£é‡‹æ¦‚å¿µ
2. æä¾›å¯¦éš›çš„ç¨‹å¼ç¢¼ç¯„ä¾‹  
3. æŒ‡å‡ºå¸¸è¦‹çš„éŒ¯èª¤æˆ–æ³¨æ„äº‹é …
4. å»ºè­°é€²ä¸€æ­¥å­¸ç¿’çš„æ–¹å‘""",

            "ç¿»è­¯å“¡": """ä½ æ˜¯ä¸€å€‹å°ˆæ¥­çš„ç¿»è­¯å“¡ã€‚è«‹å”åŠ©ç¿»è­¯ä»¥ä¸‹å…§å®¹ã€‚

è¦ç¿»è­¯çš„å…§å®¹ï¼š{question}

è«‹æä¾›ï¼š
1. æº–ç¢ºçš„ç¿»è­¯
2. æ–‡æ„èªªæ˜ï¼ˆå¦‚æœéœ€è¦ï¼‰
3. ä½¿ç”¨æƒ…å¢ƒï¼ˆå¦‚æœæ˜¯ä¿šèªæˆ–å°ˆæ¥­è¡“èªï¼‰""",

            "ç”Ÿæ´»é¡§å•": """ä½ æ˜¯ä¸€å€‹æº«æš–é«”è²¼çš„ç”Ÿæ´»é¡§å•ã€‚è«‹é‡å°ä½¿ç”¨è€…çš„ç”Ÿæ´»å•é¡Œæä¾›å»ºè­°ã€‚

å•é¡Œï¼š{question}

è«‹ç”¨ç¹é«”ä¸­æ–‡å›ç­”ï¼Œä¸¦ä¸”ï¼š
1. è¡¨é”åŒç†å¿ƒ
2. æä¾›å¯¦ç”¨çš„å»ºè­°
3. åˆ†äº«ç›¸é—œçš„ç¶“é©—æˆ–çŸ¥è­˜
4. çµ¦äºˆæ­£é¢çš„é¼“å‹µ"""
        }
        
        try:
            self.chains = {}
            for role, template_str in templates.items():
                template = PromptTemplate.from_template(template_str)
                self.chains[role] = template | self.llm
                
            logging.info(f"æˆåŠŸè¨­ç½® {len(self.chains)} å€‹è§’è‰²")
            
        except Exception as e:
            logging.error(f"è¨­ç½®æ¨¡æ¿æ™‚ç™¼ç”ŸéŒ¯èª¤ï¼š{e}")
            raise ValueError(f"æ¨¡æ¿è¨­ç½®å¤±æ•—ï¼š{e}")
    
    def get_available_roles(self) -> list:
        """å–å¾—å¯ç”¨è§’è‰²åˆ—è¡¨"""
        return list(self.chains.keys())
    
    def ask(self, role: str, question: str) -> Dict[str, Any]:
        """å‘æŒ‡å®šè§’è‰²æå•ï¼ˆå«éŒ¯èª¤è™•ç†ï¼‰"""
        # è¼¸å…¥é©—è­‰
        if not role or not question:
            return {
                "success": False,
                "error": "è§’è‰²å’Œå•é¡Œéƒ½ä¸èƒ½ç‚ºç©º"
            }
        
        if role not in self.chains:
            return {
                "success": False,
                "error": f"è§’è‰² '{role}' ä¸å­˜åœ¨ã€‚å¯ç”¨è§’è‰²ï¼š{', '.join(self.get_available_roles())}"
            }
        
        # å•é¡Œé•·åº¦æª¢æŸ¥
        if len(question) > 1000:
            return {
                "success": False,
                "error": "å•é¡Œéé•·ï¼Œè«‹æ§åˆ¶åœ¨1000å­—ä»¥å…§"
            }
        
        try:
            logging.info(f"è™•ç†å•é¡Œ - è§’è‰²ï¼š{role}ï¼Œå•é¡Œé•·åº¦ï¼š{len(question)}")
            start_time = time.time()
            
            # èª¿ç”¨éˆ
            response = self.chains[role].invoke(
                {"question": question.strip()},
                config={"timeout": self.timeout}
            )
            
            response_time = time.time() - start_time
            
            # å›æ‡‰é©—è­‰
            if not response or len(response.strip()) == 0:
                return {
                    "success": False,
                    "error": "æ¨¡å‹å›æ‡‰ç‚ºç©ºï¼Œè«‹é‡è©¦"
                }
            
            result = {
                "success": True,
                "role": role,
                "question": question,
                "response": response.strip(),
                "response_time": f"{response_time:.2f}ç§’",
                "model": self.model_name
            }
            
            logging.info(f"æˆåŠŸè™•ç†å•é¡Œï¼Œå›æ‡‰æ™‚é–“ï¼š{response_time:.2f}ç§’")
            return result
            
        except TimeoutError:
            error_msg = f"è«‹æ±‚è¶…æ™‚ï¼ˆ{self.timeout}ç§’ï¼‰ï¼Œè«‹é‡è©¦"
            logging.error(error_msg)
            return {"success": False, "error": error_msg}
            
        except ConnectionError as e:
            error_msg = f"é€£æ¥éŒ¯èª¤ï¼š{str(e)}"
            logging.error(error_msg)
            return {"success": False, "error": error_msg}
            
        except Exception as e:
            error_msg = f"æœªé æœŸçš„éŒ¯èª¤ï¼š{str(e)}"
            logging.error(error_msg)
            return {"success": False, "error": error_msg}
    
    def health_check(self) -> Dict[str, Any]:
        """å¥åº·æª¢æŸ¥"""
        try:
            start_time = time.time()
            test_response = self.llm.invoke("Hello", timeout=10)
            response_time = time.time() - start_time
            
            return {
                "status": "healthy",
                "model": self.model_name,
                "response_time": f"{response_time:.2f}ç§’",
                "available_roles": len(self.chains)
            }
        except Exception as e:
            return {
                "status": "unhealthy",
                "error": str(e)
            }

def main():
    """ä¸»ç¨‹å¼ï¼ˆå«éŒ¯èª¤è™•ç†ï¼‰"""
    print("ğŸ­ å¢å¼·ç‰ˆæ™ºèƒ½è§’è‰²å•ç­”åŠ©æ‰‹")
    print("=" * 50)
    
    try:
        # åˆå§‹åŒ–åŠ©æ‰‹
        print("â³ æ­£åœ¨åˆå§‹åŒ–åŠ©æ‰‹...")
        assistant = EnhancedSmartAssistant()
        print("âœ… åŠ©æ‰‹åˆå§‹åŒ–æˆåŠŸï¼")
        
        # å¥åº·æª¢æŸ¥
        health = assistant.health_check()
        if health["status"] == "healthy":
            print(f"ğŸ’š ç³»çµ±ç‹€æ…‹ï¼šæ­£å¸¸ï¼ˆå›æ‡‰æ™‚é–“ï¼š{health['response_time']}ï¼‰")
        else:
            print(f"âŒ ç³»çµ±ç‹€æ…‹ï¼šç•°å¸¸ - {health['error']}")
            return
            
    except Exception as e:
        print(f"âŒ åˆå§‹åŒ–å¤±æ•—ï¼š{e}")
        print("\nğŸ”§ è«‹æª¢æŸ¥ï¼š")
        print("1. Ollama æ˜¯å¦å·²å®‰è£ä¸¦å•Ÿå‹•")
        print("2. æ˜¯å¦å·²ä¸‹è¼‰ llama3.2:1b æ¨¡å‹ï¼ˆollama pull llama3.2:1bï¼‰")
        print("3. ç¶²è·¯é€£æ¥æ˜¯å¦æ­£å¸¸")
        return
    
    # é¡¯ç¤ºä½¿ç”¨èªªæ˜
    roles = assistant.get_available_roles()
    print(f"\nğŸ“‹ å¯ç”¨è§’è‰²ï¼š{', '.join(roles)}")
    print(f"ğŸ¤– ä½¿ç”¨æ¨¡å‹ï¼š{assistant.model_name}")
    
    print("\nğŸ’¡ ä½¿ç”¨èªªæ˜ï¼š")
    print("- è¼¸å…¥ 'quit' çµæŸç¨‹å¼")
    print("- è¼¸å…¥ 'roles' æŸ¥çœ‹å¯ç”¨è§’è‰²") 
    print("- è¼¸å…¥ 'health' æª¢æŸ¥ç³»çµ±ç‹€æ…‹")
    print("- æ ¼å¼ï¼š[è§’è‰²] æ‚¨çš„å•é¡Œ")
    print("- ç¯„ä¾‹ï¼šç¨‹å¼è€å¸« ä»€éº¼æ˜¯éè¿´ï¼Ÿ")
    
    while True:
        try:
            print("\n" + "-" * 50)
            user_input = input("ğŸ‘¤ è«‹è¼¸å…¥ï¼ˆè§’è‰² å•é¡Œï¼‰ï¼š").strip()
            
            # è™•ç†ç‰¹æ®ŠæŒ‡ä»¤
            if user_input.lower() == 'quit':
                print("ğŸ‘‹ å†è¦‹ï¼")
                break
            elif user_input.lower() == 'roles':
                print(f"ğŸ“‹ å¯ç”¨è§’è‰²ï¼š{', '.join(roles)}")
                continue
            elif user_input.lower() == 'health':
                health = assistant.health_check()
                print(f"ğŸ’š ç³»çµ±ç‹€æ…‹ï¼š{health}")
                continue
            elif not user_input:
                print("â“ è«‹è¼¸å…¥å•é¡Œ")
                continue
            
            # è§£æè¼¸å…¥
            parts = user_input.split(' ', 1)
            if len(parts) < 2:
                print("â“ è«‹ä½¿ç”¨æ ¼å¼ï¼š[è§’è‰²] [å•é¡Œ]")
                print(f"   å¯ç”¨è§’è‰²ï¼š{', '.join(roles)}")
                continue
            
            role, question = parts[0], parts[1]
            
            # æå•
            print(f"\nğŸ¤” å‘ {role} æå•ï¼š{question}")
            print("â³ æ€è€ƒä¸­...")
            
            result = assistant.ask(role, question)
            
            if result["success"]:
                print(f"\nğŸ­ {result['role']} çš„å›ç­”ï¼š")
                print("-" * 40)
                print(result["response"])
                print(f"\nâ±ï¸  å›æ‡‰æ™‚é–“ï¼š{result['response_time']}")
                print(f"ğŸ¤– æ¨¡å‹ï¼š{result['model']}")
            else:
                print(f"\nâŒ éŒ¯èª¤ï¼š{result['error']}")
                
        except KeyboardInterrupt:
            print("\n\nğŸ‘‹ ç¨‹å¼å·²ä¸­æ–·ï¼Œå†è¦‹ï¼")
            break
        except Exception as e:
            print(f"\nâŒ æœªé æœŸçš„éŒ¯èª¤ï¼š{e}")
            logging.error(f"ä¸»ç¨‹å¼éŒ¯èª¤ï¼š{e}")

if __name__ == "__main__":
    main()
```

## ğŸŒ æ­¥é©Ÿå…­ï¼šStreamlit Web ä»‹é¢

è®“æˆ‘å€‘ç‚ºåŠ©æ‰‹å»ºç«‹ä¸€å€‹å‹å–„çš„ Web ä»‹é¢ï¼š

```python
# streamlit_assistant.py
import streamlit as st
from enhanced_assistant import EnhancedSmartAssistant
import time
import logging

# è¨­ç½®é é¢
st.set_page_config(
    page_title="æ™ºèƒ½è§’è‰²å•ç­”åŠ©æ‰‹",
    page_icon="ğŸ­",
    layout="wide",
    initial_sidebar_state="expanded"
)

# åˆå§‹åŒ– session state
if 'assistant' not in st.session_state:
    st.session_state.assistant = None
if 'chat_history' not in st.session_state:
    st.session_state.chat_history = []
if 'current_role' not in st.session_state:
    st.session_state.current_role = "ç¨‹å¼è€å¸«"

def initialize_assistant():
    """åˆå§‹åŒ–åŠ©æ‰‹"""
    try:
        with st.spinner('æ­£åœ¨åˆå§‹åŒ–åŠ©æ‰‹...'):
            assistant = EnhancedSmartAssistant()
            return assistant
    except Exception as e:
        st.error(f"åˆå§‹åŒ–å¤±æ•—ï¼š{e}")
        st.info("è«‹ç¢ºèªï¼š\n1. Ollama å·²å®‰è£ä¸¦å•Ÿå‹•\n2. å·²ä¸‹è¼‰ llama3.1 æ¨¡å‹\n3. ç¶²è·¯é€£æ¥æ­£å¸¸")
        return None

def main():
    """ä¸»æ‡‰ç”¨ç¨‹å¼"""
    # æ¨™é¡Œ
    st.title("ğŸ­ æ™ºèƒ½è§’è‰²å•ç­”åŠ©æ‰‹")
    st.markdown("---")
    
    # å´é‚Šæ¬„
    with st.sidebar:
        st.header("âš™ï¸ è¨­å®š")
        
        # åˆå§‹åŒ–æŒ‰éˆ•
        if st.button("ğŸš€ åˆå§‹åŒ–åŠ©æ‰‹", type="primary"):
            st.session_state.assistant = initialize_assistant()
        
        # é¡¯ç¤ºç³»çµ±ç‹€æ…‹
        if st.session_state.assistant:
            with st.expander("ğŸ’š ç³»çµ±ç‹€æ…‹", expanded=True):
                health = st.session_state.assistant.health_check()
                if health["status"] == "healthy":
                    st.success(f"âœ… æ­£å¸¸é‹è¡Œ\nå›æ‡‰æ™‚é–“ï¼š{health['response_time']}")
                    st.info(f"ğŸ¤– æ¨¡å‹ï¼š{health.get('model', 'N/A')}\nè§’è‰²æ•¸é‡ï¼š{health.get('available_roles', 0)}")
                else:
                    st.error(f"âŒ ç•°å¸¸ç‹€æ…‹\néŒ¯èª¤ï¼š{health.get('error', 'Unknown')}")
            
            # è§’è‰²é¸æ“‡
            st.markdown("### ğŸ­ é¸æ“‡è§’è‰²")
            roles = st.session_state.assistant.get_available_roles()
            st.session_state.current_role = st.selectbox(
                "è«‹é¸æ“‡è¦å°è©±çš„è§’è‰²ï¼š",
                roles,
                index=roles.index(st.session_state.current_role) if st.session_state.current_role in roles else 0
            )
            
            # è§’è‰²èªªæ˜
            role_descriptions = {
                "ç¨‹å¼è€å¸«": "ğŸ’» å°ˆç²¾ç¨‹å¼è¨­è¨ˆæ•™å­¸ï¼Œæä¾›æ¸…æ¥šçš„æ¦‚å¿µè§£é‡‹å’Œç¨‹å¼ç¢¼ç¯„ä¾‹",
                "ç¿»è­¯å“¡": "ğŸŒ å°ˆæ¥­ç¿»è­¯æœå‹™ï¼Œæä¾›æº–ç¢ºç¿»è­¯å’Œæ–‡åŒ–èƒŒæ™¯èªªæ˜",
                "ç”Ÿæ´»é¡§å•": "ğŸ’¡ æº«æš–çš„ç”Ÿæ´»å»ºè­°ï¼Œæä¾›å¯¦ç”¨çš„è§£æ±ºæ–¹æ¡ˆå’Œæ­£é¢é¼“å‹µ"
            }
            
            if st.session_state.current_role in role_descriptions:
                st.info(role_descriptions[st.session_state.current_role])
            
            # æ¸…é™¤å°è©±æ­·å²
            if st.button("ğŸ—‘ï¸ æ¸…é™¤å°è©±æ­·å²"):
                st.session_state.chat_history = []
                st.rerun()
        
        else:
            st.warning("âš ï¸ è«‹å…ˆåˆå§‹åŒ–åŠ©æ‰‹")
    
    # ä¸»è¦å…§å®¹å€åŸŸ
    if st.session_state.assistant:
        # å°è©±å€åŸŸ
        st.header(f"ğŸ’¬ èˆ‡ {st.session_state.current_role} å°è©±")
        
        # é¡¯ç¤ºå°è©±æ­·å²
        chat_container = st.container()
        with chat_container:
            for i, chat in enumerate(st.session_state.chat_history):
                # ç”¨æˆ¶å•é¡Œ
                with st.chat_message("user"):
                    st.write(f"**[{chat['role']}]** {chat['question']}")
                
                # AI å›æ‡‰
                with st.chat_message("assistant"):
                    st.write(chat['response'])
                    
                    # é¡¯ç¤ºè©³ç´°è³‡è¨Š
                    col1, col2, col3 = st.columns(3)
                    with col1:
                        st.caption(f"â±ï¸ {chat['response_time']}")
                    with col2:
                        st.caption(f"ğŸ¤– {chat.get('model', 'N/A')}")
                    with col3:
                        st.caption(f"ğŸ“… {chat.get('timestamp', '')}")
        
        # è¼¸å…¥å€åŸŸ
        st.markdown("### ğŸ’­ è«‹è¼¸å…¥æ‚¨çš„å•é¡Œ")
        
        with st.form("question_form", clear_on_submit=True):
            question = st.text_area(
                "å•é¡Œå…§å®¹ï¼š",
                placeholder=f"è«‹å‘ {st.session_state.current_role} æå•...",
                help="è¼¸å…¥æ‚¨æƒ³è¦è©¢å•çš„å•é¡Œï¼Œæœ€å¤š 1000 å­—",
                max_chars=1000
            )
            
            col1, col2 = st.columns([1, 4])
            with col1:
                submit_button = st.form_submit_button("ğŸš€ æå•", type="primary")
            with col2:
                if st.form_submit_button("ğŸ’¡ ç¯„ä¾‹å•é¡Œ"):
                    example_questions = {
                        "ç¨‹å¼è€å¸«": ["ä»€éº¼æ˜¯éè¿´ï¼Ÿ", "å¦‚ä½•ä½¿ç”¨è¿´åœˆï¼Ÿ", "è§£é‡‹ç‰©ä»¶å°å‘ç¨‹å¼è¨­è¨ˆ"],
                        "ç¿»è­¯å“¡": ["Hello World", "ç¿»è­¯é€™å¥è©±ï¼šThe quick brown fox", "What does 'serendipity' mean?"],
                        "ç”Ÿæ´»é¡§å•": ["å¦‚ä½•ç®¡ç†æ™‚é–“ï¼Ÿ", "æ€éº¼æé«˜å·¥ä½œæ•ˆç‡ï¼Ÿ", "å¦‚ä½•è™•ç†å£“åŠ›ï¼Ÿ"]
                    }
                    
                    examples = example_questions.get(st.session_state.current_role, [])
                    if examples:
                        st.info(f"ğŸ’¡ {st.session_state.current_role} ç¯„ä¾‹å•é¡Œï¼š\n" + "\n".join([f"â€¢ {q}" for q in examples]))
        
        # è™•ç†æå•
        if submit_button and question.strip():
            with st.spinner(f'ğŸ¤” {st.session_state.current_role} æ­£åœ¨æ€è€ƒä¸­...'):
                result = st.session_state.assistant.ask(st.session_state.current_role, question)
                
                if result["success"]:
                    # æ·»åŠ åˆ°å°è©±æ­·å²
                    chat_entry = {
                        "role": result["role"],
                        "question": result["question"],
                        "response": result["response"],
                        "response_time": result["response_time"],
                        "model": result.get("model", "N/A"),
                        "timestamp": time.strftime("%H:%M:%S")
                    }
                    st.session_state.chat_history.append(chat_entry)
                    
                    # æˆåŠŸæç¤º
                    st.success(f"âœ… {result['role']} å·²å›ç­”ï¼å›æ‡‰æ™‚é–“ï¼š{result['response_time']}")
                    
                    # é‡æ–°é‹è¡Œä»¥æ›´æ–°å°è©±é¡¯ç¤º
                    st.rerun()
                else:
                    st.error(f"âŒ éŒ¯èª¤ï¼š{result['error']}")
        
        elif submit_button and not question.strip():
            st.warning("âš ï¸ è«‹è¼¸å…¥å•é¡Œå…§å®¹")
        
        # çµ±è¨ˆè³‡è¨Š
        if st.session_state.chat_history:
            st.markdown("---")
            with st.expander("ğŸ“Š å°è©±çµ±è¨ˆ", expanded=False):
                total_questions = len(st.session_state.chat_history)
                role_stats = {}
                
                for chat in st.session_state.chat_history:
                    role = chat['role']
                    role_stats[role] = role_stats.get(role, 0) + 1
                
                col1, col2 = st.columns(2)
                with col1:
                    st.metric("ç¸½å•é¡Œæ•¸", total_questions)
                with col2:
                    avg_time = sum(float(chat['response_time'].replace('ç§’', '')) 
                                 for chat in st.session_state.chat_history) / total_questions
                    st.metric("å¹³å‡å›æ‡‰æ™‚é–“", f"{avg_time:.2f}ç§’")
                
                st.markdown("**å„è§’è‰²ä½¿ç”¨æ¬¡æ•¸ï¼š**")
                for role, count in role_stats.items():
                    st.write(f"â€¢ {role}: {count} æ¬¡")
    
    else:
        # æ­¡è¿ç•«é¢
        st.markdown("""
        ## ğŸ‘‹ æ­¡è¿ä½¿ç”¨æ™ºèƒ½è§’è‰²å•ç­”åŠ©æ‰‹ï¼
        
        é€™æ˜¯ä¸€å€‹åŸºæ–¼ LangChain å’Œ Ollama çš„ AI åŠ©æ‰‹ï¼Œæä¾›å¤šç¨®è§’è‰²çš„å°ˆæ¥­æœå‹™ï¼š
        
        ### ğŸ­ å¯ç”¨è§’è‰²
        - **ğŸ’» ç¨‹å¼è€å¸«**ï¼šç¨‹å¼è¨­è¨ˆæ•™å­¸å’ŒæŠ€è¡“å•é¡Œè§£ç­”
        - **ğŸŒ ç¿»è­¯å“¡**ï¼šå¤šèªè¨€ç¿»è­¯å’Œæ–‡åŒ–èƒŒæ™¯èªªæ˜  
        - **ğŸ’¡ ç”Ÿæ´»é¡§å•**ï¼šç”Ÿæ´»å»ºè­°å’Œå•é¡Œè§£æ±ºæ–¹æ¡ˆ
        
        ### ğŸš€ é–‹å§‹ä½¿ç”¨
        1. é»æ“Šå·¦å´çš„ã€ŒğŸš€ åˆå§‹åŒ–åŠ©æ‰‹ã€æŒ‰éˆ•
        2. ç­‰å¾…ç³»çµ±åˆå§‹åŒ–å®Œæˆ
        3. é¸æ“‡æ‚¨æƒ³å°è©±çš„è§’è‰²
        4. é–‹å§‹æå•ï¼
        
        ### ğŸ’¡ ä½¿ç”¨æç¤º
        - å•é¡Œæœ€å¤š 1000 å­—
        - å¯ä»¥éš¨æ™‚åˆ‡æ›è§’è‰²
        - æ”¯æ´å°è©±æ­·å²è¨˜éŒ„
        - å¯ä»¥æŸ¥çœ‹å›æ‡‰æ™‚é–“å’Œçµ±è¨ˆè³‡è¨Š
        """)
        
        # ç³»çµ±éœ€æ±‚
        with st.expander("ğŸ”§ ç³»çµ±éœ€æ±‚", expanded=False):
            st.markdown("""
            ### å¿…è¦æ¢ä»¶
            - âœ… Python 3.8+
            - âœ… å·²å®‰è£ Ollama
            - âœ… å·²ä¸‹è¼‰ llama3.1 æ¨¡å‹
            - âœ… å·²å®‰è£å¿…è¦çš„ Python å¥—ä»¶
            
            ### å®‰è£æŒ‡ä»¤
            ```bash
            # å®‰è£ LangChain å¥—ä»¶
            pip install langchain-ollama langchain-core streamlit
            
            # å®‰è£ Ollama
            curl -fsSL https://ollama.com/install.sh | sh
            
            # ä¸‹è¼‰æ¨¡å‹ï¼ˆæ¨è–¦æ•™å­¸ä½¿ç”¨ï¼‰
            ollama pull llama3.2:1b
            
            # å•Ÿå‹•åŠ©æ‰‹
            streamlit run streamlit_assistant.py
            ```
            """)

if __name__ == "__main__":
    main()
```

å•Ÿå‹• Streamlit æ‡‰ç”¨ï¼š

```bash
# ç¢ºä¿åœ¨è™›æ“¬ç’°å¢ƒä¸­
source venv/bin/activate  # Linux/macOS
# æˆ– venv\Scripts\activate  # Windows

# å•Ÿå‹• Streamlit æ‡‰ç”¨
streamlit run streamlit_assistant.py
```

### ğŸ¨ Streamlit ä»‹é¢ç‰¹è‰²

**å„ªå‹¢ï¼š**
- ğŸ–±ï¸ **åœ–å½¢åŒ–ä»‹é¢**ï¼šç„¡éœ€å‘½ä»¤åˆ—æ“ä½œ
- ğŸ”„ **å³æ™‚äº’å‹•**ï¼šæ‰€è¦‹å³æ‰€å¾—çš„å°è©±é«”é©—
- ğŸ“Š **çµ±è¨ˆè³‡è¨Š**ï¼šå°è©±æ­·å²å’Œæ€§èƒ½çµ±è¨ˆ
- ğŸ­ **è§’è‰²åˆ‡æ›**ï¼šä¸€éµåˆ‡æ›ä¸åŒè§’è‰²
- ğŸ“± **éŸ¿æ‡‰å¼è¨­è¨ˆ**ï¼šæ”¯æ´æ¡Œé¢å’Œè¡Œå‹•è£ç½®

**åŠŸèƒ½äº®é»ï¼š**
- å´é‚Šæ¬„ç³»çµ±ç‹€æ…‹ç›£æ§
- å°è©±æ­·å²ä¿å­˜å’Œé¡¯ç¤º
- ç¯„ä¾‹å•é¡Œæç¤º
- å›æ‡‰æ™‚é–“å’Œçµ±è¨ˆè³‡è¨Š
- éŒ¯èª¤è™•ç†å’Œç”¨æˆ¶æç¤º

## ğŸ§ª æ¸¬è©¦å’Œé©—è­‰

å‰µå»ºæ¸¬è©¦æª”æ¡ˆä¾†é©—è­‰æˆ‘å€‘çš„æ‡‰ç”¨ï¼š

```python
# test_assistant.py
import unittest
from enhanced_assistant import EnhancedSmartAssistant
import time

class TestSmartAssistant(unittest.TestCase):
    @classmethod
    def setUpClass(cls):
        """æ¸¬è©¦é¡åˆ¥åˆå§‹åŒ–"""
        try:
            cls.assistant = EnhancedSmartAssistant()
        except Exception as e:
            raise unittest.SkipTest(f"ç„¡æ³•åˆå§‹åŒ–åŠ©æ‰‹ï¼š{e}")
    
    def test_initialization(self):
        """æ¸¬è©¦åˆå§‹åŒ–"""
        self.assertIsNotNone(self.assistant.llm)
        self.assertGreater(len(self.assistant.chains), 0)
    
    def test_available_roles(self):
        """æ¸¬è©¦è§’è‰²åˆ—è¡¨"""
        roles = self.assistant.get_available_roles()
        self.assertIsInstance(roles, list)
        self.assertIn("ç¨‹å¼è€å¸«", roles)
        self.assertIn("ç¿»è­¯å“¡", roles)
    
    def test_valid_question(self):
        """æ¸¬è©¦æœ‰æ•ˆå•é¡Œ"""
        result = self.assistant.ask("ç¨‹å¼è€å¸«", "ä»€éº¼æ˜¯è®Šæ•¸ï¼Ÿ")
        self.assertTrue(result["success"])
        self.assertIn("response", result)
        self.assertGreater(len(result["response"]), 10)
    
    def test_invalid_role(self):
        """æ¸¬è©¦ç„¡æ•ˆè§’è‰²"""
        result = self.assistant.ask("ç„¡æ•ˆè§’è‰²", "æ¸¬è©¦å•é¡Œ")
        self.assertFalse(result["success"])
        self.assertIn("error", result)
    
    def test_empty_inputs(self):
        """æ¸¬è©¦ç©ºè¼¸å…¥"""
        result = self.assistant.ask("", "æ¸¬è©¦å•é¡Œ")
        self.assertFalse(result["success"])
        
        result = self.assistant.ask("ç¨‹å¼è€å¸«", "")
        self.assertFalse(result["success"])
    
    def test_long_question(self):
        """æ¸¬è©¦éé•·å•é¡Œ"""
        long_question = "æ¸¬è©¦" * 500  # 1000+ å­—ç¬¦
        result = self.assistant.ask("ç¨‹å¼è€å¸«", long_question)
        self.assertFalse(result["success"])
    
    def test_health_check(self):
        """æ¸¬è©¦å¥åº·æª¢æŸ¥"""
        health = self.assistant.health_check()
        self.assertIn("status", health)

if __name__ == "__main__":
    # åŸ·è¡Œæ¸¬è©¦
    unittest.main(verbosity=2)
```

## ğŸ“š å®Œæ•´å°ˆæ¡ˆçµæ§‹

æ‚¨çš„å°ˆæ¡ˆç›®éŒ„æ‡‰è©²çœ‹èµ·ä¾†åƒé€™æ¨£ï¼š

```
my-first-langchain-app/
â”œâ”€â”€ venv/                    # è™›æ“¬ç’°å¢ƒ
â”œâ”€â”€ basic_llm.py            # åŸºç¤ LLM ç¯„ä¾‹
â”œâ”€â”€ prompt_template_example.py  # Prompt Template ç¯„ä¾‹
â”œâ”€â”€ langchain_chain_example.py  # LangChain éˆç¯„ä¾‹
â”œâ”€â”€ smart_assistant.py      # å®Œæ•´åŠ©æ‰‹æ‡‰ç”¨ï¼ˆå‘½ä»¤åˆ—ç‰ˆï¼‰
â”œâ”€â”€ enhanced_assistant.py   # å¢å¼·ç‰ˆåŠ©æ‰‹ï¼ˆå‘½ä»¤åˆ—ç‰ˆï¼‰
â”œâ”€â”€ streamlit_assistant.py  # Streamlit Web ä»‹é¢ç‰ˆæœ¬
â”œâ”€â”€ test_assistant.py       # æ¸¬è©¦æª”æ¡ˆ
â”œâ”€â”€ test_ollama.py         # Ollama é€£æ¥æ¸¬è©¦
â”œâ”€â”€ assistant.log          # æ‡‰ç”¨æ—¥èªŒ
â””â”€â”€ README.md              # å°ˆæ¡ˆèªªæ˜
```

## ğŸ¯ ç¸½çµ

æ­å–œï¼æ‚¨å·²ç¶“æˆåŠŸå»ºæ§‹äº†ç¬¬ä¸€å€‹ LangChain æ‡‰ç”¨ç¨‹å¼ã€‚åœ¨é€™å€‹éç¨‹ä¸­ï¼Œæ‚¨å­¸æœƒäº†ï¼š

### ğŸ”‘ æ ¸å¿ƒæ¦‚å¿µ
- **LangChain çš„åƒ¹å€¼**ï¼šå°‡è¤‡é›œçš„ AI äº’å‹•æŠ½è±¡åŒ–
- **çµ„ä»¶åŒ–è¨­è¨ˆ**ï¼šLLM + Prompt Template + Chain
- **æœ¬åœ°æ¨¡å‹æ•´åˆ**ï¼šä½¿ç”¨ Ollama çš„å„ªå‹¢

### ğŸ› ï¸ å¯¦ç”¨æŠ€èƒ½
- **éŒ¯èª¤è™•ç†**ï¼šç¶²è·¯é€£æ¥ã€è¶…æ™‚ã€è¼¸å…¥é©—è­‰
- **æ—¥èªŒè¨˜éŒ„**ï¼šè¿½è¹¤æ‡‰ç”¨ç¨‹å¼è¡Œç‚º
- **æ¸¬è©¦é©—è­‰**ï¼šç¢ºä¿ç¨‹å¼ç¢¼å“è³ª
- **ç”¨æˆ¶é«”é©—**ï¼šå‹å–„çš„å‘½ä»¤åˆ—ä»‹é¢å’Œ Web ä»‹é¢
- **ä»‹é¢è¨­è¨ˆ**ï¼šä½¿ç”¨ Streamlit å»ºæ§‹äº’å‹•å¼ Web æ‡‰ç”¨

### ğŸš€ é€²éšæº–å‚™
é€™å€‹æ‡‰ç”¨ç‚ºå¾ŒçºŒå­¸ç¿’å¥ å®šäº†åŸºç¤ï¼š
- **è¨˜æ†¶ç³»çµ±**ï¼šå¯ä»¥ç‚ºåŠ©æ‰‹åŠ å…¥å°è©±è¨˜æ†¶
- **RAG æ•´åˆ**ï¼šå¯ä»¥è®“åŠ©æ‰‹æŸ¥è©¢çŸ¥è­˜åº«
- **å·¥å…·èª¿ç”¨**ï¼šå¯ä»¥è®“åŠ©æ‰‹åŸ·è¡Œå¯¦éš›ä»»å‹™
- **éƒ¨ç½²ä¸Šç·š**ï¼šå·²æœ‰ Streamlit Web ä»‹é¢ï¼Œå¯é€²ä¸€æ­¥éƒ¨ç½²ç‚ºé›²ç«¯æœå‹™
- **å¤šæ¨¡æ…‹æ•´åˆ**ï¼šå¯åŠ å…¥åœ–ç‰‡ã€èªéŸ³ç­‰å¤šåª’é«”æ”¯æ´

## ğŸ”— ä¸‹ä¸€æ­¥

å»ºè­°æ‚¨æ¥ä¸‹ä¾†å­¸ç¿’ï¼š

1. **[Chat Models å°è©±æ¨¡å‹](/tutorials/chat-models)** - æ·±å…¥ç†è§£å°è©±æ¨¡å‹çš„åŸç†
2. **[Prompt Template æç¤ºç¯„æœ¬](/tutorials/prompt-template)** - å­¸ç¿’æ›´é€²éšçš„æç¤ºè¨­è¨ˆ
3. **[LCEL è¡¨é”å¼èªè¨€](/tutorials/lcel)** - æŒæ¡ LangChain çš„éˆå¼èªæ³•

## ğŸ¤ åƒèˆ‡è¨è«–

å¦‚æœæ‚¨åœ¨å»ºæ§‹éç¨‹ä¸­é‡åˆ°å•é¡Œï¼Œæˆ–æœ‰æ”¹é€²å»ºè­°ï¼Œæ­¡è¿ï¼š

- ğŸ“ åœ¨æˆ‘å€‘çš„ [GitHub Issues](https://github.com/your-repo/issues) æå‡ºå•é¡Œ
- ğŸ’¬ åƒèˆ‡ [è¨è«–å€](https://github.com/your-repo/discussions) åˆ†äº«ç¶“é©—
- ğŸŒŸ å¦‚æœé€™å€‹æ•™å­¸å°æ‚¨æœ‰å¹«åŠ©ï¼Œè«‹çµ¦æˆ‘å€‘ä¸€å€‹ Starï¼

---

**ğŸ‰ æ‚¨å·²ç¶“è¸å‡ºäº† LangChain é–‹ç™¼çš„ç¬¬ä¸€æ­¥ï¼**