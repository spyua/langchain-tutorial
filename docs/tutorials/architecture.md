# LangChain æ¶æ§‹èˆ‡æ ¸å¿ƒæ¦‚å¿µ

## LangChain æ¶æ§‹æ¦‚è¦½

LangChain æ¡†æ¶æä¾›äº†ä¸€ç³»åˆ—æ¨¡çµ„åŒ–çš„æŠ½è±¡åŒ–åŠŸèƒ½ï¼ˆmodular abstractionsï¼‰ï¼Œé€™äº›æ˜¯èˆ‡ LLM ä¸€èµ·å·¥ä½œæ‰€å¿…éœ€çš„ï¼ŒåŒæ™‚ä¹Ÿæä¾›äº†å»£æ³›çš„å¯¦ä½œç‰ˆæœ¬ï¼Œæ–¹ä¾¿é–‹ç™¼è€…æ‡‰ç”¨ã€‚

```mermaid
graph TB
    subgraph "LangChain v0.2+ ç¾ä»£æ¶æ§‹"
        subgraph "æ¨¡å‹å±¤"
            LLM[LLM Models<br/>å¤§å‹èªè¨€æ¨¡å‹æ¥å…¥<br/>OpenAI, Anthropic, æœ¬åœ°æ¨¡å‹ç­‰]
        end
        
        subgraph "è¼¸å…¥è™•ç†"
            Prompts[Prompts<br/>æç¤ºè©æ¨¡æ¿ç®¡ç†]
            DocLoad[Document Loaders<br/>æ–‡ä»¶è¼‰å…¥å·¥å…·]
            Splitters[Text Splitters<br/>æ–‡æœ¬åˆ†å‰²å™¨]
        end
        
        subgraph "æ ¸å¿ƒåŸ·è¡Œå¼•æ“ (LCEL)"
            Runnables[Runnables<br/>å¯åŸ·è¡Œæ¥å£]
            Chains[Chains/Pipelines<br/>çµ„åˆå¼æµç¨‹]
            LCEL[LCEL èªæ³•<br/>| ç®¡é“æ“ä½œç¬¦]
        end
        
        subgraph "é€²éšåŠŸèƒ½"
            Retrieval[Retrieval<br/>æª¢ç´¢å¢å¼·ç”Ÿæˆ]
            Memory[Memory<br/>å°è©±è¨˜æ†¶]
            OutputParsers[Output Parsers<br/>çµæ§‹åŒ–è¼¸å‡ºè§£æ]
            Callbacks[Callbacks<br/>LangSmith è¿½è¹¤]
        end
        
        subgraph "æ™ºèƒ½ä»£ç†æ¡†æ¶"
            Agents[Traditional Agents<br/>å·¥å…·èª¿ç”¨ä»£ç†]
            LangGraph[LangGraph<br/>å¤šä»£ç†å·¥ä½œæµ<br/>ç‹€æ…‹æ©Ÿ & æ¢ä»¶è·¯ç”±]
        end
        
        subgraph "å¥—ä»¶ç”Ÿæ…‹"
            Core[langchain-core<br/>æ ¸å¿ƒæŠ½è±¡]
            Community[langchain-community<br/>ç¬¬ä¸‰æ–¹æ•´åˆ]
            Integration[langchain-openai<br/>langchain-chroma<br/>å°ˆç”¨æ•´åˆåŒ…]
        end
        
        %% è³‡æ–™æµå‘
        DocLoad --> Splitters
        Splitters --> Retrieval
        Prompts --> Runnables
        LLM --> Runnables
        Retrieval --> Runnables
        Memory --> Runnables
        
        Runnables --> LCEL
        LCEL --> Chains
        Chains --> OutputParsers
        
        Runnables --> Agents
        Runnables --> LangGraph
        
        %% ç›£æ§èˆ‡è¿½è¹¤
        Chains -.-> Callbacks
        Agents -.-> Callbacks
        LangGraph -.-> Callbacks
        
        %% å¥—ä»¶ä¾è³´
        Core --> Runnables
        Community --> DocLoad
        Integration --> LLM
        Integration --> Retrieval
    end
```

### ä¸»è¦æ¨¡çµ„èªªæ˜

| æ¨¡çµ„ | åŠŸèƒ½èªªæ˜ | å¯¦éš›ç”¨é€” | v0.2+ æ–°ç‰¹æ€§ |
|------|----------|----------|------------|
| **LCEL/Runnables** | å¯çµ„åˆçš„åŸ·è¡Œä»‹é¢ | ç”¨ `\|` æ“ä½œç¬¦é€£æ¥å„çµ„ä»¶ï¼Œå»ºç«‹è³‡æ–™æµç®¡é“ | ğŸ†• æ ¸å¿ƒåŸ·è¡Œå¼•æ“ |
| **LangGraph** | å¤šä»£ç†å·¥ä½œæµæ¡†æ¶ | è¤‡é›œçš„ç‹€æ…‹æ©Ÿã€æ¢ä»¶è·¯ç”±ã€å¤š Agent å”ä½œ | ğŸ†• å–ä»£å‚³çµ± Agent |
| **Output Parsers** | çµæ§‹åŒ–è¼¸å‡ºè§£æ | Pydantic æ¨¡å‹ã€JSON Schema é©—è­‰ | âœ… åŠ å¼·é¡å‹å®‰å…¨ |
| **LLM Models** | å¤§å‹èªè¨€æ¨¡å‹æ¥å…¥ | æ”¯æ´ OpenAI GPTã€Anthropic Claudeã€æœ¬åœ°æ¨¡å‹ç­‰ | âœ… ç¨ç«‹æ•´åˆåŒ… |
| **Prompts** | æç¤ºè©æ¨¡æ¿ç®¡ç† | æ”¯æ´ ChatPromptTemplateã€MessagesPlaceholder | âœ… å¼·åŒ–å°è©±æ”¯æ´ |
| **Document Loaders** | æ–‡ä»¶è¼‰å…¥å·¥å…· | å¾ PDFã€ç¶²é ã€è³‡æ–™åº«ç­‰è¼‰å…¥ä¸¦è™•ç†è³‡æ–™ | âœ… ç§»è‡³ community åŒ… |
| **Retrieval** | æª¢ç´¢å¢å¼·ç”Ÿæˆ | `create_retrieval_chain` å–ä»£èˆŠ RetrievalQA | âœ… LCEL åŸç”Ÿæ”¯æ´ |
| **Memory** | å°è©±è¨˜æ†¶æ©Ÿåˆ¶ | `RunnableWithMessageHistory` æ–°æ¶æ§‹ | âœ… æŒä¹…åŒ–èˆ‡å¤šæœƒè©± |
| **Callbacks** | åŸ·è¡Œç›£æ§è¿½è¹¤ | LangSmith æ•´åˆã€Token è¨ˆç®—ã€æ•ˆèƒ½åˆ†æ | ğŸ†• å¯è§€æ¸¬æ€§ |
| **Traditional Agents** | å·¥å…·èª¿ç”¨ä»£ç† | `create_react_agent` å–ä»£ `initialize_agent` | âš ï¸ å»ºè­°é·ç§»è‡³ LangGraph |

## ä»€éº¼æ˜¯ã€ŒæŠ½è±¡åŒ–ã€ï¼Ÿ

### æ¦‚å¿µè§£é‡‹

åœ¨è»Ÿé«”è¨­è¨ˆè£¡ï¼Œ**æŠ½è±¡åŒ–ï¼ˆAbstractionï¼‰**å°±æ˜¯ï¼š
> éš±è—ç´°ç¯€ï¼Œåªä¿ç•™æœ€å¿…è¦çš„ç‰¹å¾µï¼Œè®“ä½¿ç”¨è€…èƒ½æ›´ç°¡å–®åœ°æ“ä½œã€‚

- **æ²’æœ‰æŠ½è±¡åŒ–** â†’ ä½ è¦è‡ªå·±è™•ç†ä¸€å¤§å †é›œäº‹ï¼ˆä¾‹å¦‚ç›´æ¥å‘¼å« APIï¼Œè¦ç®¡ Tokenã€æ ¼å¼ã€å›å‚³ JSON ç­‰ï¼‰
- **æœ‰æŠ½è±¡åŒ–** â†’ æ¡†æ¶å¹«ä½ æŠŠé›œäº‹åŒ…å¥½ï¼Œçµ¦ä½ ä¸€å€‹ä¹¾æ·¨çš„ä»‹é¢

### å¯¦éš›å°æ¯”

| å ´æ™¯ | æ²’æœ‰æŠ½è±¡åŒ– | æœ‰ LangChain æŠ½è±¡åŒ– |
|------|------------|-------------------|
| ä½¿ç”¨ä¸åŒ LLM | è¦ç‚ºæ¯å€‹ API å¯«ä¸åŒç¨‹å¼ç¢¼ | çµ±ä¸€ä»‹é¢ï¼Œä¸€è¡Œç¨‹å¼ç¢¼åˆ‡æ›æ¨¡å‹ |
| ç®¡ç†å°è©±è¨˜æ†¶ | æ‰‹å‹•å­˜å–è³‡æ–™åº«ï¼Œæ‹¼æ¥ä¸Šä¸‹æ–‡ | æ›ä¸Š Memory æ¨¡çµ„è‡ªå‹•è™•ç† |
| å¤šæ­¥é©Ÿè™•ç† | è‡ªå·±è¨­è¨ˆæµç¨‹æ§åˆ¶é‚è¼¯ | ç”¨ Chain æè¿°æ­¥é©Ÿå³å¯ |

## LangChain åŒ…è£äº†å“ªäº›è¤‡é›œåŠŸèƒ½ï¼Ÿ

### 1. ğŸ”Œ LLM ä»‹æ¥çµ±ä¸€åŒ–

**åŸæœ¬è¤‡é›œï¼š** ä¸åŒå» ç‰Œçš„ LLM API æ ¼å¼å„ç•°ï¼ŒTokenã€å›å‚³æ ¼å¼ã€æµå¼è™•ç†éƒ½ä¸åŒã€‚

**LangChain åŒ…è£ï¼š** æä¾›çµ±ä¸€çš„ä»‹é¢ï¼Œå¯ç„¡ç—›åˆ‡æ›æ¨¡å‹ã€‚

```python
# åŒ¯å…¥ä¸åŒå» å•†çš„èŠå¤©æ¨¡å‹é¡åˆ¥
from langchain_openai import ChatOpenAI      # OpenAI GPT ç³»åˆ—æ¨¡å‹
from langchain_anthropic import ChatAnthropic # Anthropic Claude ç³»åˆ—æ¨¡å‹

# çµ±ä¸€çš„æ¨¡å‹åˆå§‹åŒ–ä»‹é¢ - é€™æ˜¯ LangChain æŠ½è±¡åŒ–çš„æ ¸å¿ƒåƒ¹å€¼
# ç„¡è«–ä½¿ç”¨å“ªå®¶å» å•†çš„æ¨¡å‹ï¼Œåˆå§‹åŒ–èªæ³•éƒ½ä¿æŒä¸€è‡´
llm = ChatOpenAI(model="gpt-4")  
# æˆ–è€…åˆ‡æ›åˆ° Claude æ¨¡å‹ï¼Œåªéœ€è¦æ”¹é€™ä¸€è¡Œ
# llm = ChatAnthropic(model="claude-3-opus")

# çµ±ä¸€çš„èª¿ç”¨ä»‹é¢ - invoke() æ–¹æ³•å°æ‰€æœ‰æ¨¡å‹éƒ½ç›¸åŒ
# ä¸éœ€è¦å­¸ç¿’æ¯å®¶å» å•†çš„å°ˆå±¬ API æ ¼å¼
response = llm.invoke("å¹«æˆ‘å¯«ä¸€é¦–è©©")
print(response.content)  # æ‰€æœ‰æ¨¡å‹çš„å›æ‡‰éƒ½æœ‰çµ±ä¸€çš„ .content å±¬æ€§
```

### 2. ğŸ“ Prompt æ¨¡æ¿ç®¡ç†

**åŸæœ¬è¤‡é›œï¼š** è¦è‡ªå·±æ‹¼å­—ä¸²ï¼ŒæŠŠä¸Šä¸‹æ–‡ã€æ ¼å¼ã€è®Šæ•¸å…¨éƒ½å¯«æ­»ã€‚

**LangChain åŒ…è£ï¼š** æä¾› PromptTemplateï¼Œå¯ä»¥ç”¨è®Šæ•¸å¡«å…¥ã€‚

```python
# åŒ¯å…¥æç¤ºç¯„æœ¬é¡åˆ¥
from langchain.prompts import PromptTemplate

# å»ºç«‹æç¤ºç¯„æœ¬ - ä½¿ç”¨ {} èªæ³•å®šç¾©å¯æ›¿æ›çš„è®Šæ•¸
# é€™æ¨£å¯ä»¥é‡è¤‡ä½¿ç”¨åŒä¸€å€‹ç¯„æœ¬ï¼Œåªéœ€è¦å‚³å…¥ä¸åŒçš„è³‡æ–™
template = PromptTemplate.from_template(
    "ä½ æ˜¯ä¸€ä½ç‡Ÿé¤Šå¸«ï¼Œè«‹æ ¹æ“šé€™äº›æ•¸æ“š {health_data} æä¾›å»ºè­°"
)

# å°‡å¯¦éš›è³‡æ–™å¡«å…¥ç¯„æœ¬è®Šæ•¸
# format() æ–¹æ³•æœƒå°‡ {health_data} æ›¿æ›ç‚ºå¯¦éš›çš„å¥åº·æ•¸æ“š
prompt = template.format(health_data="è¡€ç³–åé«˜")
print(prompt)
# è¼¸å‡º: "ä½ æ˜¯ä¸€ä½ç‡Ÿé¤Šå¸«ï¼Œè«‹æ ¹æ“šé€™äº›æ•¸æ“š è¡€ç³–åé«˜ æä¾›å»ºè­°"

# ç¯„æœ¬çš„å„ªå‹¢ï¼šå¯ä»¥é‡è¤‡ä½¿ç”¨ï¼Œåªéœ€è¦æ”¹è®Šè¼¸å…¥è³‡æ–™
another_prompt = template.format(health_data="è†½å›ºé†‡éé«˜ï¼ŒBMI 28.5")
```

### 3. ğŸ§  Memoryï¼ˆå°è©±è¨˜æ†¶ï¼‰

**åŸæœ¬è¤‡é›œï¼š** LLM å¤©ç”Ÿç„¡è¨˜æ†¶ï¼Œè¦è‡ªå·±ç®¡ç†å°è©±æ­·å²ï¼Œå­˜è³‡æ–™åº«ï¼Œå†æ‰‹å‹•æ‹¼æ¥ã€‚

**LangChain åŒ…è£ï¼š** å…§å»ºå„ç¨® Memory é¡å‹ï¼Œæ›ä¸Šå°±èƒ½è¨˜ä½ä¸Šä¸‹æ–‡ã€‚

```python
# v0.2+ æ–°ç‰ˆå°è©±è¨˜æ†¶åšæ³• - ä½¿ç”¨ RunnableWithMessageHistory ç®¡ç†å°è©±ç‹€æ…‹
from langchain_core.runnables.history import RunnableWithMessageHistory
from langchain_community.chat_message_histories import ChatMessageHistory
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_openai import ChatOpenAI

# åˆå§‹åŒ– LLM æ¨¡å‹
llm = ChatOpenAI(model="gpt-4o-mini")

# MessagesPlaceholder çš„åº•å±¤è³‡æ–™çµæ§‹æ˜¯è¨Šæ¯åˆ—è¡¨
# æ¯å€‹è¨Šæ¯éƒ½æ˜¯å­—å…¸æ ¼å¼ï¼ŒåŒ…å« role å’Œ content
"""
å°è©±æ­·å²çš„è³‡æ–™çµæ§‹ç¯„ä¾‹ï¼š
[
  {"role": "user", "content": "æˆ‘å«å°æ˜"},
  {"role": "assistant", "content": "å¥½çš„ï¼Œå°æ˜ï¼Œæˆ‘è¨˜ä½äº†ã€‚"}
]
"""

# å»ºç«‹èŠå¤©æç¤ºç¯„æœ¬ - åŒ…å«ç³»çµ±è¨Šæ¯ã€æ­·å²è¨˜éŒ„å’Œä½¿ç”¨è€…è¼¸å…¥
prompt = ChatPromptTemplate.from_messages([
    # ç³»çµ±è¨Šæ¯ï¼šå®šç¾© AI åŠ©æ‰‹çš„è§’è‰²å’Œè¡Œç‚º
    ("system", "ä½ æ˜¯ä¸€å€‹æœ‰ç”¨çš„åŠ©æ‰‹ï¼Œèƒ½è¨˜ä½å°è©±æ­·å²ã€‚"),
    
    # å°è©±æ­·å²ä½”ä½ç¬¦ï¼šå‹•æ…‹æ’å…¥ä¹‹å‰çš„å°è©±è¨˜éŒ„
    MessagesPlaceholder(variable_name="chat_history"),
    
    # ç•¶å‰ä½¿ç”¨è€…è¼¸å…¥
    ("human", "{input}"),
])

# ä½¿ç”¨ LCEL ç®¡é“èªæ³•çµ„åˆæç¤ºç¯„æœ¬å’Œ LLM
chain = prompt | llm

# å»ºç«‹è¨˜æ†¶é«”å­˜å„² - ä½¿ç”¨å­—å…¸åœ¨è¨˜æ†¶é«”ä¸­ä¿å­˜ä¸åŒæœƒè©±çš„æ­·å²
store = {}

def get_session_history(session_id: str) -> ChatMessageHistory:
    """æ ¹æ“šæœƒè©± ID ç²å–æˆ–å»ºç«‹å°è©±æ­·å²
    
    Args:
        session_id: å”¯ä¸€çš„æœƒè©±è­˜åˆ¥ç¢¼ï¼Œç”¨æ–¼å€åˆ†ä¸åŒä½¿ç”¨è€…æˆ–å°è©±
    
    Returns:
        ChatMessageHistory: è©²æœƒè©±çš„å°è©±æ­·å²ç‰©ä»¶
    """
    if session_id not in store:
        # å¦‚æœæ˜¯æ–°æœƒè©±ï¼Œå»ºç«‹ç©ºçš„å°è©±æ­·å²
        store[session_id] = ChatMessageHistory()
    return store[session_id]

# å»ºç«‹å…·æœ‰è¨˜æ†¶åŠŸèƒ½çš„å°è©±éˆ
conversation = RunnableWithMessageHistory(
    chain,                           # è¦åŸ·è¡Œçš„éˆï¼ˆæç¤ºç¯„æœ¬ + LLMï¼‰
    get_session_history,            # ç²å–æœƒè©±æ­·å²çš„å‡½æ•¸
    input_messages_key="input",     # ä½¿ç”¨è€…è¼¸å…¥çš„éµå
    history_messages_key="chat_history",  # å°è©±æ­·å²çš„éµå
)

# è¨­å®šæœƒè©±é…ç½® - æŒ‡å®šæœƒè©± ID
config = {"configurable": {"session_id": "user123"}}

# ç¬¬ä¸€æ¬¡å°è©±ï¼šå»ºç«‹è¨˜æ†¶
first_response = conversation.invoke({"input": "æˆ‘å«å°æ˜"}, config=config)
print(f"ç¬¬ä¸€æ¬¡å›æ‡‰: {first_response.content}")

# ç¬¬äºŒæ¬¡å°è©±ï¼šæ¸¬è©¦è¨˜æ†¶åŠŸèƒ½
result = conversation.invoke({"input": "æˆ‘å‰›æ‰èªªæˆ‘å«ä»€éº¼åå­—ï¼Ÿ"}, config=config)
print(f"è¨˜æ†¶æ¸¬è©¦çµæœ: {result.content}")
# AI æ‡‰è©²èƒ½å¤ è¨˜ä½ä¹‹å‰æåˆ°çš„ã€Œå°æ˜ã€é€™å€‹åå­—

# æª¢è¦–ç•¶å‰æœƒè©±çš„å°è©±æ­·å²
history = store["user123"]
print(f"\nå°è©±æ­·å²å…±æœ‰ {len(history.messages)} æ¢è¨Šæ¯")
for i, msg in enumerate(history.messages):
    print(f"{i+1}. {msg.type}: {msg.content}")
```

### 4. ğŸ” Retrieval + å¤–éƒ¨çŸ¥è­˜åº«æ•´åˆ

**åŸæœ¬è¤‡é›œï¼š** è¦è‡ªå·±å¯« embeddingã€å­˜åˆ°å‘é‡è³‡æ–™åº«ã€å†å¯«æª¢ç´¢é‚è¼¯ã€‚

**LangChain åŒ…è£ï¼š** æä¾› Retrieverï¼Œä¸€å¥è©±å°±èƒ½è®“ LLM æ¥å¤–éƒ¨çŸ¥è­˜ã€‚

```python
# æª¢ç´¢å¢å¼·ç”Ÿæˆ (RAG) çš„ v0.2+ æ–°ç‰ˆå¯¦ä½œ
from langchain.chains.retrieval import create_retrieval_chain
from langchain.chains.combine_documents import create_stuff_documents_chain
from langchain_core.prompts import ChatPromptTemplate

# å»ºç«‹æ–‡ä»¶è™•ç†çš„æç¤ºç¯„æœ¬
# {context} æœƒè¢«è‡ªå‹•å¡«å…¥æª¢ç´¢åˆ°çš„ç›¸é—œæ–‡ä»¶å…§å®¹
# {input} æ˜¯ä½¿ç”¨è€…çš„å•é¡Œ
prompt = ChatPromptTemplate.from_template(
    "æ ¹æ“šä»¥ä¸‹æ–‡ä»¶å›è¦†ï¼š{context}\n\nå•é¡Œï¼š{input}\n\nè«‹åŸºæ–¼æä¾›çš„æ–‡ä»¶å…§å®¹å›ç­”ï¼Œå¦‚æœæ–‡ä»¶ä¸­æ²’æœ‰ç›¸é—œè³‡è¨Šï¼Œè«‹æ˜ç¢ºèªªæ˜ã€‚"
)

# å»ºç«‹æ–‡ä»¶åˆä½µéˆ - å°‡å¤šå€‹æª¢ç´¢åˆ°çš„æ–‡ä»¶åˆä½µå¾Œé€çµ¦ LLM
# "stuff" ç­–ç•¥ï¼šå°‡æ‰€æœ‰ç›¸é—œæ–‡ä»¶ç›´æ¥å¡å…¥ prompt ä¸­
stuff_chain = create_stuff_documents_chain(
    llm,     # èªè¨€æ¨¡å‹
    prompt   # åŒ…å« context å’Œ input çš„æç¤ºç¯„æœ¬
)

# å»ºç«‹å®Œæ•´çš„æª¢ç´¢-å›ç­”éˆ
# é€™å€‹éˆæœƒï¼š1) æ ¹æ“šå•é¡Œæª¢ç´¢ç›¸é—œæ–‡ä»¶ â†’ 2) å°‡æ–‡ä»¶å’Œå•é¡Œé€çµ¦ LLM ç”Ÿæˆå›ç­”
qa = create_retrieval_chain(
    vectorstore.as_retriever(),  # å‘é‡è³‡æ–™åº«æª¢ç´¢å™¨ï¼ˆéœ€äº‹å…ˆå»ºç«‹ï¼‰
    stuff_chain                  # æ–‡ä»¶è™•ç†å’Œå›ç­”ç”Ÿæˆéˆ
)

# åŸ·è¡Œå•ç­”
answer = qa.invoke({"input": "å…¬å¸çš„è«‹å‡æ”¿ç­–æ˜¯ä»€éº¼ï¼Ÿ"})

# æª¢è¦–çµæœ
print(f"å›ç­”: {answer['answer']}")

# å¯é¸ï¼šæª¢è¦–æª¢ç´¢åˆ°çš„ç›¸é—œæ–‡ä»¶
if 'context' in answer:
    print(f"\nåƒè€ƒæ–‡ä»¶æ•¸é‡: {len(answer['context'])}")
    for i, doc in enumerate(answer['context']):
        print(f"æ–‡ä»¶ {i+1}: {doc.page_content[:100]}...")
```

### 5. â›“ï¸ Chainsï¼ˆå¤šæ­¥é©Ÿæµç¨‹çµ„è£ï¼‰

**åŸæœ¬è¤‡é›œï¼š** è¦æ‰‹å‹•æ§åˆ¶æµç¨‹ï¼šå…ˆæª¢ç´¢è³‡æ–™ â†’ å†å• LLM â†’ å†æ ¼å¼åŒ–çµæœã€‚

**LangChain åŒ…è£ï¼š** æŠŠå¤šæ­¥é©Ÿçµ„è£æˆã€Œæµç¨‹éˆã€ã€‚

```python
# v0.2+ æ–°ç‰ˆåºåˆ—éˆåšæ³•ï¼šä½¿ç”¨ LCEL (LangChain Expression Language) ç®¡é“èªæ³•
from langchain_core.output_parsers import StrOutputParser
from langchain_core.prompts import PromptTemplate
from langchain_openai import ChatOpenAI

# åˆå§‹åŒ– LLM æ¨¡å‹
llm = ChatOpenAI(model="gpt-4o-mini")

# æ­¥é©Ÿ 1ï¼šå¥åº·æ•¸æ“šåˆ†æçš„æç¤ºç¯„æœ¬
prompt_analysis = PromptTemplate.from_template(
    "ä½œç‚ºå°ˆæ¥­é†«ç™‚é¡§å•ï¼Œè«‹è©³ç´°åˆ†æä»¥ä¸‹å¥åº·æ•¸æ“šï¼š{health_data}\n"
    "è«‹å¾ä»¥ä¸‹è§’åº¦åˆ†æï¼š1) å„é …æŒ‡æ¨™çš„æ­£å¸¸ç¯„åœæ¯”è¼ƒ 2) æ½›åœ¨å¥åº·é¢¨éšª 3) ç›¸äº’é—œè¯æ€§"
)

# æ­¥é©Ÿ 2ï¼šåŸºæ–¼åˆ†æçµæœæä¾›å»ºè­°çš„æç¤ºç¯„æœ¬
prompt_recommendation = PromptTemplate.from_template(
    "åŸºæ–¼ä»¥ä¸‹å¥åº·åˆ†æçµæœï¼š{analysis}\n\n"
    "è«‹æä¾›å…·é«”çš„æ”¹å–„å»ºè­°ï¼ŒåŒ…æ‹¬ï¼š1) é£²é£Ÿèª¿æ•´ 2) é‹å‹•è¨ˆåŠƒ 3) ç”Ÿæ´»ç¿’æ…£æ”¹è®Š 4) æ˜¯å¦éœ€è¦å°±é†«"
)

# æ­¥é©Ÿ 3ï¼šå°‡å»ºè­°æ ¼å¼åŒ–ç‚ºå‹å¥½å ±å‘Šçš„æç¤ºç¯„æœ¬
prompt_format = PromptTemplate.from_template(
    "è«‹å°‡ä»¥ä¸‹å¥åº·å»ºè­°ï¼š{recommendations}\n\n"
    "æ ¼å¼åŒ–ç‚ºçµæ§‹æ¸…æ™°ã€æ˜“è®€çš„å€‹äººå¥åº·å ±å‘Šï¼Œä½¿ç”¨å‹å¥½çš„èªèª¿å’Œæ˜ç¢ºçš„è¡Œå‹•æ­¥é©Ÿ"
)

# ä½¿ç”¨ LCEL ç®¡é“èªæ³•ï¼ˆ| æ“ä½œç¬¦ï¼‰å»ºç«‹å„å€‹è™•ç†éˆ
# æ¯å€‹éˆéƒ½åŒ…å«ï¼šæç¤ºç¯„æœ¬ â†’ LLM â†’ è¼¸å‡ºè§£æå™¨
analysis_chain = prompt_analysis | llm | StrOutputParser()
recommendation_chain = prompt_recommendation | llm | StrOutputParser()
format_chain = prompt_format | llm | StrOutputParser()

# å®Œæ•´çš„å¥åº·åˆ†ææµç¨‹å‡½æ•¸
def health_analysis_pipeline(health_data: str):
    """å¤šæ­¥é©Ÿå¥åº·åˆ†ææµç¨‹
    
    Args:
        health_data: åŸå§‹å¥åº·æ•¸æ“šå­—ä¸²
        
    Returns:
        str: æ ¼å¼åŒ–çš„å¥åº·åˆ†æå ±å‘Š
    """
    print("æ­¥é©Ÿ 1: åˆ†æå¥åº·æ•¸æ“š...")
    # ç¬¬ä¸€æ­¥ï¼šåˆ†æåŸå§‹å¥åº·æ•¸æ“š
    analysis = analysis_chain.invoke({"health_data": health_data})
    print(f"åˆ†æçµæœ: {analysis[:100]}...\n")
    
    print("æ­¥é©Ÿ 2: ç”Ÿæˆæ”¹å–„å»ºè­°...")
    # ç¬¬äºŒæ­¥ï¼šåŸºæ–¼åˆ†æçµæœç”Ÿæˆå»ºè­°
    recommendations = recommendation_chain.invoke({"analysis": analysis})
    print(f"å»ºè­°å…§å®¹: {recommendations[:100]}...\n")
    
    print("æ­¥é©Ÿ 3: æ ¼å¼åŒ–æœ€çµ‚å ±å‘Š...")
    # ç¬¬ä¸‰æ­¥ï¼šå°‡å»ºè­°æ ¼å¼åŒ–ç‚ºç”¨æˆ¶å‹å¥½çš„å ±å‘Š
    final_report = format_chain.invoke({"recommendations": recommendations})
    
    return final_report

# ä½¿ç”¨ç¯„ä¾‹
health_data = "è¡€ç³–åé«˜ 130mg/dL, BMI 25.5, é‹å‹•é‡å°‘, è¡€å£“ 140/90, ç¡çœ å“è³ªå·®"
result = health_analysis_pipeline(health_data)
print("=== æœ€çµ‚å¥åº·åˆ†æå ±å‘Š ===")
print(result)

# LCEL ç®¡é“èªæ³•çš„å„ªå‹¢ï¼š
# 1. ç¨‹å¼ç¢¼ç°¡æ½”ï¼šç”¨ | æ“ä½œç¬¦ç›´è§€è¡¨é”è³‡æ–™æµ
# 2. å‹åˆ¥å®‰å…¨ï¼šç·¨è­¯æ™‚æª¢æŸ¥ä»‹é¢ç›¸å®¹æ€§
# 3. å¯çµ„åˆæ€§ï¼šå¯ä»¥è¼•é¬†é‡æ–°æ’åˆ—æˆ–æ›¿æ›çµ„ä»¶
# 4. ä¸¦è¡Œæ”¯æ´ï¼šæŸäº›æ“ä½œå¯ä»¥è‡ªå‹•ä¸¦è¡ŒåŒ–
```

### 6. ğŸ¯ Agentsï¼ˆå‹•æ…‹æ±ºç­– & å·¥å…·èª¿ç”¨ï¼‰

**åŸæœ¬è¤‡é›œï¼š** è¦è‡ªå·±å¯« if/else åˆ¤æ–·ï¼Œæ±ºå®šä½•æ™‚è©²æŸ¥ APIã€ä½•æ™‚ç›´æ¥å›è¦†ã€‚

**LangChain åŒ…è£ï¼š** LLM è‡ªä¸»æ±ºå®šè©²èª¿ç”¨å“ªå€‹å·¥å…·ã€‚

```python
# Agent ç³»çµ±ï¼šè®“ LLM èƒ½å¤ è‡ªä¸»æ±ºç­–å’Œä½¿ç”¨å·¥å…·
from langchain.agents import create_tool_calling_agent, AgentExecutor
from langchain.tools import Tool
from langchain_core.prompts import ChatPromptTemplate
import re
import operator

# å®‰å…¨æ•¸å­¸è¨ˆç®—å™¨å¯¦ä½œ - é¿å…ä½¿ç”¨å±éšªçš„ eval() å‡½æ•¸
def safe_calculate(expression: str) -> str:
    """å®‰å…¨çš„æ•¸å­¸è¨ˆç®—å™¨ï¼Œåªå…è¨±åŸºæœ¬æ•¸å­¸é‹ç®—
    
    Args:
        expression: æ•¸å­¸è¡¨é”å¼å­—ä¸²ï¼ˆå¦‚ "(100+50)*2"ï¼‰
        
    Returns:
        str: è¨ˆç®—çµæœæˆ–éŒ¯èª¤è¨Šæ¯
    """
    try:
        # ç§»é™¤æ‰€æœ‰ç©ºç™½å­—ç¬¦
        expression = expression.replace(" ", "")
        
        # å®‰å…¨æª¢æŸ¥ï¼šåªå…è¨±æ•¸å­—ã€åŸºæœ¬é‹ç®—ç¬¦å’Œæ‹¬è™Ÿ
        allowed_chars = set("0123456789+-*/().")
        if not all(c in allowed_chars for c in expression):
            return "éŒ¯èª¤ï¼šåŒ…å«ä¸å…è¨±çš„å­—ç¬¦ï¼Œåªæ”¯æ´ +, -, *, /, (, ) å’Œæ•¸å­—"
        
        # ä½¿ç”¨ eval() çš„å®‰å…¨æ›¿ä»£æ–¹æ¡ˆï¼ˆç°¡åŒ–ç‰ˆæœ¬ï¼‰
        # åœ¨ç”Ÿç”¢ç’°å¢ƒä¸­ï¼Œå»ºè­°ä½¿ç”¨æ›´å®‰å…¨çš„æ•¸å­¸è§£æå™¨
        result = eval(expression, {"__builtins__": {}}, {})
        return f"è¨ˆç®—çµæœ: {expression} = {result}"
        
    except ZeroDivisionError:
        return "éŒ¯èª¤ï¼šé™¤é›¶éŒ¯èª¤"
    except SyntaxError:
        return "éŒ¯èª¤ï¼šæ•¸å­¸è¡¨é”å¼èªæ³•éŒ¯èª¤"
    except Exception as e:
        return f"éŒ¯èª¤ï¼šè¨ˆç®—å¤±æ•— - {str(e)}"

# å»ºç«‹å·¥å…·é›† - Agent å¯ä»¥ä½¿ç”¨çš„åŠŸèƒ½æ¸…å–®
tools = [
    Tool(
        name="å®‰å…¨è¨ˆç®—å™¨",
        func=safe_calculate,
        description="å®‰å…¨çš„æ•¸å­¸è¨ˆç®—å™¨ï¼Œæ”¯æ´åŸºæœ¬å››å‰‡é‹ç®—å’Œæ‹¬è™Ÿã€‚è¼¸å…¥æ ¼å¼ï¼šæ•¸å­¸è¡¨é”å¼å­—ä¸²ã€‚ä¾‹å¦‚ï¼š'(100+50)*2'"
    )
]

# åˆå§‹åŒ– LLM - è¨­å®š temperature=0 ç¢ºä¿å›æ‡‰çš„ä¸€è‡´æ€§
llm = ChatOpenAI(model="gpt-4o-mini", temperature=0)

# å»ºç«‹ Agent çš„æç¤ºç¯„æœ¬
prompt = ChatPromptTemplate.from_messages([
    # ç³»çµ±æŒ‡ä»¤ï¼šå®šç¾© Agent çš„è§’è‰²å’Œè¡Œç‚º
    ("system", "ä½ æ˜¯ä¸€å€‹æ™ºèƒ½åŠ©æ‰‹ï¼Œèƒ½å¤ ä½¿ç”¨æä¾›çš„å·¥å…·ä¾†å®Œæˆä»»å‹™ã€‚"
               "ç•¶ä½¿ç”¨è€…æå‡ºéœ€è¦è¨ˆç®—çš„å•é¡Œæ™‚ï¼Œè«‹ä½¿ç”¨å®‰å…¨è¨ˆç®—å™¨å·¥å…·ã€‚"
               "è«‹å…ˆåˆ†æå•é¡Œï¼Œç„¶å¾Œé¸æ“‡åˆé©çš„å·¥å…·ï¼Œæœ€å¾Œæä¾›æ¸…æ™°çš„å›ç­”ã€‚"),
    
    # ä½¿ç”¨è€…è¼¸å…¥
    ("user", "{input}"),
    
    # Agent çš„æ€è€ƒéç¨‹è¨˜éŒ„ï¼ˆscratchpadï¼‰
    # é€™è£¡æœƒè¨˜éŒ„ Agent çš„æ±ºç­–éç¨‹å’Œå·¥å…·èª¿ç”¨çµæœ
    ("assistant", "{agent_scratchpad}"),
])

# å»ºç«‹ Tool-Calling Agent
# é€™ç¨® Agent èƒ½å¤ ï¼š1) ç†è§£ä½¿ç”¨è€…æ„åœ– 2) é¸æ“‡åˆé©å·¥å…· 3) èª¿ç”¨å·¥å…· 4) è§£é‡‹çµæœ
agent = create_tool_calling_agent(
    llm,      # èªè¨€æ¨¡å‹
    tools,    # å¯ç”¨å·¥å…·åˆ—è¡¨
    prompt    # Agent çš„æç¤ºç¯„æœ¬
)

# å»ºç«‹ Agent åŸ·è¡Œå™¨ - è² è²¬ç®¡ç† Agent çš„åŸ·è¡Œæµç¨‹
agent_executor = AgentExecutor(
    agent=agent,           # Agent å¯¦ä¾‹
    tools=tools,          # å·¥å…·åˆ—è¡¨ï¼ˆèˆ‡ agent ä¸­çš„ç›¸åŒï¼‰
    verbose=True,         # å•Ÿç”¨è©³ç´°è¼¸å‡ºï¼Œå¯ä»¥çœ‹åˆ° Agent çš„æ€è€ƒéç¨‹
    max_iterations=5,     # æœ€å¤§è¿­ä»£æ¬¡æ•¸ï¼Œé˜²æ­¢ç„¡é™å¾ªç’°
    return_intermediate_steps=True  # è¿”å›ä¸­é–“æ­¥é©Ÿï¼Œä¾¿æ–¼èª¿è©¦
)

# æ¸¬è©¦è¤‡é›œæ•¸å­¸è¨ˆç®—ä»»å‹™
print("=== Agent åŸ·è¡Œæ•¸å­¸è¨ˆç®—ä»»å‹™ ===")
response = agent_executor.invoke({
    "input": "å¹«æˆ‘è¨ˆç®— (100+50)*2 çš„çµæœï¼Œç„¶å¾Œå†é™¤ä»¥ 3"
})

print(f"\næœ€çµ‚å›ç­”: {response['output']}")

# æª¢è¦– Agent çš„åŸ·è¡Œæ­¥é©Ÿ
if 'intermediate_steps' in response:
    print("\n=== Agent åŸ·è¡Œæ­¥é©Ÿ ===")
    for i, (action, observation) in enumerate(response['intermediate_steps']):
        print(f"æ­¥é©Ÿ {i+1}:")
        print(f"  å‹•ä½œ: {action.tool} - {action.tool_input}")
        print(f"  çµæœ: {observation}")

# Agent ç³»çµ±çš„å„ªå‹¢ï¼š
# 1. è‡ªä¸»æ±ºç­–ï¼šLLM èƒ½æ ¹æ“šå•é¡Œè‡ªå‹•é¸æ“‡ä½¿ç”¨å“ªå€‹å·¥å…·
# 2. å¤šæ­¥é©Ÿæ¨ç†ï¼šå¯ä»¥å°‡è¤‡é›œä»»å‹™åˆ†è§£ç‚ºå¤šå€‹æ­¥é©Ÿ
# 3. éŒ¯èª¤è™•ç†ï¼šç•¶å·¥å…·èª¿ç”¨å¤±æ•—æ™‚ï¼ŒAgent å¯ä»¥å˜—è©¦å…¶ä»–æ–¹æ³•
# 4. å¯è§£é‡‹æ€§ï¼šèƒ½å¤ è§£é‡‹ç‚ºä»€éº¼é¸æ“‡ç‰¹å®šå·¥å…·å’Œå¦‚ä½•å¾—å‡ºçµæœ
```

## æ ¸å¿ƒæ¶æ§‹ç†å¿µ

### æ¨¡çµ„åŒ–è¨­è¨ˆ

LangChain æ¡ç”¨æ¨¡çµ„åŒ–è¨­è¨ˆï¼Œæ¯å€‹çµ„ä»¶éƒ½å¯ä»¥ç¨ç«‹ä½¿ç”¨æˆ–çµ„åˆä½¿ç”¨ï¼š

- **å¯çµ„åˆæ€§**ï¼šä¸åŒæ¨¡çµ„å¯ä»¥éˆæ´»çµ„åˆ
- **å¯æ“´å±•æ€§**ï¼šå®¹æ˜“æ·»åŠ æ–°çš„æ¨¡å‹å’Œå·¥å…·
- **å¯æ›¿æ›æ€§**ï¼šåŒé¡å‹çš„çµ„ä»¶å¯ä»¥äº’ç›¸æ›¿æ›

### çµ±ä¸€çš„ä»‹é¢æŠ½è±¡

æ‰€æœ‰ LangChain çµ„ä»¶éƒ½å¯¦ç¾çµ±ä¸€çš„ `Runnable` ä»‹é¢ï¼š

- **invoke()**ï¼šåŒæ­¥åŸ·è¡Œ
- **ainvoke()**ï¼šç•°æ­¥åŸ·è¡Œ  
- **batch()**ï¼šæ‰¹é‡è™•ç†
- **stream()**ï¼šæµå¼è™•ç†

### è²æ˜å¼ç¨‹å¼è¨­è¨ˆ

v0.2+ çš„ LCEL èªæ³•è®“é–‹ç™¼è€…å¯ä»¥ç”¨è²æ˜å¼çš„æ–¹å¼æè¿°è³‡æ–™æµï¼š

```python
# LCEL è²æ˜å¼ vs å‘½ä»¤å¼ç¨‹å¼è¨­è¨ˆæ¯”è¼ƒ

# === è²æ˜å¼å¯«æ³• (LCEL) ===
# ä½¿ç”¨ç®¡é“æ“ä½œç¬¦ | æè¿°è³‡æ–™æµï¼Œç°¡æ½”ç›´è§€
chain = prompt | llm | output_parser

# åŸ·è¡Œè²æ˜å¼éˆ
result = chain.invoke({"question": "ä»€éº¼æ˜¯äººå·¥æ™ºæ…§ï¼Ÿ"})

# === ç­‰æ•ˆçš„å‘½ä»¤å¼å¯«æ³• ===
def imperative_chain(input_data):
    """å‘½ä»¤å¼å¯¦ä½œï¼šæ‰‹å‹•æ§åˆ¶æ¯å€‹æ­¥é©Ÿçš„åŸ·è¡Œé †åº
    
    Args:
        input_data: è¼¸å…¥è³‡æ–™å­—å…¸
        
    Returns:
        è§£æå¾Œçš„æœ€çµ‚çµæœ
    """
    # æ­¥é©Ÿ 1ï¼šæ ¼å¼åŒ–æç¤ºç¯„æœ¬
    print("æ­¥é©Ÿ 1: æ ¼å¼åŒ–æç¤ºç¯„æœ¬...")
    prompt_result = prompt.format(**input_data)
    print(f"æç¤ºå…§å®¹: {prompt_result[:50]}...")
    
    # æ­¥é©Ÿ 2ï¼šèª¿ç”¨ LLM ç”Ÿæˆå›æ‡‰
    print("æ­¥é©Ÿ 2: èª¿ç”¨ LLM ç”Ÿæˆå›æ‡‰...")
    llm_result = llm.invoke(prompt_result)
    print(f"LLM å›æ‡‰: {llm_result.content[:50]}...")
    
    # æ­¥é©Ÿ 3ï¼šè§£æè¼¸å‡ºæ ¼å¼
    print("æ­¥é©Ÿ 3: è§£æè¼¸å‡ºæ ¼å¼...")
    final_result = output_parser.parse(llm_result)
    print(f"æœ€çµ‚çµæœ: {final_result}")
    
    return final_result

# åŸ·è¡Œå‘½ä»¤å¼ç‰ˆæœ¬
result = imperative_chain({"question": "ä»€éº¼æ˜¯äººå·¥æ™ºæ…§ï¼Ÿ"})

# === å…©ç¨®å¯«æ³•çš„æ¯”è¼ƒ ===
"""
è²æ˜å¼ (LCEL) çš„å„ªå‹¢ï¼š
1. ç¨‹å¼ç¢¼ç°¡æ½”ï¼šä¸€è¡Œæè¿°æ•´å€‹è³‡æ–™æµ
2. å¯è®€æ€§é«˜ï¼š| æ“ä½œç¬¦ç›´è§€è¡¨é”ã€Œç„¶å¾Œã€çš„æ¦‚å¿µ
3. è‡ªå‹•æœ€ä½³åŒ–ï¼šæ¡†æ¶å¯ä»¥è‡ªå‹•ä¸¦è¡ŒåŒ–æŸäº›æ“ä½œ
4. å‹åˆ¥æª¢æŸ¥ï¼šç·¨è­¯æ™‚æª¢æŸ¥å„çµ„ä»¶é–“çš„ä»‹é¢ç›¸å®¹æ€§
5. å…§å»ºåŠŸèƒ½ï¼šè‡ªå‹•æ”¯æ´ streamingã€asyncã€batch ç­‰åŠŸèƒ½

å‘½ä»¤å¼çš„ç‰¹é»ï¼š
1. æ§åˆ¶ç²¾ç´°ï¼šå¯ä»¥åœ¨æ¯æ­¥ä¹‹é–“æ’å…¥è‡ªå®šç¾©é‚è¼¯
2. é™¤éŒ¯å®¹æ˜“ï¼šå¯ä»¥å–®æ­¥é™¤éŒ¯æ¯å€‹æ­¥é©Ÿ
3. å‚³çµ±æ€ç¶­ï¼šæ›´ç¬¦åˆå‚³çµ±ç¨‹å¼è¨­è¨ˆç¿’æ…£
4. éˆæ´»åº¦é«˜ï¼šå¯ä»¥åŠ å…¥æ¢ä»¶åˆ¤æ–·å’Œè¿´åœˆæ§åˆ¶
"""

# === LCEL é€²éšåŠŸèƒ½å±•ç¤º ===
from langchain_core.runnables import RunnableParallel

# ä¸¦è¡Œè™•ç†ï¼šåŒæ™‚åŸ·è¡Œå¤šå€‹ä¸åŒçš„åˆ†æ
parallel_chain = RunnableParallel({
    "summary": prompt_summary | llm | output_parser,    # æ‘˜è¦åˆ†æ
    "sentiment": prompt_sentiment | llm | output_parser, # æƒ…æ„Ÿåˆ†æ
    "keywords": prompt_keywords | llm | output_parser   # é—œéµå­—æå–
})

# ä¸€æ¬¡åŸ·è¡Œå¤šå€‹åˆ†æä»»å‹™ï¼Œè‡ªå‹•ä¸¦è¡ŒåŒ–æå‡æ•ˆç‡
parallel_result = parallel_chain.invoke({"text": "é€™æ˜¯ä¸€æ®µè¦åˆ†æçš„æ–‡æœ¬..."}) 
print(f"æ‘˜è¦: {parallel_result['summary']}")
print(f"æƒ…æ„Ÿ: {parallel_result['sentiment']}")
print(f"é—œéµå­—: {parallel_result['keywords']}")
```

## ç¸½çµ

LangChain åŒ…è£çš„å°±æ˜¯ã€ŒLLM é–‹ç™¼çš„é‡è¤‡ç¹ç‘£å·¥ä½œã€ï¼š

- âœ… **çµ±ä¸€çš„æ¨¡å‹ä»‹é¢** - ä¸€å¥—ç¨‹å¼ç¢¼æ”¯æ´å¤šç¨® LLM
- âœ… **æ™ºæ…§è¨˜æ†¶ç®¡ç†** - è‡ªå‹•è™•ç†å°è©±ä¸Šä¸‹æ–‡
- âœ… **æª¢ç´¢å¢å¼·ç”Ÿæˆ** - ç°¡åŒ–å¤–éƒ¨çŸ¥è­˜æ•´åˆ
- âœ… **çµæ§‹åŒ–è¼¸å‡º** - å¯é çš„è³‡æ–™æ ¼å¼è½‰æ›
- âœ… **å·¥å…·èª¿ç”¨æ¡†æ¶** - è®“ LLM èƒ½å¤ åŸ·è¡Œå‹•ä½œ
- âœ… **å¯è§€æ¸¬æ€§æ”¯æ´** - å…§å»ºç›£æ§å’Œé™¤éŒ¯åŠŸèƒ½
- âœ… **æ¨¡çµ„åŒ–è¨­è¨ˆ** - éˆæ´»çµ„åˆå„ç¨®åŠŸèƒ½

è®“ä½ å°ˆæ³¨åœ¨**æ‡‰ç”¨é‚è¼¯å’Œ Prompt è¨­è¨ˆ**ï¼Œè€Œä¸æ˜¯ä¸€ç›´ã€Œé‡é€ è¼ªå­ã€ã€‚

---

::: tip æ·±å…¥å­¸ç¿’
ç¾åœ¨ä½ å·²ç¶“äº†è§£ LangChain çš„æ¶æ§‹èˆ‡æ ¸å¿ƒæ¦‚å¿µï¼Œæ¥ä¸‹ä¾†å¯ä»¥æ·±å…¥å­¸ç¿’å„å€‹å°ˆé–€ä¸»é¡Œï¼š

- [LCEL è¡¨é”å¼èªè¨€](/tutorials/lcel) - å­¸ç¿’ç®¡é“èªæ³•å’Œçµ„åˆæ¨¡å¼
- [LangGraph å·¥ä½œæµ](/tutorials/langgraph) - æŒæ¡å¤šä»£ç†å”ä½œæ¡†æ¶  
- [çµæ§‹åŒ–è¼¸å‡ºè§£æ](/tutorials/output-parsers) - å¯¦ç¾å¯é çš„è³‡æ–™è½‰æ›
- [è¨˜æ†¶æ©Ÿåˆ¶èˆ‡å°è©±ç®¡ç†](/tutorials/memory-systems) - å»ºæ§‹æ™ºæ…§å°è©±ç³»çµ±
- [ç›£æ§èˆ‡å¯è§€æ¸¬æ€§](/tutorials/monitoring) - ç”Ÿç”¢ç’°å¢ƒæœ€ä½³å¯¦è¸
:::

::: warning ç‰ˆæœ¬ç›¸å®¹æ€§æé†’
æœ¬æ–‡æª”å·²æ›´æ–°è‡³ **LangChain v0.2+ æ¨™æº–**ï¼Œä½†æ¡†æ¶ä»åœ¨å¿«é€Ÿç™¼å±•ä¸­ï¼š

- âœ… **å·²æ›´æ–°**ï¼š`create_retrieval_chain`ã€`create_react_agent`ã€LCEL ç®¡é“èªæ³•
- âš ï¸ **æ£„ç”¨ä¸­**ï¼š`RetrievalQA`ã€`initialize_agent`ã€`ConversationalRetrievalChain`
- ğŸ†• **æ–°ç‰¹æ€§**ï¼šLangGraphã€æ›´å¼·çš„ Output Parsersã€LangSmith æ•´åˆ

å»ºè­°æ­¤é †åºæŸ¥çœ‹æœ€æ–°è³‡è¨Šï¼š
1. [å®˜æ–¹æ–‡æª”](https://python.langchain.com/) - æœ€æ–° API åƒè€ƒ
2. [LangGraph æ–‡æª”](https://langchain-ai.github.io/langgraph/) - æ–°ä¸€ä»£ Agent æ¡†æ¶
3. [LangSmith](https://smith.langchain.com/) - å¯è§€æ¸¬æ€§èˆ‡èª¿è©¦å·¥å…·
:::