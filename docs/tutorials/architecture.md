# LangChain æ¶æ§‹èˆ‡æ ¸å¿ƒæ¦‚å¿µ

## LangChain æ¶æ§‹æ¦‚è¦½

LangChain æ¡†æ¶æä¾›äº†ä¸€ç³»åˆ—æ¨¡çµ„åŒ–çš„æŠ½è±¡åŒ–åŠŸèƒ½ï¼ˆmodular abstractionsï¼‰ï¼Œé€™äº›æ˜¯èˆ‡ LLM ä¸€èµ·å·¥ä½œæ‰€å¿…éœ€çš„ï¼ŒåŒæ™‚ä¹Ÿæä¾›äº†å»£æ³›çš„å¯¦ä½œç‰ˆæœ¬ï¼Œæ–¹ä¾¿é–‹ç™¼è€…æ‡‰ç”¨ã€‚

```mermaid
graph TB
    subgraph "LangChain v0.2+ ç¾ä»£æ¶æ§‹"
        subgraph "æ¨¡å‹å±¤"
            LLM[LLM Models<br/>å¤§å‹èªè¨€æ¨¡å‹æ¥å…¥<br/>OpenAI, Anthropic, æœ¬åœ°æ¨¡å‹ç­‰]
        end
        
        subgraph "è¼¸å…¥è™•ç†"
            Prompts[Prompts<br/>æç¤ºè©æ¨¡æ¿ç®¡ç†]
            DocLoad[Document Loaders<br/>æ–‡ä»¶è¼‰å…¥å·¥å…·]
            Splitters[Text Splitters<br/>æ–‡æœ¬åˆ†å‰²å™¨]
        end
        
        subgraph "æ ¸å¿ƒåŸ·è¡Œå¼•æ“ (LCEL)"
            Runnables[Runnables<br/>å¯åŸ·è¡Œæ¥å£]
            Chains[Chains/Pipelines<br/>çµ„åˆå¼æµç¨‹]
            LCEL[LCEL èªæ³•<br/>| ç®¡é“æ“ä½œç¬¦]
        end
        
        subgraph "é€²éšåŠŸèƒ½"
            Retrieval[Retrieval<br/>æª¢ç´¢å¢å¼·ç”Ÿæˆ]
            Memory[Memory<br/>å°è©±è¨˜æ†¶]
            OutputParsers[Output Parsers<br/>çµæ§‹åŒ–è¼¸å‡ºè§£æ]
            Callbacks[Callbacks<br/>LangSmith è¿½è¹¤]
        end
        
        subgraph "æ™ºèƒ½ä»£ç†æ¡†æ¶"
            Agents[Traditional Agents<br/>å·¥å…·èª¿ç”¨ä»£ç†]
            LangGraph[LangGraph<br/>å¤šä»£ç†å·¥ä½œæµ<br/>ç‹€æ…‹æ©Ÿ & æ¢ä»¶è·¯ç”±]
        end
        
        subgraph "å¥—ä»¶ç”Ÿæ…‹"
            Core[langchain-core<br/>æ ¸å¿ƒæŠ½è±¡]
            Community[langchain-community<br/>ç¬¬ä¸‰æ–¹æ•´åˆ]
            Integration[langchain-openai<br/>langchain-chroma<br/>å°ˆç”¨æ•´åˆåŒ…]
        end
        
        %% è³‡æ–™æµå‘
        DocLoad --> Splitters
        Splitters --> Retrieval
        Prompts --> Runnables
        LLM --> Runnables
        Retrieval --> Runnables
        Memory --> Runnables
        
        Runnables --> LCEL
        LCEL --> Chains
        Chains --> OutputParsers
        
        Runnables --> Agents
        Runnables --> LangGraph
        
        %% ç›£æ§èˆ‡è¿½è¹¤
        Chains -.-> Callbacks
        Agents -.-> Callbacks
        LangGraph -.-> Callbacks
        
        %% å¥—ä»¶ä¾è³´
        Core --> Runnables
        Community --> DocLoad
        Integration --> LLM
        Integration --> Retrieval
    end
```

### ä¸»è¦æ¨¡çµ„èªªæ˜

| æ¨¡çµ„ | åŠŸèƒ½èªªæ˜ | å¯¦éš›ç”¨é€” | v0.2+ æ–°ç‰¹æ€§ |
|------|----------|----------|------------|
| **LCEL/Runnables** | å¯çµ„åˆçš„åŸ·è¡Œä»‹é¢ | ç”¨ `\|` æ“ä½œç¬¦é€£æ¥å„çµ„ä»¶ï¼Œå»ºç«‹è³‡æ–™æµç®¡é“ | ğŸ†• æ ¸å¿ƒåŸ·è¡Œå¼•æ“ |
| **LangGraph** | å¤šä»£ç†å·¥ä½œæµæ¡†æ¶ | è¤‡é›œçš„ç‹€æ…‹æ©Ÿã€æ¢ä»¶è·¯ç”±ã€å¤š Agent å”ä½œ | ğŸ†• å–ä»£å‚³çµ± Agent |
| **Output Parsers** | çµæ§‹åŒ–è¼¸å‡ºè§£æ | Pydantic æ¨¡å‹ã€JSON Schema é©—è­‰ | âœ… åŠ å¼·é¡å‹å®‰å…¨ |
| **LLM Models** | å¤§å‹èªè¨€æ¨¡å‹æ¥å…¥ | æ”¯æ´ OpenAI GPTã€Anthropic Claudeã€æœ¬åœ°æ¨¡å‹ç­‰ | âœ… ç¨ç«‹æ•´åˆåŒ… |
| **Prompts** | æç¤ºè©æ¨¡æ¿ç®¡ç† | æ”¯æ´ ChatPromptTemplateã€MessagesPlaceholder | âœ… å¼·åŒ–å°è©±æ”¯æ´ |
| **Document Loaders** | æ–‡ä»¶è¼‰å…¥å·¥å…· | å¾ PDFã€ç¶²é ã€è³‡æ–™åº«ç­‰è¼‰å…¥ä¸¦è™•ç†è³‡æ–™ | âœ… ç§»è‡³ community åŒ… |
| **Retrieval** | æª¢ç´¢å¢å¼·ç”Ÿæˆ | `create_retrieval_chain` å–ä»£èˆŠ RetrievalQA | âœ… LCEL åŸç”Ÿæ”¯æ´ |
| **Memory** | å°è©±è¨˜æ†¶æ©Ÿåˆ¶ | `RunnableWithMessageHistory` æ–°æ¶æ§‹ | âœ… æŒä¹…åŒ–èˆ‡å¤šæœƒè©± |
| **Callbacks** | åŸ·è¡Œç›£æ§è¿½è¹¤ | LangSmith æ•´åˆã€Token è¨ˆç®—ã€æ•ˆèƒ½åˆ†æ | ğŸ†• å¯è§€æ¸¬æ€§ |
| **Traditional Agents** | å·¥å…·èª¿ç”¨ä»£ç† | `create_react_agent` å–ä»£ `initialize_agent` | âš ï¸ å»ºè­°é·ç§»è‡³ LangGraph |

## ä»€éº¼æ˜¯ã€ŒæŠ½è±¡åŒ–ã€ï¼Ÿ

### æ¦‚å¿µè§£é‡‹

åœ¨è»Ÿé«”è¨­è¨ˆè£¡ï¼Œ**æŠ½è±¡åŒ–ï¼ˆAbstractionï¼‰**å°±æ˜¯ï¼š
> éš±è—ç´°ç¯€ï¼Œåªä¿ç•™æœ€å¿…è¦çš„ç‰¹å¾µï¼Œè®“ä½¿ç”¨è€…èƒ½æ›´ç°¡å–®åœ°æ“ä½œã€‚

- **æ²’æœ‰æŠ½è±¡åŒ–** â†’ ä½ è¦è‡ªå·±è™•ç†ä¸€å¤§å †é›œäº‹ï¼ˆä¾‹å¦‚ç›´æ¥å‘¼å« APIï¼Œè¦ç®¡ Tokenã€æ ¼å¼ã€å›å‚³ JSON ç­‰ï¼‰
- **æœ‰æŠ½è±¡åŒ–** â†’ æ¡†æ¶å¹«ä½ æŠŠé›œäº‹åŒ…å¥½ï¼Œçµ¦ä½ ä¸€å€‹ä¹¾æ·¨çš„ä»‹é¢

### å¯¦éš›å°æ¯”

| å ´æ™¯ | æ²’æœ‰æŠ½è±¡åŒ– | æœ‰ LangChain æŠ½è±¡åŒ– |
|------|------------|-------------------|
| ä½¿ç”¨ä¸åŒ LLM | è¦ç‚ºæ¯å€‹ API å¯«ä¸åŒç¨‹å¼ç¢¼ | çµ±ä¸€ä»‹é¢ï¼Œä¸€è¡Œç¨‹å¼ç¢¼åˆ‡æ›æ¨¡å‹ |
| ç®¡ç†å°è©±è¨˜æ†¶ | æ‰‹å‹•å­˜å–è³‡æ–™åº«ï¼Œæ‹¼æ¥ä¸Šä¸‹æ–‡ | æ›ä¸Š Memory æ¨¡çµ„è‡ªå‹•è™•ç† |
| å¤šæ­¥é©Ÿè™•ç† | è‡ªå·±è¨­è¨ˆæµç¨‹æ§åˆ¶é‚è¼¯ | ç”¨ Chain æè¿°æ­¥é©Ÿå³å¯ |

## LangChain åŒ…è£äº†å“ªäº›è¤‡é›œåŠŸèƒ½ï¼Ÿ

### 1. ğŸ”Œ LLM ä»‹æ¥çµ±ä¸€åŒ–

**åŸæœ¬è¤‡é›œï¼š** ä¸åŒå» ç‰Œçš„ LLM API æ ¼å¼å„ç•°ï¼ŒTokenã€å›å‚³æ ¼å¼ã€æµå¼è™•ç†éƒ½ä¸åŒã€‚

**LangChain åŒ…è£ï¼š** æä¾›çµ±ä¸€çš„ä»‹é¢ï¼Œå¯ç„¡ç—›åˆ‡æ›æ¨¡å‹ã€‚

```python
from langchain_openai import ChatOpenAI
from langchain_anthropic import ChatAnthropic

# æ›æ¨¡å‹åªæ›é€™è¡Œï¼Œå…¶ä»–ç¨‹å¼ä¸ç”¨æ”¹
llm = ChatOpenAI(model="gpt-4")  
# æˆ– llm = ChatAnthropic(model="claude-3-opus")

response = llm.invoke("å¹«æˆ‘å¯«ä¸€é¦–è©©")
```

### 2. ğŸ“ Prompt æ¨¡æ¿ç®¡ç†

**åŸæœ¬è¤‡é›œï¼š** è¦è‡ªå·±æ‹¼å­—ä¸²ï¼ŒæŠŠä¸Šä¸‹æ–‡ã€æ ¼å¼ã€è®Šæ•¸å…¨éƒ½å¯«æ­»ã€‚

**LangChain åŒ…è£ï¼š** æä¾› PromptTemplateï¼Œå¯ä»¥ç”¨è®Šæ•¸å¡«å…¥ã€‚

```python
from langchain.prompts import PromptTemplate

template = PromptTemplate.from_template(
    "ä½ æ˜¯ä¸€ä½ç‡Ÿé¤Šå¸«ï¼Œè«‹æ ¹æ“šé€™äº›æ•¸æ“š {health_data} æä¾›å»ºè­°"
)

prompt = template.format(health_data="è¡€ç³–åé«˜")
```

### 3. ğŸ§  Memoryï¼ˆå°è©±è¨˜æ†¶ï¼‰

**åŸæœ¬è¤‡é›œï¼š** LLM å¤©ç”Ÿç„¡è¨˜æ†¶ï¼Œè¦è‡ªå·±ç®¡ç†å°è©±æ­·å²ï¼Œå­˜è³‡æ–™åº«ï¼Œå†æ‰‹å‹•æ‹¼æ¥ã€‚

**LangChain åŒ…è£ï¼š** å…§å»ºå„ç¨® Memory é¡å‹ï¼Œæ›ä¸Šå°±èƒ½è¨˜ä½ä¸Šä¸‹æ–‡ã€‚

```python
# v0.2+ æ–°ç‰ˆå°è©±è¨˜æ†¶åšæ³•ï¼ˆä¿®æ­£ placeholder ç”¨æ³•ï¼‰
from langchain_core.runnables.history import RunnableWithMessageHistory
from langchain_community.chat_message_histories import ChatMessageHistory
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_openai import ChatOpenAI

llm = ChatOpenAI(model="gpt-4o-mini")

# MessagesPlaceholder åº•å±¤å¯¦ä½œä¼°è¨ˆç‚ºå­—å…¸ï¼Œ
"""
[
  {"role": "user", "content": "æˆ‘å«å°æ˜"},
  {"role": "assistant", "content": "å¥½çš„ï¼Œå°æ˜ï¼Œæˆ‘è¨˜ä½äº†ã€‚"}
]
"""
prompt = ChatPromptTemplate.from_messages([
    ("system", "ä½ æ˜¯ä¸€å€‹æœ‰ç”¨çš„åŠ©æ‰‹ï¼Œèƒ½è¨˜ä½å°è©±æ­·å²ã€‚"),
    MessagesPlaceholder(variable_name="chat_history"),   # â† æ­£ç¢ºå¯«æ³•
    ("human", "{input}"),
])

chain = prompt | llm

# ç°¡æ˜“ In-Memory è¨˜æ†¶å­˜æ”¾
store = {}
def get_session_history(session_id: str) -> ChatMessageHistory:
    if session_id not in store:
        store[session_id] = ChatMessageHistory()
    return store[session_id]

conversation = RunnableWithMessageHistory(
    chain,
    get_session_history,
    input_messages_key="input",
    history_messages_key="chat_history",
)

config = {"configurable": {"session_id": "user123"}}
conversation.invoke({"input": "æˆ‘å«å°æ˜"}, config=config)
result = conversation.invoke({"input": "æˆ‘å‰›æ‰èªªæˆ‘å«ä»€éº¼åå­—ï¼Ÿ"}, config=config)
print(result.content)

```

### 4. ğŸ” Retrieval + å¤–éƒ¨çŸ¥è­˜åº«æ•´åˆ

**åŸæœ¬è¤‡é›œï¼š** è¦è‡ªå·±å¯« embeddingã€å­˜åˆ°å‘é‡è³‡æ–™åº«ã€å†å¯«æª¢ç´¢é‚è¼¯ã€‚

**LangChain åŒ…è£ï¼š** æä¾› Retrieverï¼Œä¸€å¥è©±å°±èƒ½è®“ LLM æ¥å¤–éƒ¨çŸ¥è­˜ã€‚

```python
from langchain.chains.retrieval import create_retrieval_chain
from langchain.chains.combine_documents import create_stuff_documents_chain
from langchain_core.prompts import ChatPromptTemplate

# è‡ªå‹•æª¢ç´¢ç›¸é—œæ–‡ä»¶ä¸¦å›ç­”ï¼ˆv0.2+ æ–°ç‰ˆåšæ³•ï¼‰
prompt = ChatPromptTemplate.from_template("æ ¹æ“šä»¥ä¸‹æ–‡ä»¶å›è¦†ï¼š{context}\nå•é¡Œï¼š{input}")
stuff_chain = create_stuff_documents_chain(llm, prompt)
qa = create_retrieval_chain(vectorstore.as_retriever(), stuff_chain)

answer = qa.invoke({"input": "å…¬å¸çš„è«‹å‡æ”¿ç­–æ˜¯ä»€éº¼ï¼Ÿ"})
print(answer["answer"])
```

### 5. â›“ï¸ Chainsï¼ˆå¤šæ­¥é©Ÿæµç¨‹çµ„è£ï¼‰

**åŸæœ¬è¤‡é›œï¼š** è¦æ‰‹å‹•æ§åˆ¶æµç¨‹ï¼šå…ˆæª¢ç´¢è³‡æ–™ â†’ å†å• LLM â†’ å†æ ¼å¼åŒ–çµæœã€‚

**LangChain åŒ…è£ï¼š** æŠŠå¤šæ­¥é©Ÿçµ„è£æˆã€Œæµç¨‹éˆã€ã€‚

```python
# v0.2+ æ–°ç‰ˆåºåˆ—éˆåšæ³•ï¼šä½¿ç”¨ LCEL ç®¡é“èªæ³•
from langchain_core.output_parsers import StrOutputParser
from langchain_core.prompts import PromptTemplate
from langchain_openai import ChatOpenAI

llm = ChatOpenAI(model="gpt-4o-mini")

# å®šç¾©å„æ­¥é©Ÿ
prompt_analysis = PromptTemplate.from_template("åˆ†æä»¥ä¸‹å¥åº·æ•°æ“šï¼š{health_data}")
prompt_recommendation = PromptTemplate.from_template("åŸºæ–¼åˆ†æçµæœ {analysis} æä¾›å…·é«”å»ºè­°")
prompt_format = PromptTemplate.from_template("å°‡ä»¥ä¸‹å»ºè­° {recommendations} æ ¼å¼åŒ–ç‚ºç”¨æˆ¶å‹å¥½çš„å ±å‘Š")

# ä½¿ç”¨ LCEL ç®¡é“èªæ³•ä¸²æ¥ï¼ˆ| æ“ä½œç¬¦ï¼‰
analysis_chain = prompt_analysis | llm | StrOutputParser()
recommendation_chain = prompt_recommendation | llm | StrOutputParser()
format_chain = prompt_format | llm | StrOutputParser()

# å®Œæ•´çš„å¥åº·åˆ†ææµç¨‹
def health_analysis_pipeline(health_data: str):
    analysis = analysis_chain.invoke({"health_data": health_data})
    recommendations = recommendation_chain.invoke({"analysis": analysis})
    final_report = format_chain.invoke({"recommendations": recommendations})
    return final_report

# ä½¿ç”¨ç¯„ä¾‹
result = health_analysis_pipeline("è¡€ç³–åé«˜ 130mg/dL, BMI 25.5, é‹å‹•é‡å°‘")
print(result)
```

### 6. ğŸ¯ Agentsï¼ˆå‹•æ…‹æ±ºç­– & å·¥å…·èª¿ç”¨ï¼‰

**åŸæœ¬è¤‡é›œï¼š** è¦è‡ªå·±å¯« if/else åˆ¤æ–·ï¼Œæ±ºå®šä½•æ™‚è©²æŸ¥ APIã€ä½•æ™‚ç›´æ¥å›è¦†ã€‚

**LangChain åŒ…è£ï¼š** LLM è‡ªä¸»æ±ºå®šè©²èª¿ç”¨å“ªå€‹å·¥å…·ã€‚

```python
from langchain.agents import create_tool_calling_agent, AgentExecutor
from langchain.tools import Tool
from langchain_core.prompts import ChatPromptTemplate

# å®‰å…¨è¨ˆç®—å™¨ï¼ˆè©³ç´°å¯¦ç¾è«‹åƒè€ƒä»‹ç´¹ç« ç¯€ï¼‰
def safe_calculate(expression: str):
    """å®‰å…¨çš„æ•¸å­¸è¨ˆç®—å™¨ï¼Œé¿å…ä½¿ç”¨ eval()"""
    # å¯¦ç¾å®‰å…¨çš„æ•¸å­¸è¨ˆç®—é‚è¼¯
    return f"è¨ˆç®—çµæœ: {expression}"

# å®‰å…¨çš„å·¥å…·é›†
tools = [
    Tool(name="å®‰å…¨è¨ˆç®—", func=safe_calculate, description="å®‰å…¨çš„æ•¸å­¸è¨ˆç®—ï¼ˆåªå…è¨± +, -, *, /, ** é‹ç®—ï¼‰")
]

llm = ChatOpenAI(model="gpt-4o-mini", temperature=0)
prompt = ChatPromptTemplate.from_messages([
    ("system", "ä½ æ˜¯ä¸€å€‹æœ‰ç”¨çš„åŠ©æ‰‹ã€‚ä½¿ç”¨æä¾›çš„å·¥å…·ä¾†å®Œæˆä»»å‹™ã€‚"),
    ("user", "{input}"),
    ("assistant", "{agent_scratchpad}"),
])

# LLM èƒ½è‡ªä¸»æ±ºå®šä½¿ç”¨å“ªå€‹å·¥å…·
agent = create_tool_calling_agent(llm, tools, prompt)
agent_executor = AgentExecutor(agent=agent, tools=tools, verbose=True)

# è¤‡é›œä»»å‹™è‡ªä¸»åˆ†è§£èˆ‡åŸ·è¡Œ
response = agent_executor.invoke({
    "input": "å¹«æˆ‘è¨ˆç®— (100+50)*2 çš„çµæœ"
})
print(response["output"])
```

## æ ¸å¿ƒæ¶æ§‹ç†å¿µ

### æ¨¡çµ„åŒ–è¨­è¨ˆ

LangChain æ¡ç”¨æ¨¡çµ„åŒ–è¨­è¨ˆï¼Œæ¯å€‹çµ„ä»¶éƒ½å¯ä»¥ç¨ç«‹ä½¿ç”¨æˆ–çµ„åˆä½¿ç”¨ï¼š

- **å¯çµ„åˆæ€§**ï¼šä¸åŒæ¨¡çµ„å¯ä»¥éˆæ´»çµ„åˆ
- **å¯æ“´å±•æ€§**ï¼šå®¹æ˜“æ·»åŠ æ–°çš„æ¨¡å‹å’Œå·¥å…·
- **å¯æ›¿æ›æ€§**ï¼šåŒé¡å‹çš„çµ„ä»¶å¯ä»¥äº’ç›¸æ›¿æ›

### çµ±ä¸€çš„ä»‹é¢æŠ½è±¡

æ‰€æœ‰ LangChain çµ„ä»¶éƒ½å¯¦ç¾çµ±ä¸€çš„ `Runnable` ä»‹é¢ï¼š

- **invoke()**ï¼šåŒæ­¥åŸ·è¡Œ
- **ainvoke()**ï¼šç•°æ­¥åŸ·è¡Œ  
- **batch()**ï¼šæ‰¹é‡è™•ç†
- **stream()**ï¼šæµå¼è™•ç†

### è²æ˜å¼ç¨‹å¼è¨­è¨ˆ

v0.2+ çš„ LCEL èªæ³•è®“é–‹ç™¼è€…å¯ä»¥ç”¨è²æ˜å¼çš„æ–¹å¼æè¿°è³‡æ–™æµï¼š

```python
# è²æ˜å¼çš„è³‡æ–™æµæè¿°
chain = prompt | llm | output_parser

# ç­‰æ•ˆæ–¼å‘½ä»¤å¼çš„å¯«æ³•
def imperative_chain(input_data):
    prompt_result = prompt.format(**input_data)
    llm_result = llm.invoke(prompt_result)
    final_result = output_parser.parse(llm_result)
    return final_result
```

## ç¸½çµ

LangChain åŒ…è£çš„å°±æ˜¯ã€ŒLLM é–‹ç™¼çš„é‡è¤‡ç¹ç‘£å·¥ä½œã€ï¼š

- âœ… **çµ±ä¸€çš„æ¨¡å‹ä»‹é¢** - ä¸€å¥—ç¨‹å¼ç¢¼æ”¯æ´å¤šç¨® LLM
- âœ… **æ™ºæ…§è¨˜æ†¶ç®¡ç†** - è‡ªå‹•è™•ç†å°è©±ä¸Šä¸‹æ–‡
- âœ… **æª¢ç´¢å¢å¼·ç”Ÿæˆ** - ç°¡åŒ–å¤–éƒ¨çŸ¥è­˜æ•´åˆ
- âœ… **çµæ§‹åŒ–è¼¸å‡º** - å¯é çš„è³‡æ–™æ ¼å¼è½‰æ›
- âœ… **å·¥å…·èª¿ç”¨æ¡†æ¶** - è®“ LLM èƒ½å¤ åŸ·è¡Œå‹•ä½œ
- âœ… **å¯è§€æ¸¬æ€§æ”¯æ´** - å…§å»ºç›£æ§å’Œé™¤éŒ¯åŠŸèƒ½
- âœ… **æ¨¡çµ„åŒ–è¨­è¨ˆ** - éˆæ´»çµ„åˆå„ç¨®åŠŸèƒ½

è®“ä½ å°ˆæ³¨åœ¨**æ‡‰ç”¨é‚è¼¯å’Œ Prompt è¨­è¨ˆ**ï¼Œè€Œä¸æ˜¯ä¸€ç›´ã€Œé‡é€ è¼ªå­ã€ã€‚

---

::: tip æ·±å…¥å­¸ç¿’
ç¾åœ¨ä½ å·²ç¶“äº†è§£ LangChain çš„æ¶æ§‹èˆ‡æ ¸å¿ƒæ¦‚å¿µï¼Œæ¥ä¸‹ä¾†å¯ä»¥æ·±å…¥å­¸ç¿’å„å€‹å°ˆé–€ä¸»é¡Œï¼š

- [LCEL è¡¨é”å¼èªè¨€](/tutorials/lcel) - å­¸ç¿’ç®¡é“èªæ³•å’Œçµ„åˆæ¨¡å¼
- [LangGraph å·¥ä½œæµ](/tutorials/langgraph) - æŒæ¡å¤šä»£ç†å”ä½œæ¡†æ¶  
- [çµæ§‹åŒ–è¼¸å‡ºè§£æ](/tutorials/output-parsers) - å¯¦ç¾å¯é çš„è³‡æ–™è½‰æ›
- [è¨˜æ†¶æ©Ÿåˆ¶èˆ‡å°è©±ç®¡ç†](/tutorials/memory-systems) - å»ºæ§‹æ™ºæ…§å°è©±ç³»çµ±
- [ç›£æ§èˆ‡å¯è§€æ¸¬æ€§](/tutorials/monitoring) - ç”Ÿç”¢ç’°å¢ƒæœ€ä½³å¯¦è¸
:::

::: warning ç‰ˆæœ¬ç›¸å®¹æ€§æé†’
æœ¬æ–‡æª”å·²æ›´æ–°è‡³ **LangChain v0.2+ æ¨™æº–**ï¼Œä½†æ¡†æ¶ä»åœ¨å¿«é€Ÿç™¼å±•ä¸­ï¼š

- âœ… **å·²æ›´æ–°**ï¼š`create_retrieval_chain`ã€`create_react_agent`ã€LCEL ç®¡é“èªæ³•
- âš ï¸ **æ£„ç”¨ä¸­**ï¼š`RetrievalQA`ã€`initialize_agent`ã€`ConversationalRetrievalChain`
- ğŸ†• **æ–°ç‰¹æ€§**ï¼šLangGraphã€æ›´å¼·çš„ Output Parsersã€LangSmith æ•´åˆ

å»ºè­°æ­¤é †åºæŸ¥çœ‹æœ€æ–°è³‡è¨Šï¼š
1. [å®˜æ–¹æ–‡æª”](https://python.langchain.com/) - æœ€æ–° API åƒè€ƒ
2. [LangGraph æ–‡æª”](https://langchain-ai.github.io/langgraph/) - æ–°ä¸€ä»£ Agent æ¡†æ¶
3. [LangSmith](https://smith.langchain.com/) - å¯è§€æ¸¬æ€§èˆ‡èª¿è©¦å·¥å…·
:::