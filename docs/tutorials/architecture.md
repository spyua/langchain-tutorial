# LangChain æ¶æ§‹èˆ‡æ ¸å¿ƒæ¦‚å¿µ

## LangChain æ¶æ§‹æ¦‚è¦½

LangChain æ¡†æ¶æä¾›äº†ä¸€ç³»åˆ—æ¨¡çµ„åŒ–çš„æŠ½è±¡åŒ–åŠŸèƒ½ï¼ˆmodular abstractionsï¼‰ï¼Œé€™äº›æ˜¯èˆ‡ LLM ä¸€èµ·å·¥ä½œæ‰€å¿…éœ€çš„ï¼ŒåŒæ™‚ä¹Ÿæä¾›äº†å»£æ³›çš„å¯¦ä½œç‰ˆæœ¬ï¼Œæ–¹ä¾¿é–‹ç™¼è€…æ‡‰ç”¨ã€‚

```mermaid
graph TB
    subgraph "LangChain v0.2+ ç¾ä»£æ¶æ§‹"
        subgraph "æ¨¡å‹å±¤"
            LLM[LLM Models<br/>å¤§å‹èªè¨€æ¨¡å‹æ¥å…¥<br/>OpenAI, Anthropic, æœ¬åœ°æ¨¡å‹ç­‰]
        end
        
        subgraph "è¼¸å…¥è™•ç†"
            Prompts[Prompts<br/>æç¤ºè©æ¨¡æ¿ç®¡ç†]
            DocLoad[Document Loaders<br/>æ–‡ä»¶è¼‰å…¥å·¥å…·]
            Splitters[Text Splitters<br/>æ–‡æœ¬åˆ†å‰²å™¨]
        end
        
        subgraph "æ ¸å¿ƒåŸ·è¡Œå¼•æ“ (LCEL)"
            Runnables[Runnables<br/>å¯åŸ·è¡Œæ¥å£]
            Chains[Chains/Pipelines<br/>çµ„åˆå¼æµç¨‹]
            LCEL[LCEL èªæ³•<br/>| ç®¡é“æ“ä½œç¬¦]
        end
        
        subgraph "é€²éšåŠŸèƒ½"
            Retrieval[Retrieval<br/>æª¢ç´¢å¢å¼·ç”Ÿæˆ]
            Memory[Memory<br/>å°è©±è¨˜æ†¶]
            OutputParsers[Output Parsers<br/>çµæ§‹åŒ–è¼¸å‡ºè§£æ]
            Callbacks[Callbacks<br/>LangSmith è¿½è¹¤]
        end
        
        subgraph "æ™ºèƒ½ä»£ç†æ¡†æ¶"
            Agents[Traditional Agents<br/>å·¥å…·èª¿ç”¨ä»£ç†]
            LangGraph[LangGraph<br/>å¤šä»£ç†å·¥ä½œæµ<br/>ç‹€æ…‹æ©Ÿ & æ¢ä»¶è·¯ç”±]
        end
        
        subgraph "å¥—ä»¶ç”Ÿæ…‹"
            Core[langchain-core<br/>æ ¸å¿ƒæŠ½è±¡]
            Community[langchain-community<br/>ç¬¬ä¸‰æ–¹æ•´åˆ]
            Integration[langchain-openai<br/>langchain-chroma<br/>å°ˆç”¨æ•´åˆåŒ…]
        end
        
        %% è³‡æ–™æµå‘
        DocLoad --> Splitters
        Splitters --> Retrieval
        Prompts --> Runnables
        LLM --> Runnables
        Retrieval --> Runnables
        Memory --> Runnables
        
        Runnables --> LCEL
        LCEL --> Chains
        Chains --> OutputParsers
        
        Runnables --> Agents
        Runnables --> LangGraph
        
        %% ç›£æ§èˆ‡è¿½è¹¤
        Chains -.-> Callbacks
        Agents -.-> Callbacks
        LangGraph -.-> Callbacks
        
        %% å¥—ä»¶ä¾è³´
        Core --> Runnables
        Community --> DocLoad
        Integration --> LLM
        Integration --> Retrieval
    end
```

### ä¸»è¦æ¨¡çµ„èªªæ˜

| æ¨¡çµ„ | åŠŸèƒ½èªªæ˜ | å¯¦éš›ç”¨é€” | v0.2+ æ–°ç‰¹æ€§ |
|------|----------|----------|------------|
| **LCEL/Runnables** | å¯çµ„åˆçš„åŸ·è¡Œä»‹é¢ | ç”¨ `\|` æ“ä½œç¬¦é€£æ¥å„çµ„ä»¶ï¼Œå»ºç«‹è³‡æ–™æµç®¡é“ | ğŸ†• æ ¸å¿ƒåŸ·è¡Œå¼•æ“ |
| **LangGraph** | å¤šä»£ç†å·¥ä½œæµæ¡†æ¶ | è¤‡é›œçš„ç‹€æ…‹æ©Ÿã€æ¢ä»¶è·¯ç”±ã€å¤š Agent å”ä½œ | ğŸ†• å–ä»£å‚³çµ± Agent |
| **Output Parsers** | çµæ§‹åŒ–è¼¸å‡ºè§£æ | Pydantic æ¨¡å‹ã€JSON Schema é©—è­‰ | âœ… åŠ å¼·é¡å‹å®‰å…¨ |
| **LLM Models** | å¤§å‹èªè¨€æ¨¡å‹æ¥å…¥ | æ”¯æ´ OpenAI GPTã€Anthropic Claudeã€æœ¬åœ°æ¨¡å‹ç­‰ | âœ… ç¨ç«‹æ•´åˆåŒ… |
| **Prompts** | æç¤ºè©æ¨¡æ¿ç®¡ç† | æ”¯æ´ ChatPromptTemplateã€MessagesPlaceholder | âœ… å¼·åŒ–å°è©±æ”¯æ´ |
| **Document Loaders** | æ–‡ä»¶è¼‰å…¥å·¥å…· | å¾ PDFã€ç¶²é ã€è³‡æ–™åº«ç­‰è¼‰å…¥ä¸¦è™•ç†è³‡æ–™ | âœ… ç§»è‡³ community åŒ… |
| **Retrieval** | æª¢ç´¢å¢å¼·ç”Ÿæˆ | `create_retrieval_chain` å–ä»£èˆŠ RetrievalQA | âœ… LCEL åŸç”Ÿæ”¯æ´ |
| **Memory** | å°è©±è¨˜æ†¶æ©Ÿåˆ¶ | `RunnableWithMessageHistory` æ–°æ¶æ§‹ | âœ… æŒä¹…åŒ–èˆ‡å¤šæœƒè©± |
| **Callbacks** | åŸ·è¡Œç›£æ§è¿½è¹¤ | LangSmith æ•´åˆã€Token è¨ˆç®—ã€æ•ˆèƒ½åˆ†æ | ğŸ†• å¯è§€æ¸¬æ€§ |
| **Traditional Agents** | å·¥å…·èª¿ç”¨ä»£ç† | `create_react_agent` å–ä»£ `initialize_agent` | âš ï¸ å»ºè­°é·ç§»è‡³ LangGraph |

## ä»€éº¼æ˜¯ã€ŒæŠ½è±¡åŒ–ã€ï¼Ÿ

### æ¦‚å¿µè§£é‡‹

åœ¨è»Ÿé«”è¨­è¨ˆè£¡ï¼Œ**æŠ½è±¡åŒ–ï¼ˆAbstractionï¼‰**å°±æ˜¯ï¼š
> éš±è—ç´°ç¯€ï¼Œåªä¿ç•™æœ€å¿…è¦çš„ç‰¹å¾µï¼Œè®“ä½¿ç”¨è€…èƒ½æ›´ç°¡å–®åœ°æ“ä½œã€‚

- **æ²’æœ‰æŠ½è±¡åŒ–** â†’ ä½ è¦è‡ªå·±è™•ç†ä¸€å¤§å †é›œäº‹ï¼ˆä¾‹å¦‚ç›´æ¥å‘¼å« APIï¼Œè¦ç®¡ Tokenã€æ ¼å¼ã€å›å‚³ JSON ç­‰ï¼‰
- **æœ‰æŠ½è±¡åŒ–** â†’ æ¡†æ¶å¹«ä½ æŠŠé›œäº‹åŒ…å¥½ï¼Œçµ¦ä½ ä¸€å€‹ä¹¾æ·¨çš„ä»‹é¢

### å¯¦éš›å°æ¯”

| å ´æ™¯ | æ²’æœ‰æŠ½è±¡åŒ– | æœ‰ LangChain æŠ½è±¡åŒ– |
|------|------------|-------------------|
| ä½¿ç”¨ä¸åŒ LLM | è¦ç‚ºæ¯å€‹ API å¯«ä¸åŒç¨‹å¼ç¢¼ | çµ±ä¸€ä»‹é¢ï¼Œä¸€è¡Œç¨‹å¼ç¢¼åˆ‡æ›æ¨¡å‹ |
| ç®¡ç†å°è©±è¨˜æ†¶ | æ‰‹å‹•å­˜å–è³‡æ–™åº«ï¼Œæ‹¼æ¥ä¸Šä¸‹æ–‡ | æ›ä¸Š Memory æ¨¡çµ„è‡ªå‹•è™•ç† |
| å¤šæ­¥é©Ÿè™•ç† | è‡ªå·±è¨­è¨ˆæµç¨‹æ§åˆ¶é‚è¼¯ | ç”¨ Chain æè¿°æ­¥é©Ÿå³å¯ |

## LangChain åŒ…è£äº†å“ªäº›è¤‡é›œåŠŸèƒ½ï¼Ÿ

### 1. ğŸ”Œ LLM ä»‹æ¥çµ±ä¸€åŒ–

**åŸæœ¬è¤‡é›œï¼š** ä¸åŒå» ç‰Œçš„ LLM API æ ¼å¼å„ç•°ï¼ŒTokenã€å›å‚³æ ¼å¼ã€æµå¼è™•ç†éƒ½ä¸åŒã€‚

**LangChain åŒ…è£ï¼š** æä¾›çµ±ä¸€çš„ä»‹é¢ï¼Œå¯ç„¡ç—›åˆ‡æ›æ¨¡å‹ã€‚

```python
from langchain_openai import ChatOpenAI
from langchain_anthropic import ChatAnthropic

# æ›æ¨¡å‹åªæ›é€™è¡Œï¼Œå…¶ä»–ç¨‹å¼ä¸ç”¨æ”¹
llm = ChatOpenAI(model="gpt-4")  
# æˆ– llm = ChatAnthropic(model="claude-3-opus")

response = llm.invoke("å¹«æˆ‘å¯«ä¸€é¦–è©©")
```

### 2. ğŸ“ Prompt æ¨¡æ¿ç®¡ç†

**åŸæœ¬è¤‡é›œï¼š** è¦è‡ªå·±æ‹¼å­—ä¸²ï¼ŒæŠŠä¸Šä¸‹æ–‡ã€æ ¼å¼ã€è®Šæ•¸å…¨éƒ½å¯«æ­»ã€‚

**LangChain åŒ…è£ï¼š** æä¾› PromptTemplateï¼Œå¯ä»¥ç”¨è®Šæ•¸å¡«å…¥ã€‚

```python
from langchain.prompts import PromptTemplate

template = PromptTemplate.from_template(
    "ä½ æ˜¯ä¸€ä½ç‡Ÿé¤Šå¸«ï¼Œè«‹æ ¹æ“šé€™äº›æ•¸æ“š {health_data} æä¾›å»ºè­°"
)

prompt = template.format(health_data="è¡€ç³–åé«˜")
```

### 3. ğŸ§  Memoryï¼ˆå°è©±è¨˜æ†¶ï¼‰

**åŸæœ¬è¤‡é›œï¼š** LLM å¤©ç”Ÿç„¡è¨˜æ†¶ï¼Œè¦è‡ªå·±ç®¡ç†å°è©±æ­·å²ï¼Œå­˜è³‡æ–™åº«ï¼Œå†æ‰‹å‹•æ‹¼æ¥ã€‚

**LangChain åŒ…è£ï¼š** å…§å»ºå„ç¨® Memory é¡å‹ï¼Œæ›ä¸Šå°±èƒ½è¨˜ä½ä¸Šä¸‹æ–‡ã€‚

```python
# v0.2+ æ–°ç‰ˆå°è©±è¨˜æ†¶åšæ³•
from langchain_core.runnables.history import RunnableWithMessageHistory
from langchain_community.chat_message_histories import ChatMessageHistory
from langchain_core.prompts import ChatPromptTemplate
from langchain_openai import ChatOpenAI

# ç°¡å–®çš„å°è©±éˆ
llm = ChatOpenAI(model="gpt-4o-mini")
prompt = ChatPromptTemplate.from_messages([
    ("system", "ä½ æ˜¯ä¸€å€‹æœ‰ç”¨çš„åŠ©æ‰‹ï¼Œèƒ½è¨˜ä½å°è©±æ­·å²ã€‚"),
    ("placeholder", "{chat_history}"),
    ("human", "{input}"),
])
chain = prompt | llm

# è¨˜æ†¶å„²å­˜
store = {}
def get_session_history(session_id: str) -> ChatMessageHistory:
    if session_id not in store:
        store[session_id] = ChatMessageHistory()
    return store[session_id]

# å¸¶è¨˜æ†¶çš„å°è©±
conversation = RunnableWithMessageHistory(
    chain,
    get_session_history,
    input_messages_key="input",
    history_messages_key="chat_history",
)

# è‡ªå‹•è¨˜ä½ä¸Šä¸‹æ–‡
config = {"configurable": {"session_id": "user123"}}
conversation.invoke({"input": "æˆ‘å«å°æ˜"}, config=config)
result = conversation.invoke({"input": "æˆ‘å‰›æ‰èªªæˆ‘å«ä»€éº¼åå­—ï¼Ÿ"}, config=config)
print(result.content)  # æœƒè¨˜å¾—æ˜¯å°æ˜
```

### 4. ğŸ” Retrieval + å¤–éƒ¨çŸ¥è­˜åº«æ•´åˆ

**åŸæœ¬è¤‡é›œï¼š** è¦è‡ªå·±å¯« embeddingã€å­˜åˆ°å‘é‡è³‡æ–™åº«ã€å†å¯«æª¢ç´¢é‚è¼¯ã€‚

**LangChain åŒ…è£ï¼š** æä¾› Retrieverï¼Œä¸€å¥è©±å°±èƒ½è®“ LLM æ¥å¤–éƒ¨çŸ¥è­˜ã€‚

```python
from langchain.chains.retrieval import create_retrieval_chain
from langchain.chains.combine_documents import create_stuff_documents_chain
from langchain_core.prompts import ChatPromptTemplate

# è‡ªå‹•æª¢ç´¢ç›¸é—œæ–‡ä»¶ä¸¦å›ç­”ï¼ˆv0.2+ æ–°ç‰ˆåšæ³•ï¼‰
prompt = ChatPromptTemplate.from_template("æ ¹æ“šä»¥ä¸‹æ–‡ä»¶å›è¦†ï¼š{context}\nå•é¡Œï¼š{input}")
stuff_chain = create_stuff_documents_chain(llm, prompt)
qa = create_retrieval_chain(vectorstore.as_retriever(), stuff_chain)

answer = qa.invoke({"input": "å…¬å¸çš„è«‹å‡æ”¿ç­–æ˜¯ä»€éº¼ï¼Ÿ"})
print(answer["answer"])
```

### 5. â›“ï¸ Chainsï¼ˆå¤šæ­¥é©Ÿæµç¨‹çµ„è£ï¼‰

**åŸæœ¬è¤‡é›œï¼š** è¦æ‰‹å‹•æ§åˆ¶æµç¨‹ï¼šå…ˆæª¢ç´¢è³‡æ–™ â†’ å†å• LLM â†’ å†æ ¼å¼åŒ–çµæœã€‚

**LangChain åŒ…è£ï¼š** æŠŠå¤šæ­¥é©Ÿçµ„è£æˆã€Œæµç¨‹éˆã€ã€‚

```python
# v0.2+ æ–°ç‰ˆåºåˆ—éˆåšæ³•ï¼šä½¿ç”¨ LCEL ç®¡é“èªæ³•
from langchain_core.output_parsers import StrOutputParser
from langchain_core.prompts import PromptTemplate
from langchain_openai import ChatOpenAI

llm = ChatOpenAI(model="gpt-4o-mini")

# å®šç¾©å„æ­¥é©Ÿ
prompt_analysis = PromptTemplate.from_template("åˆ†æä»¥ä¸‹å¥åº·æ•°æ“šï¼š{health_data}")
prompt_recommendation = PromptTemplate.from_template("åŸºæ–¼åˆ†æçµæœ {analysis} æä¾›å…·é«”å»ºè­°")
prompt_format = PromptTemplate.from_template("å°‡ä»¥ä¸‹å»ºè­° {recommendations} æ ¼å¼åŒ–ç‚ºç”¨æˆ¶å‹å¥½çš„å ±å‘Š")

# ä½¿ç”¨ LCEL ç®¡é“èªæ³•ä¸²æ¥ï¼ˆ| æ“ä½œç¬¦ï¼‰
analysis_chain = prompt_analysis | llm | StrOutputParser()
recommendation_chain = prompt_recommendation | llm | StrOutputParser()
format_chain = prompt_format | llm | StrOutputParser()

# å®Œæ•´çš„å¥åº·åˆ†ææµç¨‹
def health_analysis_pipeline(health_data: str):
    analysis = analysis_chain.invoke({"health_data": health_data})
    recommendations = recommendation_chain.invoke({"analysis": analysis})
    final_report = format_chain.invoke({"recommendations": recommendations})
    return final_report

# ä½¿ç”¨ç¯„ä¾‹
result = health_analysis_pipeline("è¡€ç³– 120 mg/dL, BMI 25.5, é‹å‹•é‡å°‘")
print(result)
```

### 6. ğŸ¯ Agentsï¼ˆå‹•æ…‹æ±ºç­– & å·¥å…·èª¿ç”¨ï¼‰

**åŸæœ¬è¤‡é›œï¼š** è¦è‡ªå·±å¯« if/else åˆ¤æ–·ï¼Œæ±ºå®šä½•æ™‚è©²æŸ¥ APIã€ä½•æ™‚ç›´æ¥å›è¦†ã€‚

**LangChain åŒ…è£ï¼š** LLM è‡ªä¸»æ±ºå®šè©²èª¿ç”¨å“ªå€‹å·¥å…·ã€‚

```python
from langchain.agents import AgentExecutor

# LLM å¯æ ¹æ“šå•é¡Œæ±ºå®šï¼š
# - æŸ¥å¤©æ°£ API
# - æŸ¥è³‡æ–™åº«  
# - æˆ–ç›´æ¥å›ç­”
agent = AgentExecutor.from_agent_and_tools(
    agent=agent,
    tools=[weather_tool, database_tool]
)
```

## å¯¦éš›æ‡‰ç”¨å ´æ™¯

### å ´æ™¯ä¸€ï¼šå¥åº· AI åŠ©æ‰‹

**æ²’æœ‰ LangChain çš„è¤‡é›œåº¦ï¼š**
- æ‰‹å‹•ä¸²æ¥ OpenAI API
- è‡ªå·±å¯«ç¨‹å¼è™•ç†ä¸Šä¸‹æ–‡
- è‡ªå·±å¯¦ä½œ embedding + å­˜ Firestore  
- æ‰‹å¯« prompt æ‹¼æ¥é‚è¼¯
- è¨­è¨ˆè¤‡é›œçš„ API workflow

**ä½¿ç”¨ LangChain çš„ç°¡åŒ–ï¼š**
- `ChatOpenAI` æŠ½è±¡å±¤è™•ç† API
- `ConversationBufferMemory` è™•ç†å°è©±
- `RetrievalQA` é€£æ¥ BigQuery æˆ– Firestore
- `PromptTemplate` ç®¡ç†å¥åº·å»ºè­°æ ¼å¼
- `Agent` è®“ LLM è‡ªå‹•æ±ºå®šè¦ã€ŒæŸ¥è©¢æ•¸æ“šã€é‚„æ˜¯ã€Œç›´æ¥å»ºè­°ã€

### å ´æ™¯äºŒï¼šå®¢æœæ©Ÿå™¨äºº

```python
# å®Œæ•´çš„å®¢æœæ©Ÿå™¨äººï¼ˆv0.2+ æ–°ç‰ˆåšæ³•ï¼‰
from langchain.chains.retrieval import create_retrieval_chain
from langchain.chains.combine_documents import create_stuff_documents_chain
from langchain.chains.history_aware_retriever import create_history_aware_retriever
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_core.runnables.history import RunnableWithMessageHistory
from langchain_community.chat_message_histories import ChatMessageHistory

# å»ºç«‹æ­·å²æ„ŸçŸ¥æª¢ç´¢å™¨
contextualize_q_system_prompt = """çµ¦å®šèŠå¤©æ­·å²å’Œæœ€æ–°çš„ç”¨æˆ¶å•é¡Œï¼Œ
å¦‚æœå•é¡Œæ¶µåŠèŠå¤©æ­·å²ï¼Œè«‹é‡æ–°è¡¨è¿°ä¸€å€‹ç¨ç«‹çš„å•é¡Œã€‚"""
contextualize_q_prompt = ChatPromptTemplate.from_messages([
    ("system", contextualize_q_system_prompt),
    MessagesPlaceholder("chat_history"),
    ("human", "{input}"),
])

# æ­·å²æ„ŸçŸ¥çš„æª¢ç´¢å™¨
history_aware_retriever = create_history_aware_retriever(
    llm, company_docs.as_retriever(), contextualize_q_prompt
)

# å•ç­”æç¤ºè©
qa_system_prompt = """ä½ æ˜¯ä¸€å€‹å•ç­”åŠ©æ‰‹ã€‚ä½¿ç”¨ä»¥ä¸‹æª¢ç´¢åˆ°çš„ä¸Šä¸‹æ–‡ä¾†å›ç­”å•é¡Œã€‚

{context}"""
qa_prompt = ChatPromptTemplate.from_messages([
    ("system", qa_system_prompt),
    MessagesPlaceholder("chat_history"),
    ("human", "{input}"),
])

# å‰µå»ºå•ç­”éˆ
question_answer_chain = create_stuff_documents_chain(llm, qa_prompt)
rag_chain = create_retrieval_chain(history_aware_retriever, question_answer_chain)

# è¨˜æ†¶å„²å­˜
store = {}

def get_session_history(session_id: str) -> ChatMessageHistory:
    if session_id not in store:
        store[session_id] = ChatMessageHistory()
    return store[session_id]

# å¸¶è¨˜æ†¶çš„å°è©±éˆ
chatbot = RunnableWithMessageHistory(
    rag_chain,
    get_session_history,
    input_messages_key="input",
    history_messages_key="chat_history",
    output_messages_key="answer",
)

# è™•ç†å¤šè¼ªå°è©±
response = chatbot.invoke(
    {"input": "å¦‚ä½•ç”³è«‹é€€è²¨ï¼Ÿ"},
    config={"configurable": {"session_id": "user_001"}}
)
print(response["answer"])
```

## ç™½è©±ç†è§£

**ç°¡å–®ä¾†èªª**ï¼ŒLangChain å°±åƒæ˜¯ä¸€å€‹ã€ŒAI æ‡‰ç”¨ç¨‹å¼é–‹ç™¼æ¡†æ¶ã€ã€‚

å®ƒçš„ç›®çš„ä¸æ˜¯è®“ä½ åªå–®ç´”å• LLM å•é¡Œï¼Œè€Œæ˜¯è®“ LLM å¯ä»¥ï¼š
- ğŸ“– è®€å¤–éƒ¨è³‡æ–™
- ğŸ§  è¨˜ä½ä¸Šä¸‹æ–‡  
- ğŸ¤” æ±ºå®šè¡Œå‹•
- ğŸ”— å’Œå…¶ä»–ç³»çµ±äº’å‹•

### é¡æ¯”èªªæ˜

å¦‚æœæŠŠ LangChain æƒ³æˆ AI ç•Œçš„ã€ŒSpring Bootã€æˆ–ã€ŒDjangoã€ï¼š

- **Spring Boot** æŠ½è±¡åŒ–ï¼šä¸ç”¨è‡ªå·±å¯« Servletã€è™•ç† Request/Response
- **LangChain** æŠ½è±¡åŒ–ï¼šä¸ç”¨è‡ªå·±è™•ç† Promptã€APIã€Memoryã€çŸ¥è­˜åº«æª¢ç´¢

## LangChain èˆ‡ Prompt å·¥ç¨‹çš„é—œä¿‚

### å±¤ç´šå·®ç•°ç†è§£

å¯ä»¥æŠŠé—œä¿‚ç†è§£æˆï¼š

**Prompt å·¥ç¨‹**ï¼šæ˜¯**å¾®è§€å±¤ç´š**çš„æŠ€å·§ï¼Œå°ˆæ³¨æ–¼ã€Œé€™å€‹è¼¸å…¥ã€æ€éº¼å¯«ï¼Œæ‰æœƒè®“æ¨¡å‹çµ¦å‡ºæœ€ä½³çš„è¼¸å‡ºã€‚å°±åƒæ˜¯ä½ è·Ÿæ¨¡å‹çš„ã€Œä¸€å¥è©±äº’å‹•ã€ã€‚

**LangChain**ï¼šæ˜¯**å®è§€å±¤ç´š**çš„æ¡†æ¶ï¼Œå¹«åŠ©ä½ æŠŠå¤šå€‹ promptã€ä¸Šä¸‹æ–‡ã€å¤–éƒ¨è³‡æ–™åº«ï¼ˆåƒå‘é‡è³‡æ–™åº«ï¼‰ã€API å·¥å…·ã€è¨˜æ†¶æ©Ÿåˆ¶ç­‰ï¼Œçµ„ç¹”æˆå®Œæ•´æµç¨‹ã€‚é€™æ¨£å°±èƒ½æŠŠå–®ä¸€ prompt æŠ€å·§æ“´å±•æˆç”¢å“ç´šæ‡‰ç”¨ã€‚

### LangChain ä½œç‚ºé€²éš Prompt å·¥ç¨‹å·¥å…·

LangChain å¯ä»¥æ­¸é¡æˆã€Œ**é€²éš Prompt å·¥ç¨‹å·¥ä½œæµ**ã€çš„æ ¸å¿ƒå·¥å…·ï¼Œå› ç‚ºå®ƒæä¾›äº†ï¼š

| åŠŸèƒ½æ¨¡çµ„ | Prompt å·¥ç¨‹å±¤é¢ | å¯¦éš›æ‡‰ç”¨ |
|----------|----------------|----------|
| **Model I/O** | ç®¡ç†æ¨¡å‹è¼¸å…¥è¼¸å‡º | çµ±ä¸€åŒ–ä¸åŒæ¨¡å‹çš„ prompt æ ¼å¼ |
| **Retrieval** | å¾å¤–éƒ¨æ–‡ä»¶å–è³‡æ–™å†ä¸Ÿé€² Prompt | å‹•æ…‹æ³¨å…¥ç›¸é—œå…§å®¹åˆ° prompt ä¸­ |
| **Chains** | æŠŠå¤šå€‹ Prompt ä¸²æˆæµç¨‹ | å¤šæ­¥é©Ÿæ¨ç†ï¼Œå±¤å±¤éé€²çš„ prompt è¨­è¨ˆ |
| **Agents** | è®“æ¨¡å‹è‡ªå·±æ±ºå®šç”¨ä»€éº¼å·¥å…· | æ™ºèƒ½é¸æ“‡æœ€é©åˆçš„ prompt ç­–ç•¥ |
| **Memory** | ä¿ç•™ä¸Šä¸‹æ–‡ï¼Œæ”¯æŒé•·å°è©± | è®“ prompt åŒ…å«æ­·å²å°è©±è¨˜æ†¶ |
| **Callbacks** | åœ¨ç”Ÿæˆéç¨‹ä¸­æ›å‹¾äº‹ä»¶ | Token Streamingã€é€²åº¦è¿½è¸ªç­‰ |

### ç°¡å–®æ¯”å–»

**Prompt å·¥ç¨‹**ï¼šåƒæ˜¯ã€Œ**ä¸€ä»½é£Ÿè­œ**ã€â€” æ€éº¼æè¿°é£Ÿæå’Œæ­¥é©Ÿï¼Œæ‰èƒ½ç…®å‡ºä½ è¦çš„èœã€‚

```python
# å–®ä¸€ Prompt å·¥ç¨‹
prompt = "è«‹åˆ†æä»¥ä¸‹å¥åº·æ•¸æ“šä¸¦çµ¦å‡ºå»ºè­°ï¼šè¡€ç³– 120 mg/dL"
response = llm.invoke(prompt)
```

**LangChain**ï¼šåƒæ˜¯ã€Œ**ä¸€å€‹å»šæˆ¿ç³»çµ±**ã€â€” æœ‰å†°ç®±ï¼ˆè³‡æ–™æª¢ç´¢ï¼‰ã€è¨ˆæ™‚å™¨ï¼ˆå›å‘¼ï¼‰ã€èœè­œé›†åˆï¼ˆChainï¼‰ã€ç”šè‡³å¯ä»¥æ´¾åŠ©æ‰‹ï¼ˆAgentï¼‰å»è²·èœã€‚

```python
# LangChain ç³»çµ±åŒ–æµç¨‹
from langchain.chains import SequentialChain
from langchain.prompts import PromptTemplate

# æ–°ç‰ˆ LCEL ç®¡é“èªæ³•ç¯„ä¾‹ï¼šæ›´ç°¡æ½”çš„å¤šæ­¥é©Ÿæµç¨‹
from langchain_core.runnables import RunnablePassthrough

# ç›´æ¥ç”¨ | æ“ä½œç¬¦ä¸²æ¥å¤šæ­¥é©Ÿ
analysis_prompt = PromptTemplate.from_template("åˆ†æå¥åº·æ•¸æ“šï¼š{health_data}")
recommendation_prompt = PromptTemplate.from_template("åŸºæ–¼åˆ†æ {analysis} æä¾›å»ºè­°")
format_prompt = PromptTemplate.from_template("æ ¼å¼åŒ–å»ºè­° {recommendation} ç‚ºå ±å‘Š")

# LCEL ç®¡é“ï¼šè‡ªå‹•å‚³éä¸­é–“çµæœ
health_pipeline = (
    {"health_data": RunnablePassthrough()}
    | analysis_prompt
    | llm
    | {"analysis": StrOutputParser()}
    | recommendation_prompt
    | llm 
    | {"recommendation": StrOutputParser()}
    | format_prompt
    | llm
    | StrOutputParser()
)

# ä½¿ç”¨ç¯„ä¾‹
result = health_pipeline.invoke("è¡€ç³–åé«˜ 130mg/dL")
print(result)
```

### äº’è£œé—œä¿‚ç¸½çµ

| å±¤é¢ | Prompt å·¥ç¨‹ | LangChain |
|------|-------------|-----------|
| **å±¤ç´š** | å¾®è§€çš„ã€Œèªè¨€æŠ€å·§ã€ | å®è§€çš„ã€Œç³»çµ±æ¡†æ¶ã€ |
| **é—œæ³¨é»** | å–®ä¸€ prompt çš„å“è³ª | æ•´é«”æµç¨‹çš„å”èª¿ |
| **æ‡‰ç”¨å ´æ™¯** | ä¸€æ¬¡æ€§å°è©±å„ªåŒ– | å¯é‡ç”¨ã€å¯æ“´å±•çš„æ‡‰ç”¨ |
| **æŠ€èƒ½éœ€æ±‚** | èªè¨€è¡¨é”ã€é‚è¼¯çµ„ç¹” | ç³»çµ±è¨­è¨ˆã€æ¶æ§‹æ€è€ƒ |

### å¯¦éš›é–‹ç™¼æµç¨‹

```mermaid
graph LR
    A[Prompt å·¥ç¨‹<br/>è¨­è¨ˆå–®ä¸€æç¤ºè©] --> B[LangChain æ•´åˆ<br/>çµ„ç¹”å¤šæ­¥é©Ÿæµç¨‹]
    B --> C[ç³»çµ±æ¸¬è©¦<br/>é©—è­‰æ•´é«”æ•ˆæœ]
    C --> D[å„ªåŒ–èª¿æ•´<br/>å›é ­æ”¹é€² Prompt]
    D --> A
```

**è¦ç¸½çµçš„è©±ï¼š**
- **Prompt å·¥ç¨‹** = å¾®è§€çš„ã€Œèªè¨€æŠ€å·§ã€
- **LangChain** = å®è§€çš„ã€Œç³»çµ±æ¡†æ¶ã€  
- **å…©è€…æ˜¯äº’è£œé—œä¿‚**ï¼ŒLangChain è®“ä½ æŠŠ Prompt å·¥ç¨‹å¾ä¸€æ¬¡æ€§å°è©±å‡ç´šæˆå¯é‡ç”¨ã€å¯æ“´å±•çš„æ‡‰ç”¨ã€‚

## ç¸½çµ

LangChain åŒ…è£çš„å°±æ˜¯ã€ŒLLM é–‹ç™¼çš„é‡è¤‡ç¹ç‘£å·¥ä½œã€ï¼š

- âœ… **LCEL ç®¡é“èªæ³•** - çµ„åˆå¼è³‡æ–™æµè™•ç†
- âœ… **å°è©±è¨˜æ†¶æ©Ÿåˆ¶** - æŒä¹…åŒ–èˆ‡å¤šæœƒè©±æ”¯æ´
- âœ… **æª¢ç´¢å¢å¼·ç”Ÿæˆ** - æ–°ç‰ˆ RAG æµç¨‹
- âœ… **çµæ§‹åŒ–è¼¸å‡º** - Pydantic æ¨¡å‹é©—è­‰
- âœ… **å®‰å…¨å·¥å…·èª¿ç”¨** - è¼¸å…¥é©—è­‰èˆ‡æ¬Šé™æ§åˆ¶
- âœ… **å¯è§€æ¸¬æ€§** - LangSmith æ•´åˆè¿½è¹¤
- âœ… **æ¨¡çµ„åŒ–æ¶æ§‹** - ç‹¬ç«‹æ•´åˆåŒ…è¨­è¨ˆ

è®“ä½ å°ˆæ³¨åœ¨**æ‡‰ç”¨é‚è¼¯å’Œ Prompt è¨­è¨ˆ**ï¼Œè€Œä¸æ˜¯ä¸€ç›´ã€Œé‡é€ è¼ªå­ã€ã€‚

---

::: tip ä¸‹ä¸€æ­¥
ç¾åœ¨ä½ å·²ç¶“äº†è§£ LangChain çš„æ¶æ§‹èˆ‡æ ¸å¿ƒæ¦‚å¿µï¼Œæ¥ä¸‹ä¾†å¯ä»¥ï¼š
1. [ç’°å¢ƒè¨­ç½®](/tutorials/setup) - æº–å‚™é–‹ç™¼ç’°å¢ƒ
2. [å…è²» LLM æ¨¡å‹æŒ‡å—](/tutorials/free-llm-models) - äº†è§£å…è²»æ¨¡å‹é¸é …
3. [ç¬¬ä¸€å€‹æ‡‰ç”¨](/tutorials/first-app) - å‹•æ‰‹å¯¦ä½œ
:::

::: warning ç‰ˆæœ¬ç›¸å®¹æ€§æé†’
æœ¬æ–‡æª”å·²æ›´æ–°è‡³ **LangChain v0.2+ æ¨™æº–**ï¼Œä½†æ¡†æ¶ä»åœ¨å¿«é€Ÿç™¼å±•ä¸­ï¼š

- âœ… **å·²æ›´æ–°**ï¼š`create_retrieval_chain`ã€`create_react_agent`ã€LCEL ç®¡é“èªæ³•
- âš ï¸ **æ£„ç”¨ä¸­**ï¼š`RetrievalQA`ã€`initialize_agent`ã€`ConversationalRetrievalChain`
- ğŸ†• **æ–°ç‰¹æ€§**ï¼šLangGraphã€æ›´å¼·çš„ Output Parsersã€LangSmith æ•´åˆ

å»ºè­°æ­¤é †åºæŸ¥çœ‹æœ€æ–°è³‡è¨Šï¼š
1. [å®˜æ–¹æ–‡æª”](https://python.langchain.com/) - æœ€æ–° API åƒè€ƒ
2. [LangGraph æ–‡æª”](https://langchain-ai.github.io/langgraph/) - æ–°ä¸€ä»£ Agent æ¡†æ¶
3. [LangSmith](https://smith.langchain.com/) - å¯è§€æ¸¬æ€§èˆ‡èª¿è©¦å·¥å…·
:::