# ä¸²æµèŠå¤©æ¨¡å‹ï¼ˆStreaming Chat Modelsï¼‰å®Œæ•´æ•™å­¸

åœ¨ä½¿ç”¨ ChatGPTã€Claudeã€Gemini ç­‰å¤§å‹èªè¨€æ¨¡å‹ï¼ˆLLMï¼‰æ™‚ï¼Œä½ å¯èƒ½æœƒæ³¨æ„åˆ°æ–‡å­—æ˜¯**é€å­—æˆ–é€æ®µé¡¯ç¤º**ï¼ŒåƒçœŸäººæ‰“å­—èˆ¬å‡ºç¾ã€‚é€™ç¨®æ–¹å¼ç¨±ç‚º **ä¸²æµ (Streaming)**ï¼Œå·²ç¶“æˆç‚ºå°è©±å‹æ‡‰ç”¨çš„ä¸»æµæ¨¡å¼ã€‚

æœ¬æ–‡å°‡å¸¶ä½ æ·±å…¥äº†è§£ï¼š

1. ä»€éº¼æ˜¯ä¸²æµèŠå¤©æ¨¡å‹
2. ç¾è¡Œä¸»æµåšæ³•
3. éä¸²æµ vs ä¸²æµæµç¨‹æ¯”è¼ƒ
4. å·®ç•°ç¸½è¡¨
5. Python å¯¦ä½œç¯„ä¾‹
6. å·¥ç¨‹å¯¦å‹™è€ƒé‡
7. å¯¦å‹™æ‡‰ç”¨å ´æ™¯å»ºè­°
8. ä¸‰å¤§ä¾›æ‡‰å•†ä¸²æµæ”¯æ´æ¯”è¼ƒ

---

## 1ï¸âƒ£ ä»€éº¼æ˜¯ä¸²æµèŠå¤©æ¨¡å‹ï¼Ÿ

* **éä¸²æµ (Non-Streaming)**ï¼šæ¨¡å‹ç”Ÿæˆå®Œæ•´å›è¦†å¾Œï¼Œ**ä¸€æ¬¡æ€§**å›å‚³ã€‚
* **ä¸²æµ (Streaming)**ï¼šæ¨¡å‹åœ¨ç”Ÿæˆéç¨‹ä¸­ï¼Œå°‡æ–‡å­— **é€æ®µ (chunk)** å‚³å›ç”¨æˆ¶ç«¯ã€‚ä½¿ç”¨è€…ä¸ç”¨ç­‰å¾…æ•´æ®µå®Œæˆï¼Œå°±èƒ½å³æ™‚çœ‹åˆ°è¼¸å‡ºã€‚

ç¯„ä¾‹ï¼ˆä»¥ LangChain v0.2+ ç‚ºä¾‹ï¼‰ï¼š

```python
for chunk in llm.stream("è«‹ç”¨ä¸€å¥è©±è§£é‡‹ä»€éº¼æ˜¯ä¸²æµ"):
    print(chunk, end="", flush=True)
```

---

## 2ï¸âƒ£ ç¾è¡Œä¸»æµåšæ³•

* **å‰ç«¯äº’å‹•**ï¼šChatGPTã€Claudeã€Gemini ç­‰èŠå¤©æ‡‰ç”¨ **å¹¾ä¹éƒ½ä½¿ç”¨ä¸²æµ**ï¼Œå› ç‚ºèƒ½å¤§å¹…é™ä½ç­‰å¾…æ„Ÿï¼Œé«”é©—æ¥è¿‘ã€ŒçœŸäººå›è¦†ã€ã€‚
* **å¾Œç«¯æ‡‰ç”¨**ï¼šè‹¥éœ€æ±‚æ˜¯ JSONã€SQLã€å ±è¡¨ç­‰éœ€è¦å®Œæ•´çµæ§‹è¼¸å‡ºçš„ä»»å‹™ï¼Œä»æœƒæ¡ç”¨ **éä¸²æµ**ï¼Œé¿å…åŠæˆå“é›£ä»¥è§£æã€‚

ğŸ‘‰ **çµè«–**ï¼š

* **å‰ç«¯ UI / å³æ™‚äº¤äº’ = ä¸²æµ**
* **å¾Œç«¯æ‰¹æ¬¡ / åš´æ ¼çµæ§‹ = éä¸²æµ**ï¼ˆæˆ–ã€Œä¸²æµç”Ÿæˆ â†’ å®Œæˆå¾Œå†è§£æã€çš„æ··åˆç­–ç•¥ï¼‰

---

## 3ï¸âƒ£ éä¸²æµ vs ä¸²æµæµç¨‹

```mermaid
flowchart TD
    subgraph A["éä¸²æµæ¨¡å¼ (Non-Streaming)"]
        A1[ä½¿ç”¨è€…é€å‡ºè¨Šæ¯] --> A2[LLM é–‹å§‹ç”Ÿæˆå®Œæ•´å›è¦†]
        A2 --> A3[ç­‰å¾…ç”Ÿæˆå®Œæˆ]
        A3 --> A4[ä¸€æ¬¡æ€§å›å‚³å®Œæ•´è¨Šæ¯]
        A4 --> A5[ä½¿ç”¨è€…é–‹å§‹é–±è®€]
    end

    subgraph B["ä¸²æµæ¨¡å¼ (Streaming)"]
        B1[ä½¿ç”¨è€…é€å‡ºè¨Šæ¯] --> B2[LLM é–‹å§‹ç”Ÿæˆå›è¦†]
        B2 --> B3[è¼¸å‡ºç¬¬ä¸€æ®µ Token]
        B3 --> B4[å³æ™‚å›å‚³ä¸¦é¡¯ç¤ºçµ¦ä½¿ç”¨è€…]
        B4 --> B5[ä½¿ç”¨è€…ç«‹å³é–±è®€]
        B5 --> B6[æŒçºŒè¼¸å‡ºä¸‹ä¸€æ®µç›´åˆ°å®Œæˆ]
    end
```

---

## 4ï¸âƒ£ å·®ç•°ç¸½è¡¨

| é¢å‘        | éä¸²æµ (Non-Streaming) | ä¸²æµ (Streaming)                     |
| --------- | ------------------- | ---------------------------------- |
| **å‚³è¼¸å‹æ…‹**  | å®Œæˆå¾Œä¸€æ¬¡æ€§å›å‚³            | Token/Chunk é€æ®µå›å‚³ (SSE / WebSocket) |
| **é«”æ„Ÿå»¶é²**  | é«˜ï¼ˆéœ€ç­‰å¾…æ•´æ®µï¼‰            | ä½ï¼ˆTTFT å¿«ï¼Œå¯é‚Šçœ‹é‚Šå‡ºï¼‰                    |
| **çµæ§‹åŒ–è¼¸å‡º** | å®¹æ˜“é©—è­‰ï¼ˆå®Œæ•´ JSON/SQLï¼‰   | å›°é›£ï¼šéœ€åšå¢é‡é©—è­‰æˆ–æœ€å¾Œå†è§£æ                    |
| **å·¥ç¨‹è¤‡é›œåº¦** | ä½                   | ä¸­ï½é«˜ï¼ˆäº‹ä»¶è™•ç†ã€å–æ¶ˆã€é‡è©¦ã€ç·©è¡ï¼‰                 |
| **å¸¸è¦‹å ´æ™¯**  | æ‰¹æ¬¡åˆ†æã€å ±è¡¨ã€ç¨‹å¼ç¢¼ç”Ÿæˆ       | èŠå¤© UIã€å³æ™‚å®¢æœã€RAGã€èªéŸ³/è¦–è¨Šäº’å‹•             |
| **ä¸»è¦æŒ‘æˆ°**  | UX å·®ã€å®¹æ˜“è¶…æ™‚ã€ç„¡æ³•æ—©åœçœè²»    | éœ€è™•ç†ã€ŒåŠæˆå“ã€ã€äº‹ä»¶æµé †åºã€è§€æ¸¬æ€§èˆ‡å®‰å…¨æ€§             |

---

## 5ï¸âƒ£ Python å¯¦ä½œç¯„ä¾‹

### åŸºæœ¬ä¸²æµå¯¦ä½œ

#### éä¸²æµæ¨¡å¼

```python
from langchain_openai import ChatOpenAI
from langchain_core.messages import SystemMessage, HumanMessage

# å»ºç«‹èŠå¤©æ¨¡å‹
chat = ChatOpenAI(model="gpt-4o-mini", temperature=0.7)

messages = [
    SystemMessage(content="ä½ æ˜¯ä¸€ä½å°ˆæ¥­çš„æŠ€è¡“è¬›å¸«ã€‚"),
    HumanMessage(content="è«‹è§£é‡‹ä»€éº¼æ˜¯éä¸²æµæ¨¡å¼ï¼Ÿ")
]

# éä¸²æµå‘¼å« - ç­‰å¾…å®Œæ•´å›æ‡‰
response = chat.invoke(messages)
print(response.content)
```

ğŸ‘‰ ä½¿ç”¨è€…å¿…é ˆç­‰å¾…å®Œæ•´è¨Šæ¯æ‰æœƒä¸€æ¬¡æ€§çœ‹åˆ°ã€‚

#### ä¸²æµæ¨¡å¼

```python
from langchain_openai import ChatOpenAI
from langchain_core.messages import SystemMessage, HumanMessage, AIMessageChunk

# å»ºç«‹èŠå¤©æ¨¡å‹
chat = ChatOpenAI(model="gpt-4o-mini", temperature=0.7)

messages = [
    SystemMessage(content="ä½ æ˜¯ä¸€ä½å°ˆæ¥­çš„æŠ€è¡“è¬›å¸«ã€‚"),
    HumanMessage(content="è«‹è©³ç´°è§£é‡‹ä»€éº¼æ˜¯ä¸²æµæ¨¡å¼ï¼Ÿ")
]

# ä¸²æµå‘¼å« - å³æ™‚é¡¯ç¤ºå…§å®¹
print("AI å›æ‡‰ï¼š", end="")
full_response = ""

for chunk in chat.stream(messages):
    if isinstance(chunk, AIMessageChunk) and chunk.content:
        print(chunk.content, end="", flush=True)
        full_response += chunk.content

print(f"\n\nå®Œæ•´å›æ‡‰æ”¶é›†å®Œæˆï¼Œå…± {len(full_response)} å­—å…ƒ")
```

ğŸ‘‰ ä½¿ç”¨è€…å¯é‚Šç”Ÿæˆé‚Šçœ‹åˆ°å…§å®¹ï¼Œé«”é©—æ›´å³æ™‚ã€‚

### é€²éšä¸²æµè™•ç†

#### ä¸²æµç‹€æ…‹ç®¡ç†

```python
import time
from typing import List, Optional
from langchain_core.messages import AIMessageChunk

class StreamingHandler:
    def __init__(self):
        self.chunks: List[AIMessageChunk] = []
        self.start_time: Optional[float] = None
        self.first_token_time: Optional[float] = None
        self.total_tokens = 0
    
    def process_stream(self, chat, messages):
        """è™•ç†ä¸²æµå›æ‡‰ä¸¦æ”¶é›†çµ±è¨ˆè³‡æ–™"""
        self.start_time = time.time()
        self.chunks = []
        
        print("ğŸ¤– AI é–‹å§‹å›æ‡‰ï¼š", end="")
        
        for i, chunk in enumerate(chat.stream(messages)):
            if isinstance(chunk, AIMessageChunk) and chunk.content:
                # è¨˜éŒ„é¦–å€‹ token æ™‚é–“
                if self.first_token_time is None:
                    self.first_token_time = time.time()
                
                print(chunk.content, end="", flush=True)
                self.chunks.append(chunk)
                self.total_tokens += 1
        
        print("\n")  # æ›è¡Œ
        self._print_statistics()
    
    def _print_statistics(self):
        """å°å‡ºä¸²æµçµ±è¨ˆè³‡æ–™"""
        if self.start_time and self.first_token_time:
            ttft = self.first_token_time - self.start_time  # Time to First Token
            total_time = time.time() - self.start_time
            tokens_per_second = self.total_tokens / total_time if total_time > 0 else 0
            
            print(f"\nğŸ“Š ä¸²æµçµ±è¨ˆï¼š")
            print(f"   é¦–å€‹ Token å»¶é² (TTFT): {ttft:.2f}s")
            print(f"   ç¸½å›æ‡‰æ™‚é–“: {total_time:.2f}s")
            print(f"   è¼¸å‡ºé€Ÿåº¦: {tokens_per_second:.1f} tokens/sec")
            print(f"   ç¸½ chunks: {len(self.chunks)}")
    
    def get_full_response(self) -> str:
        """çµ„åˆå®Œæ•´å›æ‡‰"""
        return "".join(chunk.content for chunk in self.chunks if chunk.content)

# ä½¿ç”¨ç¯„ä¾‹
handler = StreamingHandler()
handler.process_stream(chat, messages)
full_text = handler.get_full_response()
```

#### ä¸²æµå–æ¶ˆæ©Ÿåˆ¶

```python
import threading
import time
from typing import Iterator
from langchain_core.messages import AIMessageChunk

class CancellableStream:
    def __init__(self):
        self.cancelled = False
        self._lock = threading.Lock()
    
    def cancel(self):
        """å–æ¶ˆä¸²æµ"""
        with self._lock:
            self.cancelled = True
            print("\nâš ï¸ ä¸²æµå·²è¢«ä½¿ç”¨è€…å–æ¶ˆ")
    
    def stream_with_cancellation(self, chat, messages, max_chunks: int = 100):
        """æ”¯æ´å–æ¶ˆçš„ä¸²æµè™•ç†"""
        chunk_count = 0
        
        try:
            for chunk in chat.stream(messages):
                # æª¢æŸ¥æ˜¯å¦è¢«å–æ¶ˆ
                with self._lock:
                    if self.cancelled:
                        print("\nğŸ›‘ ä¸²æµå–æ¶ˆå®Œæˆ")
                        break
                
                if isinstance(chunk, AIMessageChunk) and chunk.content:
                    print(chunk.content, end="", flush=True)
                    chunk_count += 1
                    
                    # æ¨¡æ“¬é•·å›æ‡‰çš„åˆ†æ®µè™•ç†
                    time.sleep(0.05)  # æ¨¡æ“¬ç¶²è·¯å»¶é²
                    
                    # å®‰å…¨æ©Ÿåˆ¶ï¼šé¿å…ç„¡é™é•·çš„å›æ‡‰
                    if chunk_count >= max_chunks:
                        print(f"\nâš ï¸ é”åˆ°æœ€å¤§ chunks é™åˆ¶ ({max_chunks})ï¼Œè‡ªå‹•åœæ­¢")
                        break
                        
        except Exception as e:
            print(f"\nâŒ ä¸²æµéç¨‹ç™¼ç”ŸéŒ¯èª¤: {e}")
        
        print(f"\nâœ… ä¸²æµå®Œæˆï¼Œå…±è™•ç† {chunk_count} chunks")

# ä½¿ç”¨ç¯„ä¾‹
cancellable_stream = CancellableStream()

# åœ¨å¦ä¸€å€‹ç·šç¨‹ä¸­æ¨¡æ“¬ä½¿ç”¨è€…å–æ¶ˆæ“ä½œ
def simulate_user_cancel():
    time.sleep(2)  # 2ç§’å¾Œå–æ¶ˆ
    cancellable_stream.cancel()

cancel_thread = threading.Thread(target=simulate_user_cancel)
cancel_thread.start()

# é–‹å§‹ä¸²æµ
cancellable_stream.stream_with_cancellation(chat, messages)
cancel_thread.join()
```

---

## 6ï¸âƒ£ å·¥ç¨‹å¯¦å‹™è€ƒé‡

### 1. é€šè¨Šå”å®šå·®ç•°

```python
# SSE (Server-Sent Events) ç¯„ä¾‹ - é©ç”¨æ–¼ OpenAI, Anthropic
class SSEHandler:
    def __init__(self):
        self.event_source = None
    
    def handle_sse_stream(self, url: str, headers: dict):
        """è™•ç† SSE ä¸²æµ"""
        import sseclient  # pip install sseclient-py
        
        response = requests.get(url, headers=headers, stream=True)
        client = sseclient.SSEClient(response)
        
        for event in client.events():
            if event.data != '[DONE]':
                try:
                    data = json.loads(event.data)
                    chunk_content = data['choices'][0]['delta'].get('content', '')
                    if chunk_content:
                        yield chunk_content
                except json.JSONDecodeError:
                    continue

# WebSocket ç¯„ä¾‹ - é©ç”¨æ–¼ Gemini Live API
class WebSocketHandler:
    def __init__(self):
        self.ws = None
    
    async def handle_websocket_stream(self, uri: str):
        """è™•ç† WebSocket ä¸²æµ"""
        import websockets  # pip install websockets
        
        async with websockets.connect(uri) as websocket:
            self.ws = websocket
            
            # ç™¼é€åˆå§‹è¨Šæ¯
            await websocket.send(json.dumps({
                "type": "message",
                "content": "Hello from streaming client"
            }))
            
            # æ¥æ”¶ä¸²æµå›æ‡‰
            async for message in websocket:
                data = json.loads(message)
                if data.get('type') == 'content':
                    yield data.get('text', '')
```

### 2. çµæ§‹åŒ–è¼¸å‡ºè™•ç†

```python
import json
from typing import Dict, Any, Optional

class StructuredStreamParser:
    def __init__(self):
        self.buffer = ""
        self.json_depth = 0
        self.in_string = False
        self.escape_next = False
    
    def parse_json_stream(self, chunk: str) -> Optional[Dict[str, Any]]:
        """å˜—è©¦è§£æä¸²æµä¸­çš„ JSON ç‰‡æ®µ"""
        self.buffer += chunk
        
        # ç°¡åŒ–çš„ JSON è§£æé‚è¼¯
        for char in chunk:
            if self.escape_next:
                self.escape_next = False
                continue
                
            if char == '\\':
                self.escape_next = True
                continue
                
            if char == '"' and not self.escape_next:
                self.in_string = not self.in_string
                continue
                
            if not self.in_string:
                if char == '{':
                    self.json_depth += 1
                elif char == '}':
                    self.json_depth -= 1
                    
                    # å˜—è©¦è§£æå®Œæ•´çš„ JSON
                    if self.json_depth == 0:
                        try:
                            result = json.loads(self.buffer.strip())
                            self.buffer = ""  # æ¸…ç©ºç·©è¡å€
                            return result
                        except json.JSONDecodeError:
                            # JSON é‚„ä¸å®Œæ•´ï¼Œç¹¼çºŒç´¯ç©
                            pass
        
        return None  # JSON é‚„æœªå®Œæˆ

# ä½¿ç”¨ç¯„ä¾‹
parser = StructuredStreamParser()

for chunk in chat.stream("è«‹ä»¥JSONæ ¼å¼å›å‚³ç”¨æˆ¶è³‡æ–™ï¼šå§“åã€å¹´é½¡ã€è·æ¥­"):
    if chunk.content:
        parsed_json = parser.parse_json_stream(chunk.content)
        if parsed_json:
            print(f"âœ… è§£æåˆ°å®Œæ•´ JSON: {parsed_json}")
            break
        else:
            print(chunk.content, end="", flush=True)
```

### 3. éŒ¯èª¤è™•ç†èˆ‡é‡è©¦

```python
import asyncio
import logging
from tenacity import retry, stop_after_attempt, wait_exponential

class RobustStreamHandler:
    def __init__(self):
        self.logger = logging.getLogger(__name__)
    
    @retry(
        stop=stop_after_attempt(3),
        wait=wait_exponential(multiplier=1, min=4, max=10)
    )
    async def reliable_stream(self, chat, messages):
        """å…·å‚™é‡è©¦æ©Ÿåˆ¶çš„å¯é ä¸²æµ"""
        chunks_received = 0
        
        try:
            async for chunk in chat.astream(messages):
                if chunk.content:
                    print(chunk.content, end="", flush=True)
                    chunks_received += 1
                    
                    # æ¨¡æ“¬ç¶²è·¯å•é¡Œ
                    if chunks_received == 10:  # å‡è¨­ç¬¬10å€‹chunkæ™‚ç™¼ç”Ÿå•é¡Œ
                        raise ConnectionError("æ¨¡æ“¬ç¶²è·¯ä¸­æ–·")
                        
        except Exception as e:
            self.logger.error(f"ä¸²æµä¸­æ–·: {e}ï¼Œæº–å‚™é‡è©¦...")
            raise  # è®“ tenacity è™•ç†é‡è©¦
        
        print(f"\nâœ… ä¸²æµå®Œæˆï¼Œå…±æ¥æ”¶ {chunks_received} chunks")

# ä½¿ç”¨ç¯„ä¾‹
async def main():
    handler = RobustStreamHandler()
    try:
        await handler.reliable_stream(chat, messages)
    except Exception as e:
        print(f"âŒ é‡è©¦å¾Œä»ç„¶å¤±æ•—: {e}")

# asyncio.run(main())
```

### 4. å®‰å…¨èˆ‡å…§å®¹éæ¿¾

```python
import re
from typing import List, Set

class StreamContentFilter:
    def __init__(self):
        # æ•æ„Ÿè©å½™æ¸…å–®ï¼ˆå¯¦éš›ä½¿ç”¨æ™‚æ‡‰è©²æ›´å®Œå–„ï¼‰
        self.blocked_words: Set[str] = {
            "å¯†ç¢¼", "token", "api_key", "secret", 
            "ä¿¡ç”¨å¡", "èº«åˆ†è­‰", "æ‰‹æ©Ÿè™Ÿç¢¼"
        }
        self.buffer_window = 50  # ç·©è¡è¦–çª—å¤§å°
        self.content_buffer = ""
    
    def filter_stream_chunk(self, chunk: str) -> str:
        """éæ¿¾ä¸²æµå…§å®¹"""
        self.content_buffer += chunk
        
        # ä¿æŒç·©è¡å€å¤§å°
        if len(self.content_buffer) > self.buffer_window:
            self.content_buffer = self.content_buffer[-self.buffer_window:]
        
        # æª¢æŸ¥æ•æ„Ÿå…§å®¹
        for blocked_word in self.blocked_words:
            if blocked_word in self.content_buffer:
                # ç°¡å–®çš„é®è”½ç­–ç•¥
                chunk = chunk.replace(blocked_word, "*" * len(blocked_word))
                self.content_buffer = self.content_buffer.replace(blocked_word, "*" * len(blocked_word))
        
        return chunk
    
    def validate_chunk_safety(self, chunk: str) -> bool:
        """é©—è­‰chunkæ˜¯å¦å®‰å…¨"""
        # æª¢æŸ¥æ˜¯å¦åŒ…å«å¯èƒ½çš„ç¨‹å¼ç¢¼æ³¨å…¥
        dangerous_patterns = [
            r'eval\s*\(',
            r'exec\s*\(',
            r'__import__\s*\(',
            r'<script\s*>',
        ]
        
        for pattern in dangerous_patterns:
            if re.search(pattern, chunk, re.IGNORECASE):
                return False
        
        return True

# å®‰å…¨ä¸²æµè™•ç†
content_filter = StreamContentFilter()

print("ğŸ”’ å®‰å…¨ä¸²æµæ¨¡å¼ï¼š", end="")
for chunk in chat.stream("è«‹è§£é‡‹å¦‚ä½•å®‰å…¨åœ°è™•ç†ç”¨æˆ¶å¯†ç¢¼"):
    if chunk.content:
        # å®‰å…¨æª¢æŸ¥
        if not content_filter.validate_chunk_safety(chunk.content):
            print("[å…§å®¹è¢«éæ¿¾]", end="")
            continue
        
        # æ•æ„Ÿè©éæ¿¾
        filtered_content = content_filter.filter_stream_chunk(chunk.content)
        print(filtered_content, end="", flush=True)
```

### 5. è§€æ¸¬æ€§èˆ‡ç›£æ§

```python
import time
from dataclasses import dataclass
from typing import Optional
import logging

@dataclass
class StreamMetrics:
    """ä¸²æµæŒ‡æ¨™è³‡æ–™çµæ§‹"""
    session_id: str
    start_time: float
    first_token_time: Optional[float] = None
    end_time: Optional[float] = None
    total_chunks: int = 0
    total_characters: int = 0
    errors: int = 0
    cancelled: bool = False

class StreamMonitor:
    def __init__(self):
        self.metrics = {}
        self.logger = logging.getLogger(__name__)
    
    def create_session(self, session_id: str) -> StreamMetrics:
        """å‰µå»ºç›£æ§æœƒè©±"""
        metrics = StreamMetrics(
            session_id=session_id,
            start_time=time.time()
        )
        self.metrics[session_id] = metrics
        return metrics
    
    def record_first_token(self, session_id: str):
        """è¨˜éŒ„é¦–å€‹ token æ™‚é–“"""
        if session_id in self.metrics:
            self.metrics[session_id].first_token_time = time.time()
    
    def record_chunk(self, session_id: str, chunk_size: int):
        """è¨˜éŒ„ chunk è³‡è¨Š"""
        if session_id in self.metrics:
            metrics = self.metrics[session_id]
            metrics.total_chunks += 1
            metrics.total_characters += chunk_size
    
    def record_error(self, session_id: str):
        """è¨˜éŒ„éŒ¯èª¤"""
        if session_id in self.metrics:
            self.metrics[session_id].errors += 1
    
    def finish_session(self, session_id: str, cancelled: bool = False):
        """çµæŸæœƒè©±ä¸¦ç”¢ç”Ÿå ±å‘Š"""
        if session_id not in self.metrics:
            return
        
        metrics = self.metrics[session_id]
        metrics.end_time = time.time()
        metrics.cancelled = cancelled
        
        # è¨ˆç®—é—œéµæŒ‡æ¨™
        ttft = (metrics.first_token_time - metrics.start_time 
                if metrics.first_token_time else None)
        total_time = metrics.end_time - metrics.start_time
        throughput = (metrics.total_characters / total_time 
                     if total_time > 0 else 0)
        
        # è¨˜éŒ„æŒ‡æ¨™
        self.logger.info(f"ä¸²æµæœƒè©±å®Œæˆ {session_id}")
        self.logger.info(f"  TTFT: {ttft:.3f}s" if ttft else "  TTFT: æœªè¨˜éŒ„")
        self.logger.info(f"  ç¸½æ™‚é–“: {total_time:.3f}s")
        self.logger.info(f"  ååé‡: {throughput:.1f} å­—å…ƒ/ç§’")
        self.logger.info(f"  Chunks: {metrics.total_chunks}")
        self.logger.info(f"  éŒ¯èª¤: {metrics.errors}")
        self.logger.info(f"  æ˜¯å¦å–æ¶ˆ: {metrics.cancelled}")
        
        # æ¸…ç†
        del self.metrics[session_id]
        
        return {
            "ttft": ttft,
            "total_time": total_time,
            "throughput": throughput,
            "chunks": metrics.total_chunks,
            "errors": metrics.errors,
            "cancelled": metrics.cancelled
        }

# ç›£æ§ä¸²æµæœƒè©±
monitor = StreamMonitor()
session_id = "session_123"

# é–‹å§‹ç›£æ§
metrics = monitor.create_session(session_id)

print("ğŸ“Š ç›£æ§ä¸­çš„ä¸²æµï¼š", end="")
first_chunk = True

try:
    for chunk in chat.stream("è«‹è©³ç´°èªªæ˜æ©Ÿå™¨å­¸ç¿’çš„ç™¼å±•æ­·å²"):
        if chunk.content:
            if first_chunk:
                monitor.record_first_token(session_id)
                first_chunk = False
            
            monitor.record_chunk(session_id, len(chunk.content))
            print(chunk.content, end="", flush=True)
            
except Exception as e:
    monitor.record_error(session_id)
    print(f"\nâŒ éŒ¯èª¤: {e}")
finally:
    # å®Œæˆæœƒè©±
    final_metrics = monitor.finish_session(session_id)
    print(f"\nğŸ“ˆ æœƒè©±æŒ‡æ¨™: {final_metrics}")
```

---

## 7ï¸âƒ£ å¯¦å‹™æ‡‰ç”¨å ´æ™¯å»ºè­°

| æƒ…å¢ƒ                  | å»ºè­°æ¨¡å¼          | ç†ç”±          | å¯¦ä½œè¦é» |
| ------------------- | ------------- | ----------- | ------ |
| ä¸€èˆ¬èŠå¤©ã€å®¢æœã€æ•™è‚²å°è©±        | ä¸²æµ            | é«”é©—å¥½ï¼Œæ¸›å°‘ç­‰å¾…æ„Ÿ   | SSEã€å–æ¶ˆæ©Ÿåˆ¶ã€å…§å®¹éæ¿¾ |
| æ•™å­¸æˆ–é€æ­¥æ¨ç†å±•ç¤º           | ä¸²æµ            | å¯å±•ç¤ºä¸­ç¹¼æ€è·¯     | åˆ†æ®µé¡¯ç¤ºã€æ­¥é©Ÿæ¨™è¨˜ |
| RAGï¼ˆæª¢ç´¢å¢å¼·ç”Ÿæˆï¼‰         | ä¸²æµ            | å³æ™‚é¡¯ç¤ºæª¢ç´¢çµæœèˆ‡å›ç­” | åˆ†éšæ®µè¼¸å‡ºã€ä¾†æºæ¨™è¨» |
| JSON / SQL / API è¼¸å‡º | éä¸²æµ / ä¸²æµ+å¾Œè™•ç†  | ç¢ºä¿çµæ§‹å®Œæ•´æ€§     | ç·©è¡è§£æã€é©—è­‰æ©Ÿåˆ¶ |
| èªéŸ³åŠ©ç†ã€è¦–è¨Šäº’å‹•           | ä¸²æµï¼ˆWebSocketï¼‰ | ä½å»¶é²é›™å‘é€šè¨Š     | WebSocketã€éŸ³è¨ŠåŒæ­¥ |
| å ±è¡¨åˆ†æã€æ‰¹æ¬¡ç”Ÿæˆ           | éä¸²æµ           | å®Œæ•´æ€§æ¯”äº’å‹•æ€§é‡è¦   | æ‰¹æ¬¡è™•ç†ã€çµæœé©—è­‰ |

### å¯¦éš›æ‡‰ç”¨ç¯„ä¾‹

#### èŠå¤©ä»‹é¢ä¸²æµ

```python
class ChatInterface:
    def __init__(self):
        self.chat = ChatOpenAI(model="gpt-4o-mini")
        self.conversation_history = []
    
    def stream_chat_response(self, user_message: str):
        """ä¸²æµèŠå¤©å›æ‡‰"""
        # åŠ å…¥ç”¨æˆ¶è¨Šæ¯åˆ°æ­·å²
        self.conversation_history.append(
            HumanMessage(content=user_message)
        )
        
        print(f"ğŸ‘¤ ç”¨æˆ¶: {user_message}")
        print("ğŸ¤– AI: ", end="")
        
        # ä¸²æµ AI å›æ‡‰
        ai_response = ""
        for chunk in self.chat.stream(self.conversation_history):
            if chunk.content:
                print(chunk.content, end="", flush=True)
                ai_response += chunk.content
        
        # åŠ å…¥ AI å›æ‡‰åˆ°æ­·å²
        self.conversation_history.append(
            AIMessage(content=ai_response)
        )
        print("\n" + "â”€" * 50)

# ä½¿ç”¨èŠå¤©ä»‹é¢
chat_ui = ChatInterface()
chat_ui.stream_chat_response("ä½ å¥½ï¼Œè«‹ä»‹ç´¹ä¸€ä¸‹ Python")
chat_ui.stream_chat_response("èƒ½çµ¦æˆ‘ä¸€å€‹ç°¡å–®çš„ç¨‹å¼ç¯„ä¾‹å—ï¼Ÿ")
```

#### RAG ä¸²æµå¯¦ä½œ

```python
class RAGStreamingSystem:
    def __init__(self):
        self.chat = ChatOpenAI(model="gpt-4o-mini")
        self.retriever = self._setup_retriever()  # å‡è¨­å·²è¨­ç½®
    
    def stream_rag_response(self, question: str):
        """RAG ä¸²æµå›æ‡‰"""
        print(f"ğŸ” æ­£åœ¨æœå°‹ç›¸é—œæ–‡æª”...")
        
        # æª¢ç´¢ç›¸é—œæ–‡æª”
        docs = self.retriever.retrieve(question)
        context = "\n".join([doc.page_content for doc in docs[:3]])
        
        print(f"ğŸ“š æ‰¾åˆ° {len(docs)} å€‹ç›¸é—œæ–‡æª”")
        print("ğŸ¤– AI å›æ‡‰: ", end="")
        
        # å»ºç«‹ RAG prompt
        rag_prompt = f"""åŸºæ–¼ä»¥ä¸‹æ–‡æª”å…§å®¹å›ç­”å•é¡Œï¼š

æ–‡æª”å…§å®¹ï¼š
{context}

å•é¡Œï¼š{question}

è«‹æ ¹æ“šæ–‡æª”å…§å®¹æä¾›æº–ç¢ºå›ç­”ï¼š"""
        
        messages = [
            SystemMessage(content="ä½ æ˜¯ä¸€ä½å°ˆæ¥­çš„çŸ¥è­˜åŠ©æ‰‹ï¼Œè«‹åŸºæ–¼æä¾›çš„æ–‡æª”å…§å®¹æº–ç¢ºå›ç­”å•é¡Œã€‚"),
            HumanMessage(content=rag_prompt)
        ]
        
        # ä¸²æµå›æ‡‰
        response = ""
        for chunk in self.chat.stream(messages):
            if chunk.content:
                print(chunk.content, end="", flush=True)
                response += chunk.content
        
        print("\nğŸ“– åƒè€ƒæ–‡æª”:")
        for i, doc in enumerate(docs[:3], 1):
            print(f"  {i}. {doc.metadata.get('title', 'æœªçŸ¥æ–‡æª”')}")
```

---

## 8ï¸âƒ£ ä¸‰å¤§ä¾›æ‡‰å•†ä¸²æµæ”¯æ´æ¯”è¼ƒ

| ç‰¹æ€§ | OpenAI | Anthropic Claude | Google Gemini |
|------|--------|------------------|---------------|
| **ä¸²æµå”å®š** | SSE (Server-Sent Events) | SSE | SSE + WebSocket (Live API) |
| **API æ”¯æ´** | `/chat/completions` åƒæ•¸ `stream=true` | Messages API åƒæ•¸ `stream=true` | `generateContentStream()` |
| **LangChain æ•´åˆ** | âœ… åŸç”Ÿæ”¯æ´ `.stream()` | âœ… åŸç”Ÿæ”¯æ´ `.stream()` | âœ… åŸç”Ÿæ”¯æ´ `.stream()` |
| **å·¥å…·å‘¼å«ä¸²æµ** | âœ… æ”¯æ´å¢é‡å·¥å…·å‘¼å« | âœ… æ”¯æ´å·¥å…·ä½¿ç”¨ä¸²æµ | âœ… æ”¯æ´å‡½æ•¸å‘¼å«ä¸²æµ |
| **å¤šæ¨¡æ…‹ä¸²æµ** | âŒ æ–‡å­—æ¨¡å‹ä¸æ”¯æ´ | âŒ æ–‡å­—æ¨¡å‹ä¸æ”¯æ´ | âœ… æ”¯æ´å½±åƒ+æ–‡å­—ä¸²æµ |
| **èªéŸ³ä¸²æµ** | âŒ éœ€è¦å¦å¤–çš„ Whisper API | âŒ éœ€è¦ç¬¬ä¸‰æ–¹æ•´åˆ | âœ… Gemini Live API æ”¯æ´ |
| **æœ€å¤§å…§å®¹é•·åº¦** | 128K tokens (GPT-4) | 200K tokens (Claude-3) | 1M tokens (Gemini-1.5) |
| **å»¶é²è¡¨ç¾** | ä¸­ç­‰ (~200-500ms TTFT) | ä½ (~150-300ms TTFT) | ä½ (~100-200ms TTFT) |
| **æˆæœ¬è¨ˆç®—** | æŒ‰å¯¦éš›ç”Ÿæˆ tokens è¨ˆè²» | æŒ‰å¯¦éš›ç”Ÿæˆ tokens è¨ˆè²» | æŒ‰å¯¦éš›ç”Ÿæˆ tokens è¨ˆè²» |
| **å–æ¶ˆæ”¯æ´** | âœ… é—œé–‰ SSE é€£ç·šå³å¯ | âœ… é—œé–‰ SSE é€£ç·šå³å¯ | âœ… é—œé–‰é€£ç·šå³å¯ |
| **éŒ¯èª¤è™•ç†** | HTTP ç‹€æ…‹ç¢¼ + éŒ¯èª¤äº‹ä»¶ | HTTP ç‹€æ…‹ç¢¼ + éŒ¯èª¤äº‹ä»¶ | gRPC ç‹€æ…‹ç¢¼ + éŒ¯èª¤å›èª¿ |

### ä¾›æ‡‰å•†ç‰¹å®šå¯¦ä½œç¯„ä¾‹

#### OpenAI ä¸²æµ

```python
from langchain_openai import ChatOpenAI

# OpenAI é…ç½®
openai_chat = ChatOpenAI(
    model="gpt-4o-mini",
    temperature=0.7,
    streaming=True  # å•Ÿç”¨ä¸²æµ
)

print("ğŸŸ¢ OpenAI ä¸²æµï¼š", end="")
for chunk in openai_chat.stream("è«‹èªªæ˜ OpenAI çš„æŠ€è¡“ç‰¹è‰²"):
    if chunk.content:
        print(chunk.content, end="", flush=True)
print("\n")
```

#### Anthropic Claude ä¸²æµ

```python
from langchain_anthropic import ChatAnthropic

# Claude é…ç½®
claude_chat = ChatAnthropic(
    model="claude-3-sonnet-20240229",
    temperature=0.7
)

print("ğŸŸ¡ Claude ä¸²æµï¼š", end="")
for chunk in claude_chat.stream("è«‹èªªæ˜ Anthropic Claude çš„æŠ€è¡“ç‰¹è‰²"):
    if chunk.content:
        print(chunk.content, end="", flush=True)
print("\n")
```

#### Google Gemini ä¸²æµ

```python
from langchain_google_genai import ChatGoogleGenerativeAI

# Gemini é…ç½®
gemini_chat = ChatGoogleGenerativeAI(
    model="gemini-pro",
    temperature=0.7
)

print("ğŸ”µ Gemini ä¸²æµï¼š", end="")
for chunk in gemini_chat.stream("è«‹èªªæ˜ Google Gemini çš„æŠ€è¡“ç‰¹è‰²"):
    if chunk.content:
        print(chunk.content, end="", flush=True)
print("\n")
```

### ä¾›æ‡‰å•†é¸æ“‡å»ºè­°

```python
def choose_streaming_provider(use_case: str):
    """æ ¹æ“šä½¿ç”¨å ´æ™¯é¸æ“‡æœ€é©åˆçš„ä¾›æ‡‰å•†"""
    
    recommendations = {
        "general_chat": {
            "primary": "OpenAI",
            "reason": "ç”Ÿæ…‹ç³»çµ±å®Œæ•´ï¼Œæ–‡æª”è±å¯Œï¼Œç©©å®šæ€§é«˜"
        },
        "low_latency": {
            "primary": "Google Gemini", 
            "reason": "TTFT æœ€ä½ï¼Œé©åˆå³æ™‚äº’å‹•"
        },
        "long_context": {
            "primary": "Google Gemini",
            "reason": "æ”¯æ´ 1M tokensï¼Œé©åˆé•·æ–‡æª”åˆ†æ"
        },
        "multimodal": {
            "primary": "Google Gemini",
            "reason": "åŸç”Ÿæ”¯æ´æ–‡å­—+å½±åƒä¸²æµ"
        },
        "voice_interaction": {
            "primary": "Google Gemini",
            "reason": "Gemini Live API æ”¯æ´ä½å»¶é²èªéŸ³"
        },
        "cost_sensitive": {
            "primary": "Anthropic Claude",
            "reason": "æ€§åƒ¹æ¯”è¼ƒé«˜ï¼Œå“è³ªç©©å®š"
        }
    }
    
    return recommendations.get(use_case, {
        "primary": "OpenAI",
        "reason": "é€šç”¨é¸æ“‡ï¼Œé©åˆå¤§å¤šæ•¸å ´æ™¯"
    })

# ä½¿ç”¨ç¯„ä¾‹
recommendation = choose_streaming_provider("low_latency")
print(f"å»ºè­°ä¾›æ‡‰å•†: {recommendation['primary']}")
print(f"ç†ç”±: {recommendation['reason']}")
```

---

## âœ… ç¸½çµ

* ä¸²æµæ¨¡å¼æ˜¯ **å‰ç«¯èŠå¤©æ‡‰ç”¨çš„ä¸»æµ**ï¼Œå¸¶ä¾†æ›´ä½³é«”é©—ã€‚
* éä¸²æµæ¨¡å¼ä»æœ‰åƒ¹å€¼ï¼Œå°¤å…¶åœ¨ **åš´æ ¼çµæ§‹è¼¸å‡ºæˆ–å¾Œç«¯æ‰¹æ¬¡ä»»å‹™**ã€‚
* å·¥ç¨‹å¯¦å‹™ä¸Šï¼Œä¸²æµéœ€ç‰¹åˆ¥è™•ç†ï¼š**å–æ¶ˆã€éŒ¯èª¤ã€çµæ§‹åŒ–è¼¸å‡ºã€å®‰å…¨ã€è§€æ¸¬æ€§**ã€‚
* é¸æ“‡æ¨¡å¼æ™‚ï¼Œæ‡‰æ ¹æ“š **ä½¿ç”¨æƒ…å¢ƒ** èˆ‡ **æ¥­å‹™éœ€æ±‚**åšå–æ¨ã€‚
* **ä¸‰å¤§ä¾›æ‡‰å•†å„æœ‰å„ªå‹¢**ï¼š
  - **OpenAI**ï¼šç”Ÿæ…‹ç³»çµ±å®Œæ•´ï¼Œç©©å®šå¯é 
  - **Anthropic Claude**ï¼šé«˜å“è³ªè¼¸å‡ºï¼Œè‰¯å¥½çš„æ€§åƒ¹æ¯”  
  - **Google Gemini**ï¼šä½å»¶é²ï¼Œé•·ä¸Šä¸‹æ–‡ï¼Œå¤šæ¨¡æ…‹æ”¯æ´

### å¯¦å‹™éƒ¨ç½²æª¢æŸ¥æ¸…å–®

- [ ] é¸æ“‡åˆé©çš„ä¾›æ‡‰å•†å’Œå”å®šï¼ˆSSE vs WebSocketï¼‰
- [ ] å¯¦æ–½å–æ¶ˆæ©Ÿåˆ¶ï¼ˆé¿å…ä¸å¿…è¦çš„æˆæœ¬ï¼‰
- [ ] åŠ å…¥éŒ¯èª¤è™•ç†å’Œé‡è©¦é‚è¼¯
- [ ] è¨­ç½®å…§å®¹éæ¿¾å’Œå®‰å…¨æª¢æŸ¥
- [ ] å»ºç«‹è§€æ¸¬æ€§æŒ‡æ¨™ï¼ˆTTFTã€ååé‡ã€éŒ¯èª¤ç‡ï¼‰
- [ ] è™•ç†çµæ§‹åŒ–è¼¸å‡ºçš„ç‰¹æ®Šéœ€æ±‚
- [ ] æ¸¬è©¦ä¸åŒç¶²è·¯ç’°å¢ƒä¸‹çš„è¡¨ç¾
- [ ] è¨­ç½®åˆç†çš„è¶…æ™‚å’Œé™åˆ¶æ©Ÿåˆ¶

---

::: tip ä¸‹ä¸€æ­¥å­¸ç¿’å»ºè­°
æŒæ¡äº†ä¸²æµèŠå¤©æ¨¡å‹å¾Œï¼Œå»ºè­°æ·±å…¥å­¸ç¿’ï¼š

1. [ç›£æ§èˆ‡å¯è§€æ¸¬æ€§](/tutorials/monitoring) - å­¸ç¿’ LangSmith æ•´åˆç›£æ§ä¸²æµæ€§èƒ½
2. [è¨˜æ†¶æ©Ÿåˆ¶èˆ‡å°è©±ç®¡ç†](/tutorials/memory-systems) - åœ¨ä¸²æµç’°å¢ƒä¸­ç®¡ç†å°è©±ç‹€æ…‹
3. [é€²éšæ‡‰ç”¨æ¡ˆä¾‹](/tutorials/advanced-examples) - æŸ¥çœ‹ä¼æ¥­ç´šä¸²æµå¯¦ä½œç¯„ä¾‹
4. [LCEL è¡¨é”å¼èªè¨€](/tutorials/lcel) - å­¸ç¿’å¦‚ä½•åœ¨ä¸²æµä¸­çµ„åˆè¤‡é›œçš„è™•ç†éˆ
:::

::: warning âš ï¸ ç”Ÿç”¢ç’°å¢ƒæ³¨æ„äº‹é …

**æ€§èƒ½å„ªåŒ–**ï¼š
- ç›£æ§ TTFT (Time to First Token) å’Œæ•´é«”å»¶é²
- å¯¦æ–½åˆé©çš„ç·©è¡å’Œæ‰¹æ¬¡ç­–ç•¥
- è€ƒæ…® CDN å’Œé‚Šç·£è¨ˆç®—å„ªåŒ–

**æˆæœ¬æ§åˆ¶**ï¼š
- å¯¦æ–½æ™ºèƒ½å–æ¶ˆæ©Ÿåˆ¶ç¯€çœæˆæœ¬
- ç›£æ§ token ä½¿ç”¨é‡ï¼Œé¿å…æ„å¤–é«˜é¡è²»ç”¨
- æ ¹æ“šä½¿ç”¨å ´æ™¯é¸æ“‡æˆæœ¬æ•ˆç›Šæœ€ä½³çš„æ¨¡å‹

**å®‰å…¨æ€§**ï¼š
- å¯¦æ–½å…§å®¹éæ¿¾ï¼Œé¿å…æœ‰å®³å…§å®¹å³æ™‚é¡¯ç¤º
- ä¿è­· API é‡‘é‘°ï¼Œé¿å…åœ¨å‰ç«¯æš´éœ²
- å¯¦æ–½é©ç•¶çš„é€Ÿç‡é™åˆ¶å’Œä½¿ç”¨è€…é©—è­‰
:::