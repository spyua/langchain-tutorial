# ä¸²æµèŠå¤©æ¨¡å‹ï¼ˆStreaming Chat Modelsï¼‰å®Œæ•´æ•™å­¸

åœ¨ä½¿ç”¨ ChatGPTã€Claudeã€Gemini ç­‰å¤§å‹èªè¨€æ¨¡å‹ï¼ˆLLMï¼‰æ™‚ï¼Œä½ å¯èƒ½æœƒæ³¨æ„åˆ°æ–‡å­—æ˜¯**é€å­—æˆ–é€æ®µé¡¯ç¤º**ï¼ŒåƒçœŸäººæ‰“å­—èˆ¬å‡ºç¾ã€‚é€™ç¨®æ–¹å¼ç¨±ç‚º **ä¸²æµ (Streaming)**ï¼Œå·²ç¶“æˆç‚ºå°è©±å‹æ‡‰ç”¨çš„ä¸»æµæ¨¡å¼ã€‚

æœ¬æ–‡å°‡å¸¶ä½ æ·±å…¥äº†è§£ï¼š

1. ä»€éº¼æ˜¯ä¸²æµèŠå¤©æ¨¡å‹
2. ç¾è¡Œä¸»æµåšæ³•
3. éä¸²æµ vs ä¸²æµæµç¨‹æ¯”è¼ƒ
4. å·®ç•°ç¸½è¡¨
5. Python å¯¦ä½œç¯„ä¾‹
6. å·¥ç¨‹å¯¦å‹™è€ƒé‡
7. å¯¦å‹™æ‡‰ç”¨å ´æ™¯å»ºè­°
8. ä¸‰å¤§ä¾›æ‡‰å•†ä¸²æµæ”¯æ´æ¯”è¼ƒ

---

## 1ï¸âƒ£ ä»€éº¼æ˜¯ä¸²æµèŠå¤©æ¨¡å‹ï¼Ÿ

* **éä¸²æµ (Non-Streaming)**ï¼šæ¨¡å‹ç”Ÿæˆå®Œæ•´å›è¦†å¾Œï¼Œ**ä¸€æ¬¡æ€§**å›å‚³ã€‚
* **ä¸²æµ (Streaming)**ï¼šæ¨¡å‹åœ¨ç”Ÿæˆéç¨‹ä¸­ï¼Œå°‡æ–‡å­— **é€æ®µ (chunk)** å‚³å›ç”¨æˆ¶ç«¯ã€‚ä½¿ç”¨è€…ä¸ç”¨ç­‰å¾…æ•´æ®µå®Œæˆï¼Œå°±èƒ½å³æ™‚çœ‹åˆ°è¼¸å‡ºã€‚

ç¯„ä¾‹ï¼ˆä»¥ LangChain v0.2+ ç‚ºä¾‹ï¼‰ï¼š

```python
# ä½¿ç”¨ .stream() æ–¹æ³•é€²è¡Œä¸²æµå‘¼å«
for chunk in llm.stream("è«‹ç”¨ä¸€å¥è©±è§£é‡‹ä»€éº¼æ˜¯ä¸²æµ"):
    # end="" é¿å…è‡ªå‹•æ›è¡Œï¼Œflush=True ç«‹å³è¼¸å‡ºåˆ°çµ‚ç«¯
    print(chunk, end="", flush=True)
```

---

## 2ï¸âƒ£ ç¾è¡Œä¸»æµåšæ³•

* **å‰ç«¯äº’å‹•**ï¼šChatGPTã€Claudeã€Gemini ç­‰èŠå¤©æ‡‰ç”¨ **å¹¾ä¹éƒ½ä½¿ç”¨ä¸²æµ**ï¼Œå› ç‚ºèƒ½å¤§å¹…é™ä½ç­‰å¾…æ„Ÿï¼Œé«”é©—æ¥è¿‘ã€ŒçœŸäººå›è¦†ã€ã€‚
* **å¾Œç«¯æ‡‰ç”¨**ï¼šè‹¥éœ€æ±‚æ˜¯ JSONã€SQLã€å ±è¡¨ç­‰éœ€è¦å®Œæ•´çµæ§‹è¼¸å‡ºçš„ä»»å‹™ï¼Œä»æœƒæ¡ç”¨ **éä¸²æµ**ï¼Œé¿å…åŠæˆå“é›£ä»¥è§£æã€‚

ğŸ‘‰ **çµè«–**ï¼š

* **å‰ç«¯ UI / å³æ™‚äº¤äº’ = ä¸²æµ**
* **å¾Œç«¯æ‰¹æ¬¡ / åš´æ ¼çµæ§‹ = éä¸²æµ**ï¼ˆæˆ–ã€Œä¸²æµç”Ÿæˆ â†’ å®Œæˆå¾Œå†è§£æã€çš„æ··åˆç­–ç•¥ï¼‰

---

## 3ï¸âƒ£ éä¸²æµ vs ä¸²æµæµç¨‹

```mermaid
flowchart TD
    subgraph A["éä¸²æµæ¨¡å¼ (Non-Streaming)"]
        A1[ä½¿ç”¨è€…é€å‡ºè¨Šæ¯] --> A2[LLM é–‹å§‹ç”Ÿæˆå®Œæ•´å›è¦†]
        A2 --> A3[ç­‰å¾…ç”Ÿæˆå®Œæˆ]
        A3 --> A4[ä¸€æ¬¡æ€§å›å‚³å®Œæ•´è¨Šæ¯]
        A4 --> A5[ä½¿ç”¨è€…é–‹å§‹é–±è®€]
    end

    subgraph B["ä¸²æµæ¨¡å¼ (Streaming)"]
        B1[ä½¿ç”¨è€…é€å‡ºè¨Šæ¯] --> B2[LLM é–‹å§‹ç”Ÿæˆå›è¦†]
        B2 --> B3[è¼¸å‡ºç¬¬ä¸€æ®µ Token]
        B3 --> B4[å³æ™‚å›å‚³ä¸¦é¡¯ç¤ºçµ¦ä½¿ç”¨è€…]
        B4 --> B5[ä½¿ç”¨è€…ç«‹å³é–±è®€]
        B5 --> B6[æŒçºŒè¼¸å‡ºä¸‹ä¸€æ®µç›´åˆ°å®Œæˆ]
    end
```

---

## 4ï¸âƒ£ å·®ç•°ç¸½è¡¨

| é¢å‘        | éä¸²æµ (Non-Streaming) | ä¸²æµ (Streaming)                     |
| --------- | ------------------- | ---------------------------------- |
| **å‚³è¼¸å‹æ…‹**  | å®Œæˆå¾Œä¸€æ¬¡æ€§å›å‚³            | Token/Chunk é€æ®µå›å‚³ (SSE / WebSocket) |
| **é«”æ„Ÿå»¶é²**  | é«˜ï¼ˆéœ€ç­‰å¾…æ•´æ®µï¼‰            | ä½ï¼ˆTTFT å¿«ï¼Œå¯é‚Šçœ‹é‚Šå‡ºï¼‰                    |
| **çµæ§‹åŒ–è¼¸å‡º** | å®¹æ˜“é©—è­‰ï¼ˆå®Œæ•´ JSON/SQLï¼‰   | å›°é›£ï¼šéœ€åšå¢é‡é©—è­‰æˆ–æœ€å¾Œå†è§£æ                    |
| **å·¥ç¨‹è¤‡é›œåº¦** | ä½                   | ä¸­ï½é«˜ï¼ˆäº‹ä»¶è™•ç†ã€å–æ¶ˆã€é‡è©¦ã€ç·©è¡ï¼‰                 |
| **å¸¸è¦‹å ´æ™¯**  | æ‰¹æ¬¡åˆ†æã€å ±è¡¨ã€ç¨‹å¼ç¢¼ç”Ÿæˆ       | èŠå¤© UIã€å³æ™‚å®¢æœã€RAGã€èªéŸ³/è¦–è¨Šäº’å‹•             |
| **ä¸»è¦æŒ‘æˆ°**  | å»¶é²æ„Ÿé«˜ã€å¯èƒ½è¶…æ™‚ã€ç„¡æ³•ä¸­é€”å–æ¶ˆ    | éœ€è™•ç†ã€ŒåŠæˆå“ã€ã€äº‹ä»¶æµé †åºã€è§€æ¸¬æ€§èˆ‡å®‰å…¨æ€§             |

---

## 5ï¸âƒ£ Python å¯¦ä½œç¯„ä¾‹

### åŸºæœ¬ä¸²æµå¯¦ä½œ

#### éä¸²æµæ¨¡å¼

```python
# åŒ¯å…¥å¿…è¦çš„é¡åˆ¥
from langchain_openai import ChatOpenAI  # OpenAI èŠå¤©æ¨¡å‹
from langchain_core.messages import SystemMessage, HumanMessage  # è¨Šæ¯é¡å‹

# å»ºç«‹èŠå¤©æ¨¡å‹å¯¦ä¾‹
chat = ChatOpenAI(
    model="gpt-4o-mini",  # ä½¿ç”¨ GPT-4o mini æ¨¡å‹ï¼ˆæˆæœ¬æ•ˆç›Šä½³ï¼‰
    temperature=0.7       # è¨­å®šå‰µæ„åº¦ï¼ˆ0-1ï¼Œæ•¸å€¼è¶Šé«˜è¶Šæœ‰å‰µæ„ï¼‰
)

# å»ºç«‹è¨Šæ¯æ¸…å–®
messages = [
    # ç³»çµ±è¨Šæ¯ï¼šå®šç¾© AI çš„è§’è‰²å’Œè¡Œç‚º
    SystemMessage(content="ä½ æ˜¯ä¸€ä½å°ˆæ¥­çš„æŠ€è¡“è¬›å¸«ã€‚"),
    # äººé¡è¨Šæ¯ï¼šä½¿ç”¨è€…çš„å•é¡Œ
    HumanMessage(content="è«‹è§£é‡‹ä»€éº¼æ˜¯éä¸²æµæ¨¡å¼ï¼Ÿ")
]

# éä¸²æµå‘¼å« - ç­‰å¾…å®Œæ•´å›æ‡‰å¾Œä¸€æ¬¡æ€§è¿”å›
response = chat.invoke(messages)
# å°å‡ºå®Œæ•´çš„å›æ‡‰å…§å®¹
print(response.content)
```

ğŸ‘‰ ä½¿ç”¨è€…å¿…é ˆç­‰å¾…å®Œæ•´è¨Šæ¯æ‰æœƒä¸€æ¬¡æ€§çœ‹åˆ°ã€‚

#### ä¸²æµæ¨¡å¼

```python
# åŒ¯å…¥å¿…è¦çš„é¡åˆ¥
from langchain_openai import ChatOpenAI  # OpenAI èŠå¤©æ¨¡å‹
from langchain_core.messages import SystemMessage, HumanMessage, AIMessageChunk  # è¨Šæ¯é¡å‹

# å»ºç«‹èŠå¤©æ¨¡å‹å¯¦ä¾‹ï¼ˆè¨­å®šèˆ‡éä¸²æµç›¸åŒï¼‰
chat = ChatOpenAI(
    model="gpt-4o-mini",  # ä½¿ç”¨ç›¸åŒçš„æ¨¡å‹
    temperature=0.7       # ç›¸åŒçš„å‰µæ„åº¦è¨­å®š
)

# å»ºç«‹è¨Šæ¯æ¸…å–®
messages = [
    # ç³»çµ±è¨Šæ¯ï¼šå®šç¾© AI çš„è§’è‰²
    SystemMessage(content="ä½ æ˜¯ä¸€ä½å°ˆæ¥­çš„æŠ€è¡“è¬›å¸«ã€‚"),
    # äººé¡è¨Šæ¯ï¼šè©¢å•ä¸²æµç›¸é—œå•é¡Œ
    HumanMessage(content="è«‹è©³ç´°è§£é‡‹ä»€éº¼æ˜¯ä¸²æµæ¨¡å¼ï¼Ÿ")
]

# ä¸²æµå‘¼å« - å³æ™‚é¡¯ç¤ºå…§å®¹
print("AI å›æ‡‰ï¼š", end="")  # å°å‡ºæ¨™é¡Œï¼Œä¸æ›è¡Œ
full_response = ""           # ç”¨æ–¼æ”¶é›†å®Œæ•´å›æ‡‰çš„è®Šæ•¸

# ä½¿ç”¨ .stream() æ–¹æ³•ç²å¾—ä¸²æµå›æ‡‰
for chunk in chat.stream(messages):
    # æª¢æŸ¥ chunk æ˜¯å¦ç‚º AIMessageChunk ä¸”åŒ…å«å…§å®¹
    if isinstance(chunk, AIMessageChunk) and chunk.content:
        # å³æ™‚å°å‡ºå…§å®¹ç‰‡æ®µï¼Œä¸æ›è¡Œä¸”ç«‹å³åˆ·æ–°è¼¸å‡ºç·©è¡å€
        print(chunk.content, end="", flush=True)
        # å°‡ç‰‡æ®µåŠ å…¥å®Œæ•´å›æ‡‰ä¸­
        full_response += chunk.content

# ä¸²æµå®Œæˆå¾Œå°å‡ºçµ±è¨ˆè³‡è¨Š
print(f"\n\nå®Œæ•´å›æ‡‰æ”¶é›†å®Œæˆï¼Œå…± {len(full_response)} å­—å…ƒ")
```

ğŸ‘‰ ä½¿ç”¨è€…å¯é‚Šç”Ÿæˆé‚Šçœ‹åˆ°å…§å®¹ï¼Œé«”é©—æ›´å³æ™‚ã€‚

### é€²éšä¸²æµè™•ç†

#### ä¸²æµç‹€æ…‹ç®¡ç†

```python
import time  # ç”¨æ–¼è¨ˆç®—æ™‚é–“çµ±è¨ˆ
from typing import List, Optional  # å‹åˆ¥æç¤º
from langchain_core.messages import AIMessageChunk  # ä¸²æµè¨Šæ¯å¡Šé¡å‹

class StreamingHandler:
    """ä¸²æµè™•ç†å™¨ï¼šè² è²¬ç®¡ç†ä¸²æµç‹€æ…‹å’Œçµ±è¨ˆè³‡æ–™"""
    
    def __init__(self):
        # åˆå§‹åŒ–å¯¦ä¾‹è®Šæ•¸
        self.chunks: List[AIMessageChunk] = []  # å„²å­˜æ‰€æœ‰æ”¶åˆ°çš„è¨Šæ¯å¡Š
        self.start_time: Optional[float] = None  # é–‹å§‹è™•ç†æ™‚é–“
        self.first_token_time: Optional[float] = None  # é¦–å€‹ token å›æ‡‰æ™‚é–“
        self.total_tokens = 0  # ç¸½ token è¨ˆæ•¸å™¨
    
    def process_stream(self, chat, messages):
        """è™•ç†ä¸²æµå›æ‡‰ä¸¦æ”¶é›†çµ±è¨ˆè³‡æ–™"""
        # è¨˜éŒ„é–‹å§‹æ™‚é–“ï¼ˆç”¨æ–¼è¨ˆç®—ç¸½è™•ç†æ™‚é–“ï¼‰
        self.start_time = time.time()
        # é‡ç½® chunks æ¸…å–®ï¼ˆæ¸…é™¤ä¸Šæ¬¡çš„çµæœï¼‰
        self.chunks = []
        
        # å°å‡ºè™•ç†é–‹å§‹æç¤º
        print("ğŸ¤– AI é–‹å§‹å›æ‡‰ï¼š", end="")
        
        # éæ­·ä¸²æµå›æ‡‰çš„æ¯å€‹ chunk
        for i, chunk in enumerate(chat.stream(messages)):
            # æª¢æŸ¥ chunk æ˜¯å¦åŒ…å«æœ‰æ•ˆå…§å®¹
            if isinstance(chunk, AIMessageChunk) and chunk.content:
                # è¨˜éŒ„é¦–å€‹ token çš„æ™‚é–“ï¼ˆTTFT æŒ‡æ¨™ï¼‰
                if self.first_token_time is None:
                    self.first_token_time = time.time()
                
                # å³æ™‚å°å‡ºå…§å®¹ï¼ˆæä¾›å³æ™‚å›é¥‹ï¼‰
                print(chunk.content, end="", flush=True)
                # å°‡ chunk åŠ å…¥æ”¶é›†æ¸…å–®
                self.chunks.append(chunk)
                # å¢åŠ  token è¨ˆæ•¸ï¼ˆç°¡åŒ–è¨ˆç®—ï¼Œå¯¦éš›æ‡‰ç”¨å¯ç”¨æ›´ç²¾ç¢ºçš„æ–¹æ³•ï¼‰
                self.total_tokens += 1
        
        print("\n")  # å®Œæˆå¾Œæ›è¡Œ
        self._print_statistics()  # å°å‡ºçµ±è¨ˆè³‡æ–™
    
    def _print_statistics(self):
        """å°å‡ºä¸²æµçµ±è¨ˆè³‡æ–™"""
        # ç¢ºä¿æœ‰è¶³å¤ çš„æ™‚é–“æ•¸æ“š
        if self.start_time and self.first_token_time:
            # è¨ˆç®—é¦–å€‹ token å»¶é²ï¼ˆé‡è¦çš„æ€§èƒ½æŒ‡æ¨™ï¼‰
            ttft = self.first_token_time - self.start_time
            # è¨ˆç®—ç¸½è™•ç†æ™‚é–“
            total_time = time.time() - self.start_time
            # è¨ˆç®—å¹³å‡è¼¸å‡ºé€Ÿåº¦ï¼ˆtokens/ç§’ï¼‰
            tokens_per_second = self.total_tokens / total_time if total_time > 0 else 0
            
            # æ ¼å¼åŒ–å°å‡ºçµ±è¨ˆè³‡è¨Š
            print(f"\nğŸ“Š ä¸²æµçµ±è¨ˆï¼š")
            print(f"   é¦–å€‹ Token å»¶é² (TTFT): {ttft:.2f}s")  # Time to First Token
            print(f"   ç¸½å›æ‡‰æ™‚é–“: {total_time:.2f}s")
            print(f"   è¼¸å‡ºé€Ÿåº¦: {tokens_per_second:.1f} tokens/sec")
            print(f"   ç¸½ chunks: {len(self.chunks)}")  # æ”¶åˆ°çš„è¨Šæ¯å¡Šæ•¸é‡
    
    def get_full_response(self) -> str:
        """çµ„åˆå®Œæ•´å›æ‡‰"""
        # å°‡æ‰€æœ‰æœ‰å…§å®¹çš„ chunks çµ„åˆæˆå®Œæ•´å­—ä¸²
        return "".join(chunk.content for chunk in self.chunks if chunk.content)

# ä½¿ç”¨ç¯„ä¾‹
handler = StreamingHandler()  # å»ºç«‹è™•ç†å™¨å¯¦ä¾‹
handler.process_stream(chat, messages)  # è™•ç†ä¸²æµ
full_text = handler.get_full_response()  # ç²å¾—å®Œæ•´å›æ‡‰æ–‡å­—
```

#### ä¸²æµå–æ¶ˆæ©Ÿåˆ¶

```python
import threading  # ç”¨æ–¼åŸ·è¡Œç·’å®‰å…¨æ“ä½œ
import time  # ç”¨æ–¼æ™‚é–“å»¶é²å’Œè¨ˆç®—
from typing import Iterator  # å‹åˆ¥æç¤º
from langchain_core.messages import AIMessageChunk  # ä¸²æµè¨Šæ¯å¡Šé¡å‹

class CancellableStream:
    """å¯å–æ¶ˆçš„ä¸²æµè™•ç†å™¨"""
    
    def __init__(self):
        # å–æ¶ˆç‹€æ…‹æ¨™èªŒï¼ˆåŸ·è¡Œç·’å®‰å…¨ï¼‰
        self.cancelled = False
        # åŸ·è¡Œç·’é–ï¼Œç¢ºä¿ cancelled ç‹€æ…‹çš„å®‰å…¨å­˜å–
        self._lock = threading.Lock()
    
    def cancel(self):
        """å–æ¶ˆä¸²æµï¼ˆå¯å¾å…¶ä»–åŸ·è¡Œç·’å‘¼å«ï¼‰"""
        # ä½¿ç”¨é–ç¢ºä¿åŸ·è¡Œç·’å®‰å…¨
        with self._lock:
            self.cancelled = True  # è¨­å®šå–æ¶ˆæ¨™èªŒ
            print("\nâš ï¸ ä¸²æµå·²è¢«ä½¿ç”¨è€…å–æ¶ˆ")
    
    def stream_with_cancellation(self, chat, messages, max_chunks: int = 100):
        """æ”¯æ´å–æ¶ˆçš„ä¸²æµè™•ç†"""
        chunk_count = 0  # è™•ç†çš„ chunks è¨ˆæ•¸å™¨
        
        try:
            # é–‹å§‹ä¸²æµè™•ç†
            for chunk in chat.stream(messages):
                # åŸ·è¡Œç·’å®‰å…¨åœ°æª¢æŸ¥å–æ¶ˆç‹€æ…‹
                with self._lock:
                    if self.cancelled:
                        print("\nğŸ›‘ ä¸²æµå–æ¶ˆå®Œæˆ")
                        break  # ç«‹å³é€€å‡ºè¿´åœˆ
                
                # è™•ç†æœ‰å…§å®¹çš„è¨Šæ¯å¡Š
                if isinstance(chunk, AIMessageChunk) and chunk.content:
                    # å³æ™‚é¡¯ç¤ºå…§å®¹
                    print(chunk.content, end="", flush=True)
                    chunk_count += 1  # å¢åŠ è¨ˆæ•¸
                    
                    # æ¨¡æ“¬ç¶²è·¯å»¶é²ï¼ˆå¯¦éš›ç’°å¢ƒä¸­ä¸éœ€è¦ï¼‰
                    time.sleep(0.05)
                    
                    # å®‰å…¨æ©Ÿåˆ¶ï¼šé˜²æ­¢ç„¡é™é•·çš„å›æ‡‰æ¶ˆè€—è³‡æº
                    if chunk_count >= max_chunks:
                        print(f"\nâš ï¸ é”åˆ°æœ€å¤§ chunks é™åˆ¶ ({max_chunks})ï¼Œè‡ªå‹•åœæ­¢")
                        break
                        
        except Exception as e:
            # è™•ç†ä¸²æµéç¨‹ä¸­çš„ä¾‹å¤–æƒ…æ³
            print(f"\nâŒ ä¸²æµéç¨‹ç™¼ç”ŸéŒ¯èª¤: {e}")
        
        # å°å‡ºè™•ç†çµæœçµ±è¨ˆ
        print(f"\nâœ… ä¸²æµå®Œæˆï¼Œå…±è™•ç† {chunk_count} chunks")

# ä½¿ç”¨ç¯„ä¾‹
cancellable_stream = CancellableStream()  # å»ºç«‹å¯å–æ¶ˆçš„ä¸²æµå¯¦ä¾‹

# åœ¨å¦ä¸€å€‹ç·šç¨‹ä¸­æ¨¡æ“¬ä½¿ç”¨è€…å–æ¶ˆæ“ä½œ
def simulate_user_cancel():
    """æ¨¡æ“¬ä½¿ç”¨è€…åœ¨ 2 ç§’å¾ŒæŒ‰ä¸‹å–æ¶ˆæŒ‰éˆ•"""
    time.sleep(2)  # ç­‰å¾… 2 ç§’
    cancellable_stream.cancel()  # å‘¼å«å–æ¶ˆæ–¹æ³•

# å»ºç«‹ä¸¦å•Ÿå‹•å–æ¶ˆæ¨¡æ“¬åŸ·è¡Œç·’
cancel_thread = threading.Thread(target=simulate_user_cancel)
cancel_thread.start()  # å•Ÿå‹•èƒŒæ™¯åŸ·è¡Œç·’

# é–‹å§‹ä¸²æµè™•ç†ï¼ˆä¸»åŸ·è¡Œç·’ï¼‰
cancellable_stream.stream_with_cancellation(chat, messages)
# ç­‰å¾…å–æ¶ˆåŸ·è¡Œç·’å®Œæˆ
cancel_thread.join()
```

---

## 6ï¸âƒ£ å·¥ç¨‹å¯¦å‹™è€ƒé‡

### 1. æˆæœ¬æ§åˆ¶èˆ‡æ—©åœæ©Ÿåˆ¶

```python
class CostAwareStreaming:
    """å…·å‚™æˆæœ¬æ§åˆ¶åŠŸèƒ½çš„ä¸²æµè™•ç†å™¨"""
    
    def __init__(self, max_tokens: int = 4000):
        # è¨­å®šæœ€å¤§å…è¨±çš„ token æ•¸é‡
        self.max_tokens = max_tokens
        # ç•¶å‰ç´¯è¨ˆçš„ token æ•¸é‡
        self.current_tokens = 0
        # æ¯å€‹ token çš„æˆæœ¬ï¼ˆä»¥ GPT-4o-mini ç‚ºä¾‹ï¼‰
        self.cost_per_token = 0.0001  
    
    def stream_with_cost_control(self, chat, messages):
        """å¸¶æˆæœ¬æ§åˆ¶çš„ä¸²æµè™•ç†"""
        # é‡ç½®ç•¶å‰ token è¨ˆæ•¸å™¨
        self.current_tokens = 0
        
        # éæ­·ä¸²æµå›æ‡‰
        for chunk in chat.stream(messages):
            if chunk.content:
                # ä¼°ç®—ç•¶å‰ chunk çš„ token æ•¸é‡
                # ç°¡åŒ–ç®—æ³•ï¼šè‹±æ–‡å–®å­—æ•¸ * 1.3ï¼ˆè€ƒæ…®æ¨™é»ç¬¦è™Ÿå’Œç·¨ç¢¼ï¼‰
                estimated_tokens = len(chunk.content.split()) * 1.3
                # ç´¯è¨ˆ token æ•¸é‡
                self.current_tokens += estimated_tokens
                
                # æª¢æŸ¥æ˜¯å¦è¶…éé ç®—é™åˆ¶
                if self.current_tokens > self.max_tokens:
                    print(f"\nâš ï¸ é”åˆ° token é™åˆ¶ ({self.max_tokens})ï¼Œè‡ªå‹•åœæ­¢")
                    break  # ç«‹å³åœæ­¢ä¸²æµï¼Œç¯€çœæˆæœ¬
                
                # å¦‚æœæœªè¶…éé™åˆ¶ï¼Œæ­£å¸¸é¡¯ç¤ºå…§å®¹
                print(chunk.content, end="", flush=True)
        
        # è¨ˆç®—ä¸¦é¡¯ç¤ºé ä¼°æˆæœ¬
        estimated_cost = self.current_tokens * self.cost_per_token
        print(f"\nğŸ’° é ä¼°æˆæœ¬: ${estimated_cost:.4f}")
        print(f"ğŸ“Š ä½¿ç”¨ tokens: {self.current_tokens:.0f}/{self.max_tokens}")
```

**é‡é»ï¼š**
- âœ… **æ™ºèƒ½åœæ­¢**ï¼šæ”¯æ´ã€Œåœæ­¢ç”Ÿæˆã€æŒ‰éˆ•ï¼Œç«‹å³ä¸­æ–·ä¸²æµç¯€çœæˆæœ¬
- âœ… **Token é™åˆ¶**ï¼šè¨­ç½®æœ€å¤§ token æ•¸é¿å…æ„å¤–è¶…é•·å›æ‡‰
- âœ… **å¯¦æ™‚ç›£æ§**ï¼šè¿½è¹¤ç´¯è¨ˆ token ä½¿ç”¨é‡å’Œé ä¼°æˆæœ¬

### 2. é€šè¨Šå”å®šå·®ç•°

```python
# SSE (Server-Sent Events) ç¯„ä¾‹ - é©ç”¨æ–¼ OpenAI, Anthropic
class SSEHandler:
    """Server-Sent Events ä¸²æµè™•ç†å™¨"""
    
    def __init__(self):
        # å„²å­˜äº‹ä»¶æºé€£ç·šçš„åƒè€ƒ
        self.event_source = None
    
    def handle_sse_stream(self, url: str, headers: dict):
        """è™•ç† SSE ä¸²æµé€£ç·š"""
        import sseclient  # éœ€è¦å®‰è£ï¼špip install sseclient-py
        import requests   # ç”¨æ–¼ HTTP è«‹æ±‚
        import json       # ç”¨æ–¼è§£æ JSON è³‡æ–™
        
        # å»ºç«‹ä¸²æµ HTTP è«‹æ±‚
        response = requests.get(url, headers=headers, stream=True)
        # å»ºç«‹ SSE å®¢æˆ¶ç«¯
        client = sseclient.SSEClient(response)
        
        # è™•ç†æ¯å€‹ SSE äº‹ä»¶
        for event in client.events():
            # æª¢æŸ¥æ˜¯å¦ç‚ºçµæŸæ¨™è¨˜
            if event.data != '[DONE]':
                try:
                    # è§£æ JSON æ ¼å¼çš„äº‹ä»¶è³‡æ–™
                    data = json.loads(event.data)
                    # æå–å…§å®¹ç‰‡æ®µ
                    chunk_content = data['choices'][0]['delta'].get('content', '')
                    # å¦‚æœæœ‰å…§å®¹å‰‡ç”¢å‡º
                    if chunk_content:
                        yield chunk_content
                except json.JSONDecodeError:
                    # å¿½ç•¥ç„¡æ•ˆçš„ JSON è³‡æ–™
                    continue

# WebSocket ç¯„ä¾‹ - é©ç”¨æ–¼ Gemini Live API
class WebSocketHandler:
    """WebSocket ä¸²æµè™•ç†å™¨ï¼ˆé©ç”¨æ–¼ä½å»¶é²å ´æ™¯ï¼‰"""
    
    def __init__(self):
        # å„²å­˜ WebSocket é€£ç·šçš„åƒè€ƒ
        self.ws = None
    
    async def handle_websocket_stream(self, uri: str):
        """è™•ç† WebSocket ä¸²æµé€£ç·š"""
        import websockets  # éœ€è¦å®‰è£ï¼špip install websockets
        import json        # ç”¨æ–¼ JSON åºåˆ—åŒ–/ååºåˆ—åŒ–
        
        # å»ºç«‹ WebSocket é€£ç·šï¼ˆä½¿ç”¨ async context managerï¼‰
        async with websockets.connect(uri) as websocket:
            self.ws = websocket  # å„²å­˜é€£ç·šåƒè€ƒ
            
            # ç™¼é€åˆå§‹è¨Šæ¯åˆ°æœå‹™å™¨
            await websocket.send(json.dumps({
                "type": "message",
                "content": "Hello from streaming client"
            }))
            
            # æŒçºŒæ¥æ”¶ä¸²æµå›æ‡‰
            async for message in websocket:
                # è§£ææ”¶åˆ°çš„ JSON è¨Šæ¯
                data = json.loads(message)
                # æª¢æŸ¥è¨Šæ¯é¡å‹ä¸¦æå–å…§å®¹
                if data.get('type') == 'content':
                    yield data.get('text', '')
```

### 3. çµæ§‹åŒ–è¼¸å‡ºè™•ç†

```python
import json
from typing import Dict, Any, Optional

class StructuredStreamParser:
    def __init__(self):
        self.buffer = ""
        self.json_depth = 0
        self.in_string = False
        self.escape_next = False
    
    def parse_json_stream(self, chunk: str) -> Optional[Dict[str, Any]]:
        """å˜—è©¦è§£æä¸²æµä¸­çš„ JSON ç‰‡æ®µ"""
        self.buffer += chunk
        
        # ç°¡åŒ–çš„ JSON è§£æé‚è¼¯
        for char in chunk:
            if self.escape_next:
                self.escape_next = False
                continue
                
            if char == '\\':
                self.escape_next = True
                continue
                
            if char == '"' and not self.escape_next:
                self.in_string = not self.in_string
                continue
                
            if not self.in_string:
                if char == '{':
                    self.json_depth += 1
                elif char == '}':
                    self.json_depth -= 1
                    
                    # å˜—è©¦è§£æå®Œæ•´çš„ JSON
                    if self.json_depth == 0:
                        try:
                            result = json.loads(self.buffer.strip())
                            self.buffer = ""  # æ¸…ç©ºç·©è¡å€
                            return result
                        except json.JSONDecodeError:
                            # JSON é‚„ä¸å®Œæ•´ï¼Œç¹¼çºŒç´¯ç©
                            pass
        
        return None  # JSON é‚„æœªå®Œæˆ

# ä½¿ç”¨ç¯„ä¾‹
parser = StructuredStreamParser()

for chunk in chat.stream("è«‹ä»¥JSONæ ¼å¼å›å‚³ç”¨æˆ¶è³‡æ–™ï¼šå§“åã€å¹´é½¡ã€è·æ¥­"):
    if chunk.content:
        parsed_json = parser.parse_json_stream(chunk.content)
        if parsed_json:
            print(f"âœ… è§£æåˆ°å®Œæ•´ JSON: {parsed_json}")
            break
        else:
            print(chunk.content, end="", flush=True)
```

### 4. éŒ¯èª¤è™•ç†èˆ‡é‡è©¦æ©Ÿåˆ¶ï¼ˆé¿å…å‰¯ä½œç”¨ï¼‰

```python
import asyncio
import logging
from tenacity import retry, stop_after_attempt, wait_exponential

class RobustStreamHandler:
    def __init__(self):
        self.logger = logging.getLogger(__name__)
    
    @retry(
        stop=stop_after_attempt(3),
        wait=wait_exponential(multiplier=1, min=4, max=10)
    )
    async def reliable_stream(self, chat, messages):
        """å…·å‚™é‡è©¦æ©Ÿåˆ¶çš„å¯é ä¸²æµ"""
        chunks_received = 0
        
        try:
            async for chunk in chat.astream(messages):
                if chunk.content:
                    print(chunk.content, end="", flush=True)
                    chunks_received += 1
                    
                    # æ¨¡æ“¬ç¶²è·¯å•é¡Œ
                    if chunks_received == 10:  # å‡è¨­ç¬¬10å€‹chunkæ™‚ç™¼ç”Ÿå•é¡Œ
                        raise ConnectionError("æ¨¡æ“¬ç¶²è·¯ä¸­æ–·")
                        
        except Exception as e:
            self.logger.error(f"ä¸²æµä¸­æ–·: {e}ï¼Œæº–å‚™é‡è©¦...")
            raise  # è®“ tenacity è™•ç†é‡è©¦
        
        print(f"\nâœ… ä¸²æµå®Œæˆï¼Œå…±æ¥æ”¶ {chunks_received} chunks")

# ä½¿ç”¨ç¯„ä¾‹
async def main():
    handler = RobustStreamHandler()
    try:
        await handler.reliable_stream(chat, messages)
    except Exception as e:
        print(f"âŒ é‡è©¦å¾Œä»ç„¶å¤±æ•—: {e}")

# asyncio.run(main())
```

âš ï¸ **é‡è¦æé†’ï¼šé¿å…é‡è©¦å‰¯ä½œç”¨**
```python
# âŒ éŒ¯èª¤ï¼šé‡è©¦å¯èƒ½å°è‡´é‡è¤‡æ“ä½œ
async def bad_retry_example(user_action):
    # é€™ç¨®é‡è©¦å¯èƒ½å°è‡´é‡è¤‡å¯«å…¥è³‡æ–™åº«æˆ–é‡è¤‡ç™¼é€éƒµä»¶
    for attempt in range(3):
        try:
            result = await process_user_action(user_action)  # å¯èƒ½æœ‰å‰¯ä½œç”¨
            await save_to_database(result)  # é‡è¤‡å¯«å…¥ï¼
            await send_email(result)  # é‡è¤‡ç™¼é€ï¼
            return result
        except Exception:
            continue  # å±éšªçš„é‡è©¦

# âœ… æ­£ç¢ºï¼šåˆ†é›¢ç´”å‡½æ•¸å’Œå‰¯ä½œç”¨æ“ä½œ
async def safe_retry_example(user_input):
    # 1. å…ˆé‡è©¦ç´”å‡½æ•¸éƒ¨åˆ†ï¼ˆä¸²æµç”Ÿæˆï¼‰
    generated_content = None
    for attempt in range(3):
        try:
            generated_content = await generate_stream_content(user_input)
            break
        except Exception as e:
            if attempt == 2:
                raise e
    
    # 2. æˆåŠŸå¾Œæ‰åŸ·è¡Œå‰¯ä½œç”¨æ“ä½œï¼ˆåªåŸ·è¡Œä¸€æ¬¡ï¼‰
    if generated_content:
        await save_to_database(generated_content)  # åªåŸ·è¡Œä¸€æ¬¡
        await send_email(generated_content)  # åªåŸ·è¡Œä¸€æ¬¡
```

### 5. å®‰å…¨èˆ‡å…§å®¹éæ¿¾

```python
import re
from typing import List, Set

class StreamContentFilter:
    def __init__(self):
        # âš ï¸ ç¤ºç¯„ç”¨æ•æ„Ÿè©å½™æ¸…å–®ï¼ˆç”Ÿç”¢ç’°å¢ƒéœ€ä¾ç…§å¯¦éš›å ´æ™¯å»ºç«‹å®Œæ•´çš„åˆè¦åå–®ï¼‰
        self.blocked_words: Set[str] = {
            "å¯†ç¢¼", "token", "api_key", "secret", 
            "ä¿¡ç”¨å¡", "èº«åˆ†è­‰", "æ‰‹æ©Ÿè™Ÿç¢¼"
        }
        # å¯¦éš›æ‡‰ç”¨å»ºè­°ï¼š
        # 1. ä½¿ç”¨å°ˆæ¥­çš„ DLP (Data Loss Prevention) æœå‹™
        # 2. æ ¹æ“šè¡Œæ¥­è¦ç¯„ï¼ˆå¦‚ GDPRã€HIPAAï¼‰å»ºç«‹åˆè¦è©åº«
        # 3. å®šæœŸæ›´æ–°å’Œå¯©æ ¸æ•æ„Ÿè©å½™æ¸…å–®
        self.buffer_window = 50  # ç·©è¡è¦–çª—å¤§å°
        self.content_buffer = ""
    
    def filter_stream_chunk(self, chunk: str) -> str:
        """éæ¿¾ä¸²æµå…§å®¹"""
        self.content_buffer += chunk
        
        # ä¿æŒç·©è¡å€å¤§å°
        if len(self.content_buffer) > self.buffer_window:
            self.content_buffer = self.content_buffer[-self.buffer_window:]
        
        # æª¢æŸ¥æ•æ„Ÿå…§å®¹
        for blocked_word in self.blocked_words:
            if blocked_word in self.content_buffer:
                # ç°¡å–®çš„é®è”½ç­–ç•¥
                chunk = chunk.replace(blocked_word, "*" * len(blocked_word))
                self.content_buffer = self.content_buffer.replace(blocked_word, "*" * len(blocked_word))
        
        return chunk
    
    def validate_chunk_safety(self, chunk: str) -> bool:
        """é©—è­‰chunkæ˜¯å¦å®‰å…¨ï¼ˆåƒ…ç‚ºç¤ºç¯„ï¼Œç”Ÿç”¢ç’°å¢ƒéœ€è¦æ›´å¼·å¤§çš„æ©Ÿåˆ¶ï¼‰"""
        # âš ï¸ æ³¨æ„ï¼šç°¡å–®çš„ Regex æª¢æŸ¥å®¹æ˜“ç¹éï¼Œç”Ÿç”¢ç’°å¢ƒå»ºè­°ï¼š
        # 1. ä½¿ç”¨å°ˆæ¥­çš„ä»£ç¢¼å®‰å…¨æƒæå·¥å…·
        # 2. å¯¦æ–½ runtime sandbox éš”é›¢åŸ·è¡Œç’°å¢ƒ  
        # 3. æ¡ç”¨ Content Security Policy (CSP) ç­‰ç€è¦½å™¨å®‰å…¨æ©Ÿåˆ¶
        
        dangerous_patterns = [
            r'eval\s*\(',     # åŸºæœ¬ç¤ºç¯„ï¼Œå¯¦éš›å ´æ™¯éœ€è¦æ›´å®Œæ•´çš„æ¨¡å¼
            r'exec\s*\(',
            r'__import__\s*\(',
            r'<script\s*>',
        ]
        
        for pattern in dangerous_patterns:
            if re.search(pattern, chunk, re.IGNORECASE):
                return False
        
        return True

# å®‰å…¨ä¸²æµè™•ç†
content_filter = StreamContentFilter()

print("ğŸ”’ å®‰å…¨ä¸²æµæ¨¡å¼ï¼š", end="")
for chunk in chat.stream("è«‹è§£é‡‹å¦‚ä½•å®‰å…¨åœ°è™•ç†ç”¨æˆ¶å¯†ç¢¼"):
    if chunk.content:
        # å®‰å…¨æª¢æŸ¥
        if not content_filter.validate_chunk_safety(chunk.content):
            print("[å…§å®¹è¢«éæ¿¾]", end="")
            continue
        
        # æ•æ„Ÿè©éæ¿¾
        filtered_content = content_filter.filter_stream_chunk(chunk.content)
        print(filtered_content, end="", flush=True)
```

### 6. è§€æ¸¬æ€§èˆ‡ç›£æ§

```python
import time
from dataclasses import dataclass
from typing import Optional
import logging

@dataclass
class StreamMetrics:
    """ä¸²æµæŒ‡æ¨™è³‡æ–™çµæ§‹"""
    session_id: str
    start_time: float
    first_token_time: Optional[float] = None
    end_time: Optional[float] = None
    total_chunks: int = 0
    total_characters: int = 0
    errors: int = 0
    cancelled: bool = False

class StreamMonitor:
    def __init__(self):
        self.metrics = {}
        self.logger = logging.getLogger(__name__)
    
    def create_session(self, session_id: str) -> StreamMetrics:
        """å‰µå»ºç›£æ§æœƒè©±"""
        metrics = StreamMetrics(
            session_id=session_id,
            start_time=time.time()
        )
        self.metrics[session_id] = metrics
        return metrics
    
    def record_first_token(self, session_id: str):
        """è¨˜éŒ„é¦–å€‹ token æ™‚é–“"""
        if session_id in self.metrics:
            self.metrics[session_id].first_token_time = time.time()
    
    def record_chunk(self, session_id: str, chunk_size: int):
        """è¨˜éŒ„ chunk è³‡è¨Š"""
        if session_id in self.metrics:
            metrics = self.metrics[session_id]
            metrics.total_chunks += 1
            metrics.total_characters += chunk_size
    
    def record_error(self, session_id: str):
        """è¨˜éŒ„éŒ¯èª¤"""
        if session_id in self.metrics:
            self.metrics[session_id].errors += 1
    
    def finish_session(self, session_id: str, cancelled: bool = False):
        """çµæŸæœƒè©±ä¸¦ç”¢ç”Ÿå ±å‘Š"""
        if session_id not in self.metrics:
            return
        
        metrics = self.metrics[session_id]
        metrics.end_time = time.time()
        metrics.cancelled = cancelled
        
        # è¨ˆç®—é—œéµæŒ‡æ¨™
        ttft = (metrics.first_token_time - metrics.start_time 
                if metrics.first_token_time else None)
        total_time = metrics.end_time - metrics.start_time
        throughput = (metrics.total_characters / total_time 
                     if total_time > 0 else 0)
        
        # è¨˜éŒ„æŒ‡æ¨™
        self.logger.info(f"ä¸²æµæœƒè©±å®Œæˆ {session_id}")
        self.logger.info(f"  TTFT: {ttft:.3f}s" if ttft else "  TTFT: æœªè¨˜éŒ„")
        self.logger.info(f"  ç¸½æ™‚é–“: {total_time:.3f}s")
        self.logger.info(f"  ååé‡: {throughput:.1f} å­—å…ƒ/ç§’")
        self.logger.info(f"  Chunks: {metrics.total_chunks}")
        self.logger.info(f"  éŒ¯èª¤: {metrics.errors}")
        self.logger.info(f"  æ˜¯å¦å–æ¶ˆ: {metrics.cancelled}")
        
        # æ¸…ç†
        del self.metrics[session_id]
        
        return {
            "ttft": ttft,
            "total_time": total_time,
            "throughput": throughput,
            "chunks": metrics.total_chunks,
            "errors": metrics.errors,
            "cancelled": metrics.cancelled
        }

# ç›£æ§ä¸²æµæœƒè©±
monitor = StreamMonitor()
session_id = "session_123"

# é–‹å§‹ç›£æ§
metrics = monitor.create_session(session_id)

print("ğŸ“Š ç›£æ§ä¸­çš„ä¸²æµï¼š", end="")
first_chunk = True

try:
    for chunk in chat.stream("è«‹è©³ç´°èªªæ˜æ©Ÿå™¨å­¸ç¿’çš„ç™¼å±•æ­·å²"):
        if chunk.content:
            if first_chunk:
                monitor.record_first_token(session_id)
                first_chunk = False
            
            monitor.record_chunk(session_id, len(chunk.content))
            print(chunk.content, end="", flush=True)
            
except Exception as e:
    monitor.record_error(session_id)
    print(f"\nâŒ éŒ¯èª¤: {e}")
finally:
    # å®Œæˆæœƒè©±
    final_metrics = monitor.finish_session(session_id)
    print(f"\nğŸ“ˆ æœƒè©±æŒ‡æ¨™: {final_metrics}")
```

---

## 7ï¸âƒ£ å¯¦å‹™æ‡‰ç”¨å ´æ™¯å»ºè­°

| æƒ…å¢ƒ                  | å»ºè­°æ¨¡å¼          | ç†ç”±          | å¯¦ä½œè¦é» |
| ------------------- | ------------- | ----------- | ------ |
| ä¸€èˆ¬èŠå¤©ã€å®¢æœã€æ•™è‚²å°è©±        | ä¸²æµ            | é«”é©—å¥½ï¼Œæ¸›å°‘ç­‰å¾…æ„Ÿ   | SSEã€å–æ¶ˆæ©Ÿåˆ¶ã€å…§å®¹éæ¿¾ |
| æ•™å­¸æˆ–é€æ­¥æ¨ç†å±•ç¤º           | ä¸²æµ            | å¯å±•ç¤ºä¸­ç¹¼æ€è·¯     | åˆ†æ®µé¡¯ç¤ºã€æ­¥é©Ÿæ¨™è¨˜ |
| RAGï¼ˆæª¢ç´¢å¢å¼·ç”Ÿæˆï¼‰         | ä¸²æµ            | å³æ™‚é¡¯ç¤ºæª¢ç´¢çµæœèˆ‡å›ç­” | åˆ†éšæ®µè¼¸å‡ºã€ä¾†æºæ¨™è¨» |
| JSON / SQL / API è¼¸å‡º | éä¸²æµ / ä¸²æµ+å¾Œè™•ç†  | ç¢ºä¿çµæ§‹å®Œæ•´æ€§     | ç·©è¡è§£æã€é©—è­‰æ©Ÿåˆ¶ |
| èªéŸ³åŠ©ç†ã€è¦–è¨Šäº’å‹•           | ä¸²æµï¼ˆWebSocketï¼‰ | ä½å»¶é²é›™å‘é€šè¨Š     | WebSocketã€éŸ³è¨ŠåŒæ­¥ |
| å ±è¡¨åˆ†æã€æ‰¹æ¬¡ç”Ÿæˆ           | éä¸²æµ           | å®Œæ•´æ€§æ¯”äº’å‹•æ€§é‡è¦   | æ‰¹æ¬¡è™•ç†ã€çµæœé©—è­‰ |

### å¯¦éš›æ‡‰ç”¨ç¯„ä¾‹

#### èŠå¤©ä»‹é¢ä¸²æµ

```python
# åŒ¯å…¥å¿…è¦çš„æ¨¡çµ„
from langchain_openai import ChatOpenAI  # OpenAI èŠå¤©æ¨¡å‹
from langchain_core.messages import HumanMessage, AIMessage  # è¨Šæ¯é¡å‹

class ChatInterface:
    """ä¸²æµèŠå¤©ä»‹é¢ï¼šæ”¯æ´å¤šè¼ªå°è©±çš„èŠå¤©æ©Ÿå™¨äºº"""
    
    def __init__(self):
        # åˆå§‹åŒ–èŠå¤©æ¨¡å‹
        self.chat = ChatOpenAI(model="gpt-4o-mini")
        # å„²å­˜å®Œæ•´çš„å°è©±æ­·å²ï¼ˆåŒ…å«æ‰€æœ‰å°è©±è¼ªæ¬¡ï¼‰
        self.conversation_history = []
    
    def stream_chat_response(self, user_message: str):
        """è™•ç†ç”¨æˆ¶è¨Šæ¯ä¸¦ä¸²æµ AI å›æ‡‰"""
        # å°‡æ–°çš„ç”¨æˆ¶è¨Šæ¯åŠ å…¥å°è©±æ­·å²
        self.conversation_history.append(
            HumanMessage(content=user_message)  # å»ºç«‹äººé¡è¨Šæ¯ç‰©ä»¶
        )
        
        # é¡¯ç¤ºç”¨æˆ¶è¼¸å…¥
        print(f"ğŸ‘¤ ç”¨æˆ¶: {user_message}")
        print("ğŸ¤– AI: ", end="")  # AI å›æ‡‰æ¨™ç±¤ï¼Œä¸æ›è¡Œ
        
        # ç”¨æ–¼æ”¶é›†å®Œæ•´ AI å›æ‡‰çš„è®Šæ•¸
        ai_response = ""
        
        # ä¸²æµè™•ç† AI å›æ‡‰ï¼ˆå‚³å…¥å®Œæ•´å°è©±æ­·å²ï¼‰
        for chunk in self.chat.stream(self.conversation_history):
            if chunk.content:  # æª¢æŸ¥ chunk æ˜¯å¦åŒ…å«å…§å®¹
                # å³æ™‚é¡¯ç¤ºå…§å®¹ç‰‡æ®µ
                print(chunk.content, end="", flush=True)
                # ç´¯ç©å®Œæ•´å›æ‡‰
                ai_response += chunk.content
        
        # å°‡å®Œæ•´çš„ AI å›æ‡‰åŠ å…¥å°è©±æ­·å²
        self.conversation_history.append(
            AIMessage(content=ai_response)  # å»ºç«‹ AI è¨Šæ¯ç‰©ä»¶
        )
        
        # å°å‡ºåˆ†éš”ç·šï¼Œä¾¿æ–¼é–±è®€
        print("\n" + "â”€" * 50)

# ä½¿ç”¨ç¯„ä¾‹ï¼šå»ºç«‹èŠå¤©ä»‹é¢ä¸¦é€²è¡Œå¤šè¼ªå°è©±
chat_ui = ChatInterface()  # å»ºç«‹èŠå¤©ä»‹é¢å¯¦ä¾‹

# ç¬¬ä¸€è¼ªå°è©±
chat_ui.stream_chat_response("ä½ å¥½ï¼Œè«‹ä»‹ç´¹ä¸€ä¸‹ Python")

# ç¬¬äºŒè¼ªå°è©±ï¼ˆAI æœƒè¨˜å¾—å‰é¢çš„å°è©±ï¼‰
chat_ui.stream_chat_response("èƒ½çµ¦æˆ‘ä¸€å€‹ç°¡å–®çš„ç¨‹å¼ç¯„ä¾‹å—ï¼Ÿ")
```

#### RAG ä¸²æµå¯¦ä½œ

```python
class RAGStreamingSystem:
    def __init__(self):
        self.chat = ChatOpenAI(model="gpt-4o-mini")
        self.retriever = self._setup_retriever()  # å‡è¨­å·²è¨­ç½®
    
    def stream_rag_response(self, question: str):
        """RAG ä¸²æµå›æ‡‰"""
        print(f"ğŸ” æ­£åœ¨æœå°‹ç›¸é—œæ–‡æª”...")
        
        # æª¢ç´¢ç›¸é—œæ–‡æª”
        docs = self.retriever.invoke(question)
        context = "\n".join([doc.page_content for doc in docs[:3]])
        
        print(f"ğŸ“š æ‰¾åˆ° {len(docs)} å€‹ç›¸é—œæ–‡æª”")
        print("ğŸ¤– AI å›æ‡‰: ", end="")
        
        # å»ºç«‹ RAG prompt
        rag_prompt = f"""åŸºæ–¼ä»¥ä¸‹æ–‡æª”å…§å®¹å›ç­”å•é¡Œï¼š

æ–‡æª”å…§å®¹ï¼š
{context}

å•é¡Œï¼š{question}

è«‹æ ¹æ“šæ–‡æª”å…§å®¹æä¾›æº–ç¢ºå›ç­”ï¼š"""
        
        messages = [
            SystemMessage(content="ä½ æ˜¯ä¸€ä½å°ˆæ¥­çš„çŸ¥è­˜åŠ©æ‰‹ï¼Œè«‹åŸºæ–¼æä¾›çš„æ–‡æª”å…§å®¹æº–ç¢ºå›ç­”å•é¡Œã€‚"),
            HumanMessage(content=rag_prompt)
        ]
        
        # ä¸²æµå›æ‡‰
        response = ""
        for chunk in self.chat.stream(messages):
            if chunk.content:
                print(chunk.content, end="", flush=True)
                response += chunk.content
        
        print("\nğŸ“– åƒè€ƒæ–‡æª”:")
        for i, doc in enumerate(docs[:3], 1):
            print(f"  {i}. {doc.metadata.get('title', 'æœªçŸ¥æ–‡æª”')}")

# é€²éš RAGï¼šä½¿ç”¨ astream_events åŒæ™‚ä¸²æµæª¢ç´¢èˆ‡ç”Ÿæˆ
class AdvancedRAGStreaming:
    def __init__(self):
        self.chat = ChatOpenAI(model="gpt-4o-mini")
        self.retriever = self._setup_retriever()
    
    async def advanced_rag_stream(self, question: str):
        """ä½¿ç”¨ astream_events çš„é€²éš RAG ä¸²æµ"""
        from langchain_core.runnables import RunnableLambda
        
        # å»ºç«‹ RAG éˆ
        rag_chain = (
            RunnableLambda(lambda x: self.retriever.invoke(x["question"])) |
            RunnableLambda(self._format_context) |
            self.chat
        )
        
        print(f"ğŸ” é–‹å§‹ RAG ä¸²æµè™•ç†...")
        
        # ä½¿ç”¨ astream_events ç²å¾—è©³ç´°çš„åŸ·è¡Œäº‹ä»¶
        async for event in rag_chain.astream_events(
            {"question": question}, 
            version="v1"
        ):
            if event["event"] == "on_retriever_end":
                docs = event["data"]["output"]
                print(f"ğŸ“š æª¢ç´¢å®Œæˆï¼Œæ‰¾åˆ° {len(docs)} å€‹æ–‡æª”")
                
            elif event["event"] == "on_chat_model_stream":
                chunk = event["data"]["chunk"]
                if chunk.content:
                    print(chunk.content, end="", flush=True)
        
        print("\nâœ… RAG ä¸²æµå®Œæˆ")
    
    def _format_context(self, docs):
        """æ ¼å¼åŒ–æª¢ç´¢åˆ°çš„æ–‡æª”"""
        context = "\n".join([doc.page_content for doc in docs[:3]])
        return f"åŸºæ–¼ä»¥ä¸‹æ–‡æª”å…§å®¹å›ç­”å•é¡Œï¼š\n{context}\n\nå•é¡Œï¼š"
```

---

## 8ï¸âƒ£ ä¸‰å¤§ä¾›æ‡‰å•†ä¸²æµæ”¯æ´æ¯”è¼ƒ

| ç‰¹æ€§ | OpenAI | Anthropic Claude | Google Gemini |
|------|--------|------------------|---------------|
| **ä¸²æµå”å®š** | SSE (Server-Sent Events) | SSE | SSE + WebSocket (Live API) |
| **API æ”¯æ´** | `/chat/completions` åƒæ•¸ `stream=true` | Messages API åƒæ•¸ `stream=true` | `generateContentStream()` |
| **LangChain æ•´åˆ** | âœ… åŸç”Ÿæ”¯æ´ `.stream()` | âœ… åŸç”Ÿæ”¯æ´ `.stream()` | âœ… åŸç”Ÿæ”¯æ´ `.stream()` |
| **å·¥å…·å‘¼å«ä¸²æµ** | âœ… æ”¯æ´å¢é‡å·¥å…·å‘¼å« | âœ… æ”¯æ´å·¥å…·ä½¿ç”¨ä¸²æµ | âœ… æ”¯æ´å‡½æ•¸å‘¼å«ä¸²æµ |
| **å¤šæ¨¡æ…‹ä¸²æµ** | âŒ æ–‡å­—æ¨¡å‹ä¸æ”¯æ´ | âŒ æ–‡å­—æ¨¡å‹ä¸æ”¯æ´ | âœ… æ”¯æ´å½±åƒ+æ–‡å­—ä¸²æµ |
| **èªéŸ³ä¸²æµ** | âŒ éœ€è¦å¦å¤–çš„ Whisper API | âŒ éœ€è¦ç¬¬ä¸‰æ–¹æ•´åˆ | âœ… Gemini Live API æ”¯æ´ |
| **æœ€å¤§å…§å®¹é•·åº¦** | 128K tokens (GPT-4) | 200K tokens (Claude-3) | 1M tokens (Gemini-1.5 Pro/Flash) |
| **å»¶é²è¡¨ç¾** | ä¸­ç­‰å»¶é² | ç›¸å°ä½å»¶é² | ç›¸å°æœ€ä½å»¶é² |
| **æˆæœ¬è¨ˆç®—** | æŒ‰å¯¦éš›ç”Ÿæˆ tokens è¨ˆè²» | æŒ‰å¯¦éš›ç”Ÿæˆ tokens è¨ˆè²» | æŒ‰å¯¦éš›ç”Ÿæˆ tokens è¨ˆè²» |
| **å–æ¶ˆæ”¯æ´** | âœ… é—œé–‰ SSE é€£ç·šå³å¯ | âœ… é—œé–‰ SSE é€£ç·šå³å¯ | âœ… é—œé–‰é€£ç·šå³å¯ |
| **éŒ¯èª¤è™•ç†** | HTTP ç‹€æ…‹ç¢¼ + éŒ¯èª¤äº‹ä»¶ | HTTP ç‹€æ…‹ç¢¼ + éŒ¯èª¤äº‹ä»¶ | gRPC ç‹€æ…‹ç¢¼ + éŒ¯èª¤å›èª¿ |

### ä¾›æ‡‰å•†ç‰¹å®šå¯¦ä½œç¯„ä¾‹

#### OpenAI ä¸²æµ

```python
# åŒ¯å…¥ OpenAI èŠå¤©æ¨¡å‹é¡åˆ¥
from langchain_openai import ChatOpenAI

# OpenAI æ¨¡å‹é…ç½®
openai_chat = ChatOpenAI(
    model="gpt-4o-mini",  # ä½¿ç”¨æˆæœ¬æ•ˆç›Šé«˜çš„ mini ç‰ˆæœ¬
    temperature=0.7       # è¨­å®šå‰µæ„åº¦
    # æ³¨æ„ï¼šLangChain v0.2+ ä¸å†éœ€è¦ streaming=True åƒæ•¸
    # .stream() æ–¹æ³•æœƒè‡ªå‹•è™•ç†ä¸²æµ
)

# åŸ·è¡Œä¸²æµæŸ¥è©¢
print("ğŸŸ¢ OpenAI ä¸²æµï¼š", end="")
for chunk in openai_chat.stream("è«‹èªªæ˜ OpenAI çš„æŠ€è¡“ç‰¹è‰²"):
    if chunk.content:  # æª¢æŸ¥å…§å®¹æ˜¯å¦å­˜åœ¨
        print(chunk.content, end="", flush=True)  # å³æ™‚é¡¯ç¤º
print("\n")  # å®Œæˆå¾Œæ›è¡Œ
```

#### Anthropic Claude ä¸²æµ

```python
# åŒ¯å…¥ Anthropic Claude èŠå¤©æ¨¡å‹é¡åˆ¥
from langchain_anthropic import ChatAnthropic

# Claude æ¨¡å‹é…ç½®
claude_chat = ChatAnthropic(
    model="claude-3-sonnet-20240229",  # ä½¿ç”¨ Claude-3 Sonnet æ¨¡å‹
    temperature=0.7                    # è¨­å®šå‰µæ„åº¦
    # Claude ç¾å·²åŸç”Ÿæ”¯æ´ system prompts å’Œä¸²æµ
)

# åŸ·è¡Œä¸²æµæŸ¥è©¢
print("ğŸŸ¡ Claude ä¸²æµï¼š", end="")
for chunk in claude_chat.stream("è«‹èªªæ˜ Anthropic Claude çš„æŠ€è¡“ç‰¹è‰²"):
    if chunk.content:  # æª¢æŸ¥å…§å®¹æ˜¯å¦å­˜åœ¨
        print(chunk.content, end="", flush=True)  # å³æ™‚é¡¯ç¤º
print("\n")  # å®Œæˆå¾Œæ›è¡Œ
```

#### Google Gemini ä¸²æµ

```python
# åŒ¯å…¥ Google Gemini èŠå¤©æ¨¡å‹é¡åˆ¥
from langchain_google_genai import ChatGoogleGenerativeAI

# Gemini æ¨¡å‹é…ç½®
gemini_chat = ChatGoogleGenerativeAI(
    model="gemini-pro",  # ä½¿ç”¨ Gemini Pro æ¨¡å‹
    temperature=0.7      # è¨­å®šå‰µæ„åº¦
    # Gemini æ”¯æ´å¤šæ¨¡æ…‹å’Œé•·ä¸Šä¸‹æ–‡ï¼ˆ1.5 ç‰ˆæœ¬å¯é” 1M tokensï¼‰
)

# åŸ·è¡Œä¸²æµæŸ¥è©¢
print("ğŸ”µ Gemini ä¸²æµï¼š", end="")
for chunk in gemini_chat.stream("è«‹èªªæ˜ Google Gemini çš„æŠ€è¡“ç‰¹è‰²"):
    if chunk.content:  # æª¢æŸ¥å…§å®¹æ˜¯å¦å­˜åœ¨
        print(chunk.content, end="", flush=True)  # å³æ™‚é¡¯ç¤º
print("\n")  # å®Œæˆå¾Œæ›è¡Œ
```

### ä¾›æ‡‰å•†é¸æ“‡å»ºè­°

```python
def choose_streaming_provider(use_case: str):
    """æ ¹æ“šä½¿ç”¨å ´æ™¯é¸æ“‡æœ€é©åˆçš„ä¾›æ‡‰å•†"""
    
    # å®šç¾©ä¸åŒä½¿ç”¨å ´æ™¯çš„ä¾›æ‡‰å•†å»ºè­°
    recommendations = {
        "general_chat": {
            "primary": "OpenAI",
            "reason": "ç”Ÿæ…‹ç³»çµ±å®Œæ•´ï¼Œæ–‡æª”è±å¯Œï¼Œç©©å®šæ€§é«˜"
        },
        "low_latency": {
            "primary": "Google Gemini", 
            "reason": "TTFT æœ€ä½ï¼Œé©åˆå³æ™‚äº’å‹•"
        },
        "long_context": {
            "primary": "Google Gemini",
            "reason": "æ”¯æ´ 1M tokensï¼Œé©åˆé•·æ–‡æª”åˆ†æ"
        },
        "multimodal": {
            "primary": "Google Gemini",
            "reason": "åŸç”Ÿæ”¯æ´æ–‡å­—+å½±åƒä¸²æµ"
        },
        "voice_interaction": {
            "primary": "Google Gemini",
            "reason": "Gemini Live API æ”¯æ´ä½å»¶é²èªéŸ³"
        },
        "quality_focus": {
            "primary": "Anthropic Claude",
            "reason": "é©åˆé•·ä¸Šä¸‹æ–‡æ‡‰ç”¨ï¼Œå“è³ªç©©å®š"
        }
    }
    
    # æ ¹æ“šä½¿ç”¨å ´æ™¯è¿”å›å»ºè­°ï¼Œå¦‚æœæ‰¾ä¸åˆ°å‰‡è¿”å›é è¨­é¸é …
    return recommendations.get(use_case, {
        "primary": "OpenAI",
        "reason": "é€šç”¨é¸æ“‡ï¼Œé©åˆå¤§å¤šæ•¸å ´æ™¯"
    })

# ä½¿ç”¨ç¯„ä¾‹ï¼šæŸ¥è©¢ä½å»¶é²å ´æ™¯çš„å»ºè­°
recommendation = choose_streaming_provider("low_latency")
print(f"å»ºè­°ä¾›æ‡‰å•†: {recommendation['primary']}")  # å°å‡ºå»ºè­°çš„ä¾›æ‡‰å•†
print(f"ç†ç”±: {recommendation['reason']}")         # å°å‡ºé¸æ“‡ç†ç”±
```

---

## âœ… ç¸½çµ

* ä¸²æµæ¨¡å¼æ˜¯ **å‰ç«¯èŠå¤©æ‡‰ç”¨çš„ä¸»æµ**ï¼Œå¸¶ä¾†æ›´ä½³é«”é©—ã€‚
* éä¸²æµæ¨¡å¼ä»æœ‰åƒ¹å€¼ï¼Œå°¤å…¶åœ¨ **åš´æ ¼çµæ§‹è¼¸å‡ºæˆ–å¾Œç«¯æ‰¹æ¬¡ä»»å‹™**ã€‚
* å·¥ç¨‹å¯¦å‹™ä¸Šï¼Œä¸²æµéœ€ç‰¹åˆ¥è™•ç†ï¼š**å–æ¶ˆã€éŒ¯èª¤ã€çµæ§‹åŒ–è¼¸å‡ºã€å®‰å…¨ã€è§€æ¸¬æ€§**ã€‚
* é¸æ“‡æ¨¡å¼æ™‚ï¼Œæ‡‰æ ¹æ“š **ä½¿ç”¨æƒ…å¢ƒ** èˆ‡ **æ¥­å‹™éœ€æ±‚**åšå–æ¨ã€‚
* **ä¸‰å¤§ä¾›æ‡‰å•†å„æœ‰å„ªå‹¢**ï¼š
  - **OpenAI**ï¼šç”Ÿæ…‹ç³»çµ±å®Œæ•´ï¼Œç©©å®šå¯é 
  - **Anthropic Claude**ï¼šé«˜å“è³ªè¼¸å‡ºï¼Œè‰¯å¥½çš„æ€§åƒ¹æ¯”  
  - **Google Gemini**ï¼šä½å»¶é²ï¼Œé•·ä¸Šä¸‹æ–‡ï¼Œå¤šæ¨¡æ…‹æ”¯æ´

### å¯¦å‹™éƒ¨ç½²æª¢æŸ¥æ¸…å–®

- [ ] é¸æ“‡åˆé©çš„ä¾›æ‡‰å•†å’Œå”å®šï¼ˆSSE vs WebSocketï¼‰
- [ ] å¯¦æ–½å–æ¶ˆæ©Ÿåˆ¶ï¼ˆé¿å…ä¸å¿…è¦çš„æˆæœ¬ï¼‰
- [ ] åŠ å…¥éŒ¯èª¤è™•ç†å’Œé‡è©¦é‚è¼¯
- [ ] è¨­ç½®å…§å®¹éæ¿¾å’Œå®‰å…¨æª¢æŸ¥
- [ ] å»ºç«‹è§€æ¸¬æ€§æŒ‡æ¨™ï¼ˆTTFTã€ååé‡ã€éŒ¯èª¤ç‡ï¼‰
- [ ] è™•ç†çµæ§‹åŒ–è¼¸å‡ºçš„ç‰¹æ®Šéœ€æ±‚
- [ ] æ¸¬è©¦ä¸åŒç¶²è·¯ç’°å¢ƒä¸‹çš„è¡¨ç¾
- [ ] è¨­ç½®åˆç†çš„è¶…æ™‚å’Œé™åˆ¶æ©Ÿåˆ¶

---

::: tip ä¸‹ä¸€æ­¥å­¸ç¿’å»ºè­°
æŒæ¡äº†ä¸²æµèŠå¤©æ¨¡å‹å¾Œï¼Œå»ºè­°æ·±å…¥å­¸ç¿’ï¼š

1. [ç›£æ§èˆ‡å¯è§€æ¸¬æ€§](/tutorials/monitoring) - å­¸ç¿’ LangSmith æ•´åˆç›£æ§ä¸²æµæ€§èƒ½
2. [è¨˜æ†¶æ©Ÿåˆ¶èˆ‡å°è©±ç®¡ç†](/tutorials/memory-systems) - åœ¨ä¸²æµç’°å¢ƒä¸­ç®¡ç†å°è©±ç‹€æ…‹
3. [é€²éšæ‡‰ç”¨æ¡ˆä¾‹](/tutorials/advanced-examples) - æŸ¥çœ‹ä¼æ¥­ç´šä¸²æµå¯¦ä½œç¯„ä¾‹
4. [LCEL è¡¨é”å¼èªè¨€](/tutorials/lcel) - å­¸ç¿’å¦‚ä½•åœ¨ä¸²æµä¸­çµ„åˆè¤‡é›œçš„è™•ç†éˆ
:::

::: warning âš ï¸ ç”Ÿç”¢ç’°å¢ƒæ³¨æ„äº‹é …

**æ€§èƒ½å„ªåŒ–**ï¼š
- ç›£æ§ TTFT (Time to First Token) å’Œæ•´é«”å»¶é²
- å¯¦æ–½åˆé©çš„ç·©è¡å’Œæ‰¹æ¬¡ç­–ç•¥
- è€ƒæ…® CDN å’Œé‚Šç·£è¨ˆç®—å„ªåŒ–

**æˆæœ¬æ§åˆ¶**ï¼š
- å¯¦æ–½æ™ºèƒ½å–æ¶ˆæ©Ÿåˆ¶ç¯€çœæˆæœ¬
- ç›£æ§ token ä½¿ç”¨é‡ï¼Œé¿å…æ„å¤–é«˜é¡è²»ç”¨
- æ ¹æ“šä½¿ç”¨å ´æ™¯é¸æ“‡æˆæœ¬æ•ˆç›Šæœ€ä½³çš„æ¨¡å‹

**å®‰å…¨æ€§**ï¼š
- å¯¦æ–½å…§å®¹éæ¿¾ï¼Œé¿å…æœ‰å®³å…§å®¹å³æ™‚é¡¯ç¤º
- ä¿è­· API é‡‘é‘°ï¼Œé¿å…åœ¨å‰ç«¯æš´éœ²
- å¯¦æ–½é©ç•¶çš„é€Ÿç‡é™åˆ¶å’Œä½¿ç”¨è€…é©—è­‰
:::