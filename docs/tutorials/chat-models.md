# Chat Models å°è©±æ¨¡å‹

## ä»€éº¼æ˜¯ Chat Modelsï¼Ÿ

åƒ GPT-4 é€™æ¨£çš„èŠå¤©æ¨¡å‹ï¼Œå·²ç¶“æˆç‚ºä½¿ç”¨ OpenAI API çš„ä¸»è¦æ–¹å¼ã€‚å®ƒä¸å†æ˜¯å–®ç´”çš„ã€Œè¼¸å…¥æ–‡å­— â†’ è¼¸å‡ºæ–‡å­—ã€æ¨¡å¼ï¼Œè€Œæ˜¯ä»¥**è¨Šæ¯ï¼ˆmessagesï¼‰**ä½œç‚ºè¼¸å…¥èˆ‡è¼¸å‡ºçš„äº’å‹•æ–¹å¼ã€‚

### å‚³çµ±æ¨¡å‹ vs Chat Models

```mermaid
graph LR
    subgraph "å‚³çµ±æ–‡å­—æ¨¡å‹"
        A[è¼¸å…¥æ–‡å­—] --> B[LLMè™•ç†] --> C[è¼¸å‡ºæ–‡å­—]
    end
    
    subgraph "Chat Models (å°è©±æ¨¡å‹)"
        D[è¨Šæ¯æ¸…å–®] --> E[Chat LLM] --> F[AIMessage]
        
        subgraph "è¨Šæ¯é¡å‹"
            G[SystemMessage<br/>ç³»çµ±æŒ‡ä»¤]
            H[HumanMessage<br/>äººé¡è¼¸å…¥] 
            I[AIMessage<br/>AIå›æ‡‰]
            J[AIMessageChunk<br/>ä¸²æµå›æ‡‰]
            K[ToolMessage<br/>å·¥å…·å›æ‡‰]
        end
        
        D -.-> G
        D -.-> H
        F -.-> I
    end
```

**ä¸»è¦å·®ç•°ï¼š**

| ç‰¹æ€§ | å‚³çµ±æ¨¡å‹ | Chat Models |
|------|----------|------------|
| **è¼¸å…¥æ ¼å¼** | ç´”æ–‡å­—å­—ä¸² | **è¨Šæ¯æ¸…å–®**ï¼ˆSystem/Human/AI/Toolâ€¦ï¼‰ |
| **ä¸Šä¸‹æ–‡ç®¡ç†** | éœ€æ‰‹å‹•æŠŠæ­·å²æ‹¼é€²æç¤º | **ä»¥è¨Šæ¯ç‚ºå–®ä½è¼ƒå¥½ç¶­è­·**ï¼›ä½†**ä¸æœƒè‡ªå‹•è¨˜ä½**ï¼Œä»éœ€é¡¯å¼æä¾›æˆ–ç”¨è¨˜æ†¶å…ƒä»¶ |
| **è§’è‰²å€åˆ†** | ç„¡æ˜ç¢ºè§’è‰² | **æ˜ç¢ºè§’è‰²**ï¼ˆSystem/Human/AIï¼›éƒ¨åˆ†ä¾›æ‡‰å•†é‚„æœ‰ Tool/Functionï¼‰ |
| **å¤šè¼ªå°è©±** | å¯è¡Œï¼Œä½†éœ€è‡ªè¡Œç®¡ç†æ­·å² | **æ›´é †æ‰‹**ï¼›**ä»éœ€æä¾›æ­·å²æˆ–ç”¨è¨˜æ†¶æ¨¡çµ„** |
| **å·¥å…·æ•´åˆï¼ˆFunction/Tool Callingï¼‰** | å¤šåŠè¦è‡ªè¨‚æ ¼å¼èˆ‡è§£æ | å¤šæ•¸ä¾›æ‡‰å•†**åŸç”Ÿæˆ–ä¸€éšæ”¯æ´**ï¼›LangChain æœ‰å°æ‡‰è¨Šæ¯å‹åˆ¥èˆ‡åŸ·è¡Œå™¨ï¼ˆä¾æ¨¡å‹è€Œå®šï¼‰ |
| **ä¸²æµå›æ‡‰** | å¤šæ•¸ä¾›æ‡‰å•†æ”¯æ´ï¼ˆtoken æµï¼‰ | å¤šæ•¸ä¾›æ‡‰å•†æ”¯æ´ï¼›LangChain ä»¥ `AIMessageChunk` è¡¨ç¤ºç‰‡æ®µ |

## è¨Šæ¯é¡å‹è©³è§£ (LangChain 0.3+)

LangChain 0.3+ ç‰ˆæœ¬æ”¯æ´å¤šç¨®è¨Šæ¯é¡å‹ï¼Œå…¨éƒ¨åŒ¯å…¥è·¯å¾‘å·²æ›´æ–°ç‚ºï¼š

```python
from langchain_core.messages import (
    SystemMessage, HumanMessage, AIMessage,
    AIMessageChunk, ToolMessage, ChatMessage
)
```

### 1. SystemMessage ç³»çµ±è¨Šæ¯

**ä½œç”¨ï¼š** è¨­å®š AI çš„è§’è‰²ã€è¡Œç‚ºè¦ç¯„å’Œä¸Šä¸‹æ–‡èƒŒæ™¯

```python
from langchain_core.messages import SystemMessage

system_msg = SystemMessage(content="ä½ æ˜¯ä¸€ä½å°ˆæ¥­çš„ Python å·¥ç¨‹å¸«ï¼Œå°ˆç²¾æ–¼ Web é–‹ç™¼ã€‚")
```

**ç‰¹é»ï¼š**
- ğŸ¯ å®šç¾© AI çš„ã€Œèº«ä»½ã€å’Œã€Œå°ˆæ¥­é ˜åŸŸã€
- ğŸ“‹ è¨­å®šå›æ‡‰çš„é¢¨æ ¼å’Œæ ¼å¼è¦æ±‚
- ğŸ”’ OpenAI ç‰¹åˆ¥å¼·åŒ–äº†å° SystemMessage çš„éµå¾ªåº¦
- âœ… Anthropic Claude ç¾å·²å®Œå…¨æ”¯æ´ system prompts

### 2. HumanMessage äººé¡è¨Šæ¯

**ä½œç”¨ï¼š** ä»£è¡¨ä½¿ç”¨è€…çš„è¼¸å…¥ã€å•é¡Œæˆ–è«‹æ±‚

```python
from langchain_core.messages import HumanMessage

human_msg = HumanMessage(content="è«‹è§£é‡‹ä»€éº¼æ˜¯ RESTful API çš„è¨­è¨ˆåŸå‰‡ï¼Ÿ")
```

### 3. AIMessage AI è¨Šæ¯

**ä½œç”¨ï¼š** ä»£è¡¨ AI ç³»çµ±çš„å›æ‡‰

```python
from langchain_core.messages import AIMessage

ai_msg = AIMessage(content="RESTful API è¨­è¨ˆåŸå‰‡åŒ…å«ä»¥ä¸‹å¹¾å€‹æ ¸å¿ƒæ¦‚å¿µ...")
```

### 4. AIMessageChunk ä¸²æµè¨Šæ¯å¡Š

**ç”¨é€”ï¼š** ä¸²æµå›æ‡‰æ™‚çš„åˆ†æ®µè¨Šæ¯ï¼Œç”± `.stream()` æ–¹æ³•ç”¢ç”Ÿ

```python
from langchain_core.messages import AIMessageChunk

# ä¸²æµå›æ‡‰æœƒç”¢å‡º AIMessageChunk ç‰©ä»¶
chunks: list[AIMessageChunk] = []
for chunk in chat.stream(messages):
    print(chunk.content, end="", flush=True)
    chunks.append(chunk)

# å¯ä»¥å°‡æ‰€æœ‰ chunks çµ„åˆæˆå®Œæ•´å›æ‡‰
full_response = "".join(chunk.content for chunk in chunks)
```

### 5. ToolMessage å·¥å…·è¨Šæ¯

**ç”¨é€”ï¼š** å·¥å…·å‘¼å«çš„å›æ‡‰çµæœï¼Œå¿…é ˆåŒ…å«æ­£ç¢ºçš„ `tool_call_id`

```python
from langchain_core.messages import ToolMessage

# tool_call_id å¿…é ˆå°æ‡‰æ¨¡å‹å›å‚³çš„ ID
tool_msg = ToolMessage(
    content="è¨ˆç®—çµæœ: 42",
    tool_call_id="call_abc123"  # ä¾†è‡ªæ¨¡å‹çš„çœŸå¯¦ ID
)
```

### 6. ChatMessage é€šç”¨å°è©±è¨Šæ¯

**ç”¨é€”ï¼š** è‡ªå®šç¾©è§’è‰²çš„éˆæ´»è¨Šæ¯é¡å‹

```python
from langchain_core.messages import ChatMessage

custom_msg = ChatMessage(
    content="é€™æ˜¯ä¾†è‡ªè³‡æ–™åˆ†æå¸«çš„å»ºè­°...",
    role="data_analyst"
)
```

## åŸºæœ¬ä½¿ç”¨ç¯„ä¾‹

### ç¯„ä¾‹ä¸€ï¼šå°ˆæ¥­ç¬‘è©±ç”Ÿæˆå™¨

```python
from langchain_openai import ChatOpenAI
from langchain_core.messages import SystemMessage, HumanMessage

# å»ºç«‹ Chat Modelï¼ˆtemperature=0.5 å¢åŠ å‰µæ„æ€§ï¼‰
chat = ChatOpenAI(
    model="gpt-4o-mini",
    temperature=0.5
)

# å®šç¾©è¨Šæ¯æ¸…å–®
messages = [
    SystemMessage(content="ä½ æ˜¯ä¸€ä½è³‡æ·±çš„è»Ÿé«”å·¥ç¨‹å¸«ï¼Œæ“…é•·ç”¨ç¨‹å¼è¨­è¨ˆçš„è§’åº¦ä¾†èªªç¬‘è©±ã€‚"),
    HumanMessage(content="è«‹åˆ†äº«ä¸€å€‹é—œæ–¼è»Ÿé«”å·¥ç¨‹å¸«çš„ç¬‘è©±ã€‚")
]

# å‘¼å«æ¨¡å‹ï¼ˆæ¨è–¦çš„ç¾ä»£åŒ–æ–¹å¼ï¼‰
response = chat.invoke(messages)
print(response.content)
```

**è¼¸å‡ºç¯„ä¾‹ï¼š**
```
ç‚ºä»€éº¼è»Ÿé«”å·¥ç¨‹å¸«ç¸½æ˜¯ææ··è–èª•ç¯€å’Œè¬è–ç¯€ï¼Ÿ
å› ç‚º Oct 31 == Dec 25ï¼
ï¼ˆå…«é€²ä½çš„ 31 ç­‰æ–¼åé€²ä½çš„ 25ï¼‰
```

### ç¯„ä¾‹äºŒï¼šå¤šè¼ªå°è©±ç³»çµ±

```python
from langchain_openai import ChatOpenAI
from langchain_core.messages import SystemMessage, HumanMessage, AIMessage

chat = ChatOpenAI(model="gpt-4o-mini", temperature=0.1)

# å»ºç«‹å°è©±æ­·å²
conversation_history = [
    SystemMessage(content="ä½ æ˜¯ä¸€ä½å‹å–„çš„ Python æ•™å­¸åŠ©æ‰‹ã€‚"),
    HumanMessage(content="ä»€éº¼æ˜¯ Python ä¸­çš„ list comprehensionï¼Ÿ"),
    AIMessage(content="List comprehension æ˜¯ Python ä¸­å»ºç«‹åˆ—è¡¨çš„ç°¡æ½”èªæ³•..."),
    HumanMessage(content="å¯ä»¥çµ¦æˆ‘ä¸€å€‹å¯¦éš›çš„ä¾‹å­å—ï¼Ÿ")
]

# ç¹¼çºŒå°è©±
response = chat.invoke(conversation_history)
print(response.content)
```

## ç¾ä»£åŒ–çš„å‘¼å«æ–¹å¼

### æ¨™æº– Runnable ä»‹é¢

LangChain 0.3+ å…¨é¢æ¡ç”¨ Runnable ä»‹é¢ï¼Œæä¾›çµ±ä¸€çš„èª¿ç”¨æ–¹å¼ï¼š

```python
# âœ… æ¨è–¦çš„ç¾ä»£å‘¼å«æ–¹å¼
response = chat.invoke(messages)           # åŒæ­¥å‘¼å«
response = await chat.ainvoke(messages)    # éåŒæ­¥å‘¼å«
responses = chat.batch([messages])         # æ‰¹æ¬¡è™•ç†
stream = chat.stream(messages)             # ä¸²æµå›æ‡‰

# âŒ èˆŠç‰ˆå‘¼å«æ–¹å¼ï¼ˆLegacyï¼Œä¸å»ºè­°ä½¿ç”¨ï¼‰
response = chat(messages)  # å·²å»¢æ£„
```

### ä¸²æµå›æ‡‰ç¯„ä¾‹

```python
from langchain_openai import ChatOpenAI
from langchain_core.messages import SystemMessage, HumanMessage, AIMessageChunk

chat = ChatOpenAI(model="gpt-4o-mini")
messages = [
    SystemMessage(content="ä½ æ˜¯ä¸€ä½æœ‰ç”¨çš„åŠ©æ‰‹ã€‚"),
    HumanMessage(content="è«‹è©³ç´°è§£é‡‹ä»€éº¼æ˜¯æ©Ÿå™¨å­¸ç¿’ã€‚")
]

# ä¸²æµå›æ‡‰è™•ç†
chunks: list[AIMessageChunk] = []
print("AI å›æ‡‰:", end=" ")

for chunk in chat.stream(messages):
    print(chunk.content, end="", flush=True)
    chunks.append(chunk)

# çµ„åˆå®Œæ•´å›æ‡‰
full_response = "".join(chunk.content for chunk in chunks)
print(f"\n\nå®Œæ•´å›æ‡‰é•·åº¦: {len(full_response)} å­—å…ƒ")
```

### å®‰å…¨çš„å·¥å…·å‘¼å«ç¯„ä¾‹

```python
from langchain_core.tools import tool
from langchain_core.messages import ToolMessage
import re
import ast
import operator

@tool
def safe_calculate(expression: str) -> str:
    """å®‰å…¨çš„æ•¸å­¸è¨ˆç®—å™¨ï¼ˆåƒ…æ”¯æ´åŸºæœ¬é‹ç®—ï¼š+, -, *, /, **, ()ï¼‰"""
    
    # å®‰å…¨æª¢æŸ¥ï¼šåªå…è¨±æ•¸å­—ã€åŸºæœ¬é‹ç®—ç¬¦è™Ÿå’Œç©ºç™½
    if not re.fullmatch(r"[0-9+\-*/.() \t]+", expression):
        return "éŒ¯èª¤: åŒ…å«ä¸å…è¨±çš„å­—å…ƒ"
    
    try:
        # ä½¿ç”¨ ast.literal_eval çš„å®‰å…¨æ›¿ä»£æ–¹æ¡ˆ
        # åœ¨ç”Ÿç”¢ç’°å¢ƒä¸­ï¼Œå»ºè­°ä½¿ç”¨å°ˆé–€çš„æ•¸å­¸è¡¨é”å¼è§£æå™¨
        node = ast.parse(expression, mode='eval')
        
        # ç°¡åŒ–ç¤ºç¯„ï¼šåƒ…å…è¨±åŸºæœ¬æ•¸å­¸é‹ç®—
        allowed_operators = {
            ast.Add: operator.add,
            ast.Sub: operator.sub,
            ast.Mult: operator.mul,
            ast.Div: operator.truediv,
            ast.Pow: operator.pow,
            ast.USub: operator.neg,
        }
        
        def eval_node(node):
            if isinstance(node, ast.Constant):
                return node.value
            elif isinstance(node, ast.BinOp):
                left = eval_node(node.left)
                right = eval_node(node.right)
                op = allowed_operators.get(type(node.op))
                if op:
                    return op(left, right)
                else:
                    raise ValueError("ä¸æ”¯æ´çš„é‹ç®—ç¬¦")
            elif isinstance(node, ast.UnaryOp):
                operand = eval_node(node.operand)
                op = allowed_operators.get(type(node.op))
                if op:
                    return op(operand)
                else:
                    raise ValueError("ä¸æ”¯æ´çš„ä¸€å…ƒé‹ç®—ç¬¦")
            else:
                raise ValueError("ä¸æ”¯æ´çš„è¡¨é”å¼é¡å‹")
        
        result = eval_node(node.body)
        return f"è¨ˆç®—çµæœ: {result}"
        
    except Exception as e:
        return f"è¨ˆç®—éŒ¯èª¤: {str(e)}"

# ä½¿ç”¨ç¯„ä¾‹ï¼ˆæ³¨æ„ï¼štool_call_id å¿…é ˆä¾†è‡ªæ¨¡å‹çš„å¯¦éš›å›æ‡‰ï¼‰
def create_tool_message(content: str, call_id: str):
    """å‰µå»ºå·¥å…·è¨Šæ¯ï¼Œcall_id æ‡‰è©²ä¾†è‡ªæ¨¡å‹çš„çœŸå¯¦ tool_call_id"""
    return ToolMessage(
        content=content,
        tool_call_id=call_id  # å¯¦éš›ä½¿ç”¨æ™‚å¿…é ˆæ˜¯æ¨¡å‹å›å‚³çš„ ID
    )

# ç¯„ä¾‹ç”¨æ³•
result = safe_calculate("2 + 3 * 4")
print(result)  # è¼¸å‡º: è¨ˆç®—çµæœ: 14
```

## é€²éšåŠŸèƒ½èˆ‡æœ€ä½³å¯¦è¸

### 1. ä½¿ç”¨ ChatPromptTemplate (ç¾ä»£åŒ–æ–¹å¼)

LangChain 0.3+ æ¨è–¦ä½¿ç”¨ tuple æ ¼å¼çš„ç¾ä»£åŒ–æ¨¡æ¿ï¼š

```python
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_openai import ChatOpenAI

# ç¾ä»£åŒ–çš„ ChatPromptTemplate å¯«æ³•
prompt = ChatPromptTemplate.from_messages([
    ("system", "ä½ æ˜¯ä¸€ä½å°ˆç²¾æ–¼{domain}çš„å°ˆå®¶ï¼Œå…·æœ‰{years}å¹´çš„ç¶“é©—ã€‚"),
    MessagesPlaceholder("chat_history"),  # ç”¨æ–¼æ’å…¥æ­·å²å°è©±
    ("human", "é—œæ–¼{topic}ï¼Œè«‹æä¾›å°ˆæ¥­çš„å»ºè­°å’Œæœ€ä½³å¯¦è¸ã€‚"),
])

# å»ºç«‹æ¨¡å‹
model = ChatOpenAI(model="gpt-4o-mini", temperature=0.2)

# ä½¿ç”¨æ¨¡æ¿
response = model.invoke(prompt.format_prompt(
    domain="æ©Ÿå™¨å­¸ç¿’",
    years="10",
    topic="æ¨¡å‹éƒ¨ç½²ç­–ç•¥",
    chat_history=[]  # ç©ºçš„æ­·å²å°è©±
))

print(response.content)
```

### 2. èˆ‡ LCEL éˆå¼çµ„åˆ

çµåˆ LangChain Expression Language çš„å¼·å¤§åŠŸèƒ½ï¼š

```python
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.output_parsers import StrOutputParser
from langchain_openai import ChatOpenAI

# å»ºç«‹è™•ç†éˆ
prompt = ChatPromptTemplate.from_messages([
    ("system", "ä½ æ˜¯ä¸€ä½{role}ï¼Œè«‹ç”¨å°ˆæ¥­ä¸”æ˜“æ‡‚çš„æ–¹å¼å›ç­”å•é¡Œã€‚"),
    ("human", "{question}")
])

model = ChatOpenAI(model="gpt-4o-mini")
output_parser = StrOutputParser()

# LCEL éˆå¼çµ„åˆ
chain = prompt | model | output_parser

# ä½¿ç”¨éˆ
result = chain.invoke({
    "role": "Python æ•™å­¸å°ˆå®¶",
    "question": "ä»€éº¼æ˜¯è£é£¾å™¨ï¼ˆdecoratorï¼‰ï¼Ÿ"
})

print(result)
```

### 3. ä¸åŒæ¨¡å‹çš„ SystemMessage æ”¯æ´

```python
from langchain_openai import ChatOpenAI
from langchain_anthropic import ChatAnthropic  
from langchain_ollama import ChatOllama

# OpenAI GPT-4 - å®Œå…¨åŸç”Ÿæ”¯æ´
openai_chat = ChatOpenAI(model="gpt-4o-mini")

# Anthropic Claude - ç¾å·²å®Œå…¨æ”¯æ´ system prompts
claude_chat = ChatAnthropic(
    model="claude-3-sonnet-20240229"
    # system prompts é€é ChatAnthropic è‡ªå‹•è™•ç†
)

# æœ¬åœ°æ¨¡å‹ï¼ˆOllamaï¼‰- æ”¯æ´å·¥å…·å‘¼å«å’Œ system prompts
local_chat = ChatOllama(model="llama3.2:1b")  # æ¨è–¦æ•™å­¸ä½¿ç”¨
```

### 4. éŒ¯èª¤è™•ç†èˆ‡é‡è©¦æ©Ÿåˆ¶

```python
from langchain_core.language_models import BaseChatModel  # æ›´æ–°çš„åŒ¯å…¥è·¯å¾‘
from langchain_core.messages import BaseMessage
from typing import List
import time
import logging

def safe_chat_invoke(
    chat: BaseChatModel, 
    messages: List[BaseMessage], 
    max_retries: int = 3
):
    """å®‰å…¨çš„èŠå¤©æ¨¡å‹å‘¼å«ï¼ŒåŒ…å«æŒ‡æ•¸é€€é¿é‡è©¦æ©Ÿåˆ¶"""
    
    logger = logging.getLogger(__name__)
    
    for attempt in range(max_retries):
        try:
            response = chat.invoke(messages)
            return response
            
        except Exception as e:
            logger.warning(f"å˜—è©¦ {attempt + 1}/{max_retries} å¤±æ•—: {e}")
            
            if attempt == max_retries - 1:
                logger.error(f"æ‰€æœ‰é‡è©¦éƒ½å¤±æ•—ï¼Œæœ€å¾ŒéŒ¯èª¤: {e}")
                raise e
                
            # æŒ‡æ•¸é€€é¿ç­–ç•¥
            sleep_time = 2 ** attempt
            logger.info(f"ç­‰å¾… {sleep_time} ç§’å¾Œé‡è©¦...")
            time.sleep(sleep_time)

# ä½¿ç”¨ç¯„ä¾‹
try:
    response = safe_chat_invoke(chat, messages, max_retries=3)
    print(response.content)
except Exception as e:
    print(f"ç„¡æ³•ç²å¾—å›æ‡‰: {e}")
```

## å¯¦éš›æ‡‰ç”¨å ´æ™¯

### å ´æ™¯ä¸€ï¼šæ™ºèƒ½å®¢æœåŠ©æ‰‹

```python
from langchain_openai import ChatOpenAI
from langchain_core.messages import SystemMessage, HumanMessage, BaseMessage
from typing import List, Optional

def create_customer_service_bot():
    chat = ChatOpenAI(model="gpt-4o-mini", temperature=0.3)
    
    system_prompt = """ä½ æ˜¯ TechCorp çš„å®¢æœä»£è¡¨ï¼Œå…·å‚™ä»¥ä¸‹ç‰¹è³ªï¼š
    1. å‹å–„ä¸”å°ˆæ¥­çš„æºé€šé¢¨æ ¼
    2. ç†Ÿæ‚‰å…¬å¸ç”¢å“å’Œæœå‹™
    3. èƒ½å¤ å¿«é€Ÿè§£æ±ºå®¢æˆ¶å•é¡Œ
    4. å¦‚ç„¡æ³•è§£æ±ºï¼Œæœƒé©æ™‚è½‰ä»‹çµ¦å°ˆæ¥­éƒ¨é–€
    
    å›ç­”æ ¼å¼ï¼š
    - å…ˆè¡¨é”ç†è§£å®¢æˆ¶å•é¡Œ
    - æä¾›å…·é«”è§£æ±ºæ–¹æ¡ˆ
    - ç¢ºèªå®¢æˆ¶æ˜¯å¦æ»¿æ„
    """
    
    def respond_to_customer(
        customer_message: str, 
        conversation_history: Optional[List[BaseMessage]] = None
    ) -> str:
        messages = [SystemMessage(content=system_prompt)]
        
        if conversation_history:
            messages.extend(conversation_history)
        
        messages.append(HumanMessage(content=customer_message))
        
        response = chat.invoke(messages)
        return response.content
    
    return respond_to_customer

# ä½¿ç”¨ç¯„ä¾‹
customer_service = create_customer_service_bot()
response = customer_service("æˆ‘çš„è¨‚å–®é²é²æ²’æœ‰æ”¶åˆ°ï¼Œè©²æ€éº¼è¾¦ï¼Ÿ")
print(response)
```

### å ´æ™¯äºŒï¼šç¨‹å¼ç¢¼å¯©æŸ¥åŠ©æ‰‹

```python
from langchain_openai import ChatOpenAI
from langchain_core.messages import SystemMessage, HumanMessage

def create_code_reviewer():
    chat = ChatOpenAI(model="gpt-4o-mini", temperature=0.1)
    
    system_msg = SystemMessage(content="""
    ä½ æ˜¯ä¸€ä½è³‡æ·±çš„ç¨‹å¼ç¢¼å¯©æŸ¥å°ˆå®¶ï¼Œå°ˆç²¾æ–¼ï¼š
    - ç¨‹å¼ç¢¼å“è³ªè©•ä¼°
    - å®‰å…¨æ€§æ¼æ´æª¢æ¸¬
    - æ•ˆèƒ½å„ªåŒ–å»ºè­°
    - æœ€ä½³å¯¦è¸æ¨è–¦
    
    å¯©æŸ¥æ ¼å¼ï¼š
    1. ç¨‹å¼ç¢¼å„ªé»
    2. éœ€è¦æ”¹é€²çš„åœ°æ–¹
    3. å…·é«”å»ºè­°
    4. é¢¨éšªè©•ä¼°ï¼ˆåŒ…æ‹¬å®‰å…¨æ€§è€ƒé‡ï¼‰
    """)
    
    def review_code(code: str, language: str = "python") -> str:
        messages = [
            system_msg,
            HumanMessage(content=f"è«‹å¯©æŸ¥ä»¥ä¸‹ {language} ç¨‹å¼ç¢¼ï¼š\n\n```{language}\n{code}\n```")
        ]
        
        response = chat.invoke(messages)
        return response.content
    
    return review_code

# ä½¿ç”¨ç¯„ä¾‹
code_reviewer = create_code_reviewer()

code_to_review = """
def process_users(users):
    result = []
    for user in users:
        if user['age'] > 18:
            result.append(user['name'])
    return result
"""

review = code_reviewer(code_to_review)
print(review)
```

## æ•ˆèƒ½å„ªåŒ–èˆ‡æˆæœ¬æ§åˆ¶

### 1. ç¾ä»£åŒ–æ¨¡å‹é¸æ“‡ç­–ç•¥ (2025 æ¨è–¦)

```python
from langchain_openai import ChatOpenAI

def get_appropriate_model(task_complexity: str):
    """æ ¹æ“šä»»å‹™è¤‡é›œåº¦é¸æ“‡æœ€æ–°çš„æ¨¡å‹"""
    if task_complexity == "simple":
        return ChatOpenAI(model="gpt-4o-mini", temperature=0.2)  # æ›¿ä»£ gpt-3.5-turbo
    elif task_complexity == "medium":
        return ChatOpenAI(model="gpt-4o", temperature=0.3)
    else:  # complex
        return ChatOpenAI(model="gpt-4", temperature=0.4)  # æˆ–ä½¿ç”¨ gpt-o1 ç³»åˆ—
```

### 2. æ™ºèƒ½ Token ç®¡ç†èˆ‡å„ªåŒ–

```python
from langchain_core.messages import SystemMessage, BaseMessage
from langchain_core.messages.utils import count_tokens_approximately
from typing import List

def trim_conversation_to_budget(
    messages: List[BaseMessage], 
    budget: int = 80000  # ç¾ä»£æ¨¡å‹çš„ä¸Šä¸‹æ–‡çª—è¼ƒå¤§
) -> List[BaseMessage]:
    """æ™ºèƒ½ä¿®å‰ªå°è©±ï¼Œä¿æŒåœ¨ token é ç®—å…§"""
    
    # ä¿ç•™ SystemMessageï¼ˆé€šå¸¸åœ¨é–‹é ­ï¼‰
    system_messages = [msg for msg in messages if isinstance(msg, SystemMessage)]
    other_messages = [msg for msg in messages if not isinstance(msg, SystemMessage)]
    
    # ä½¿ç”¨å®˜æ–¹ token è¨ˆæ•¸å·¥å…·
    current_tokens = count_tokens_approximately(messages)
    
    if current_tokens <= budget:
        return messages
    
    # å¾æœ€èˆŠçš„å°è©±é–‹å§‹ç§»é™¤ï¼ˆä¿ç•™æœ€è¿‘çš„å°è©±æ›´é‡è¦ï¼‰
    while current_tokens > budget and len(other_messages) > 2:
        # ç§»é™¤æœ€èˆŠçš„ä¸€å° Human-AI å°è©±
        if len(other_messages) >= 2:
            other_messages = other_messages[2:]  # ç§»é™¤å‰å…©æ¢è¨Šæ¯
        else:
            other_messages = other_messages[1:]  # ç§»é™¤ä¸€æ¢è¨Šæ¯
        
        test_messages = system_messages + other_messages
        current_tokens = count_tokens_approximately(test_messages)
    
    return system_messages + other_messages

# ä½¿ç”¨ç¯„ä¾‹
optimized_messages = trim_conversation_to_budget(conversation_history, budget=50000)
response = chat.invoke(optimized_messages)
```

### 3. å®˜æ–¹å¿«å–æ©Ÿåˆ¶

```python
from langchain.globals import set_llm_cache
from langchain.cache import InMemoryCache
from langchain_openai import ChatOpenAI

# æ–¹æ³•ä¸€ï¼šä½¿ç”¨å…¨åŸŸå¿«å–
set_llm_cache(InMemoryCache())

# æ–¹æ³•äºŒï¼šæ¨¡å‹ç´šå¿«å–ï¼ˆæ¨è–¦ï¼‰
chat_with_cache = ChatOpenAI(
    model="gpt-4o-mini",
    cache=True  # å•Ÿç”¨æ¨¡å‹å¿«å–
)

# ä½¿ç”¨å¿«å–çš„èŠå¤©æ¨¡å‹
response1 = chat_with_cache.invoke(messages)  # ç¬¬ä¸€æ¬¡èª¿ç”¨
response2 = chat_with_cache.invoke(messages)  # å¾å¿«å–è¿”å›ï¼Œç›¸åŒè¼¸å…¥

# æ–¹æ³•ä¸‰ï¼šè‡ªå®šç¾©å¿«å–å¯¦ä½œï¼ˆé€²éšç”¨æ³•ï¼‰
import hashlib
from typing import Dict, Any

class SmartChatCache:
    def __init__(self, model: ChatOpenAI):
        self.model = model
        self.cache: Dict[str, Any] = {}
    
    def _generate_cache_key(self, messages: List[BaseMessage]) -> str:
        """åŸºæ–¼è¨Šæ¯å…§å®¹å’Œæ¨¡å‹åƒæ•¸ç”Ÿæˆå¿«å–éµ"""
        content = str([msg.content for msg in messages])
        model_config = f"{self.model.model_name}_{self.model.temperature}"
        combined = f"{content}_{model_config}"
        return hashlib.sha256(combined.encode()).hexdigest()
    
    def invoke_with_smart_cache(self, messages: List[BaseMessage]):
        cache_key = self._generate_cache_key(messages)
        
        if cache_key in self.cache:
            print(f"ğŸ’¨ å¾æ™ºèƒ½å¿«å–è¿”å›çµæœ (key: {cache_key[:8]}...)")
            return self.cache[cache_key]
        
        print("ğŸ”„ èª¿ç”¨æ¨¡å‹ç”Ÿæˆæ–°å›æ‡‰...")
        response = self.model.invoke(messages)
        self.cache[cache_key] = response
        
        return response
    
    def clear_cache(self):
        """æ¸…ç©ºå¿«å–"""
        self.cache.clear()
        print("ğŸ—‘ï¸ å¿«å–å·²æ¸…ç©º")

# ä½¿ç”¨æ™ºèƒ½å¿«å–
smart_cache = SmartChatCache(ChatOpenAI(model="gpt-4o-mini"))
response = smart_cache.invoke_with_smart_cache(messages)
```

## å¸¸è¦‹å•é¡Œèˆ‡è§£æ±ºæ–¹æ¡ˆ

### Q1: å¦‚ä½•è™•ç†ä¸åŒæ¨¡å‹å° SystemMessage çš„æ”¯æ´å·®ç•°ï¼Ÿ

ç¾ä»£ç‰ˆæœ¬çš„ä¸»è¦èŠå¤©æ¨¡å‹éƒ½å·²æ”¯æ´ SystemMessageï¼ŒåŒ…æ‹¬ï¼š

```python
from langchain_openai import ChatOpenAI
from langchain_anthropic import ChatAnthropic
from langchain_ollama import ChatOllama

def create_universal_chat_model(model_provider: str):
    """å»ºç«‹é€šç”¨çš„èŠå¤©æ¨¡å‹ï¼ˆ2025 å¹´ç‰ˆæœ¬ï¼‰"""
    
    if model_provider == "openai":
        return ChatOpenAI(model="gpt-4o-mini")  # å®Œå…¨æ”¯æ´ SystemMessage
    
    elif model_provider == "anthropic":
        return ChatAnthropic(model="claude-3-sonnet-20240229")  # ç¾å·²å®Œå…¨æ”¯æ´
    
    elif model_provider == "local":
        return ChatOllama(model="llama3.2:1b")  # æ¨è–¦æ•™å­¸ä½¿ç”¨ï¼Œæ”¯æ´ SystemMessage
    
    else:
        raise ValueError(f"ä¸æ”¯æ´çš„æ¨¡å‹æä¾›å•†: {model_provider}")

# ä½¿ç”¨ç¯„ä¾‹ - æ‰€æœ‰æ¨¡å‹éƒ½èƒ½æ­£å¸¸è™•ç† SystemMessage
models = {
    "openai": create_universal_chat_model("openai"),
    "anthropic": create_universal_chat_model("anthropic"),
    "local": create_universal_chat_model("local")
}

# çµ±ä¸€çš„è¨Šæ¯æ ¼å¼é©ç”¨æ–¼æ‰€æœ‰æ¨¡å‹
from langchain_core.messages import SystemMessage, HumanMessage

messages = [
    SystemMessage(content="ä½ æ˜¯ä¸€ä½å°ˆæ¥­çš„ç¨‹å¼è¨­è¨ˆå°å¸«ã€‚"),
    HumanMessage(content="è«‹è§£é‡‹ä»€éº¼æ˜¯éè¿´ï¼Ÿ")
]

for provider, model in models.items():
    print(f"\n=== {provider.upper()} å›æ‡‰ ===")
    response = model.invoke(messages)
    print(response.content[:200] + "...")
```

### Q2: ç¾ä»£åŒ–è¨˜æ†¶ç®¡ç† - LCEL vs å‚³çµ± Memory

æ¨è–¦ä½¿ç”¨ LCEL å’Œ RunnableWithMessageHistory æ›¿ä»£å‚³çµ± Memoryï¼š

```python
from langchain_core.runnables.history import RunnableWithMessageHistory
from langchain_community.chat_message_histories import ChatMessageHistory
from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder
from langchain_openai import ChatOpenAI

# ç¾ä»£åŒ–è¨˜æ†¶ç®¡ç†æ–¹å¼
def create_modern_conversation_manager():
    """ä½¿ç”¨ LCEL çš„ç¾ä»£å°è©±ç®¡ç†å™¨"""
    
    # å»ºç«‹èŠå¤©æ¨¡æ¿
    prompt = ChatPromptTemplate.from_messages([
        ("system", "ä½ æ˜¯ä¸€ä½å‹å–„çš„åŠ©æ‰‹ï¼Œèƒ½è¨˜ä½æˆ‘å€‘çš„å°è©±æ­·å²ã€‚"),
        MessagesPlaceholder(variable_name="history"),
        ("human", "{input}"),
    ])
    
    # å»ºç«‹è™•ç†éˆ
    model = ChatOpenAI(model="gpt-4o-mini")
    chain = prompt | model
    
    # è¨˜æ†¶å„²å­˜
    store = {}
    
    def get_session_history(session_id: str) -> ChatMessageHistory:
        if session_id not in store:
            store[session_id] = ChatMessageHistory()
        return store[session_id]
    
    # åŒ…è£æˆæœ‰è¨˜æ†¶çš„éˆ
    conversation_chain = RunnableWithMessageHistory(
        chain,
        get_session_history,
        input_messages_key="input",
        history_messages_key="history",
    )
    
    return conversation_chain

# ä½¿ç”¨ç¾ä»£åŒ–å°è©±ç®¡ç†
conversation = create_modern_conversation_manager()

# é…ç½®æœƒè©±
config = {"configurable": {"session_id": "user_abc123"}}

# å¤šè¼ªå°è©±
response1 = conversation.invoke(
    {"input": "æˆ‘å« Aliceï¼Œæ˜¯ä¸€åè»Ÿé«”å·¥ç¨‹å¸«"}, 
    config=config
)
print("AI:", response1.content)

response2 = conversation.invoke(
    {"input": "æˆ‘å‰›æ‰èªªæˆ‘çš„è·æ¥­æ˜¯ä»€éº¼ï¼Ÿ"}, 
    config=config
)
print("AI:", response2.content)  # AI æœƒè¨˜å¾— Alice æ˜¯è»Ÿé«”å·¥ç¨‹å¸«

# å°æ¯”ï¼šå‚³çµ± Memory æ–¹å¼ï¼ˆä¸æ¨è–¦æ–°å°ˆæ¡ˆä½¿ç”¨ï¼‰
# æ³¨æ„ï¼šå®˜æ–¹å»ºè­°æ–°å°ˆæ¡ˆå„ªå…ˆä½¿ç”¨ LCEL å’Œ LangGraph é€²è¡Œç‹€æ…‹ç®¡ç†
```

### Q3: å¦‚ä½•è™•ç†è¶…é•·å°è©±æ­·å²çš„ç¾ä»£åŒ–æ–¹æ¡ˆ

```python
from langchain_core.messages import trim_messages
from langchain_core.messages.utils import count_tokens_approximately

def create_smart_conversation_with_trimming():
    """æ™ºèƒ½å°è©±ç®¡ç†ï¼Œè‡ªå‹•è™•ç†é•·å°è©±"""
    
    prompt = ChatPromptTemplate.from_messages([
        ("system", "ä½ æ˜¯ä¸€ä½æœ‰ç”¨çš„åŠ©æ‰‹ã€‚ä»¥ä¸‹æ˜¯æˆ‘å€‘çš„å°è©±æ­·å²æ‘˜è¦å’Œæœ€è¿‘çš„å°è©±ã€‚"),
        MessagesPlaceholder(variable_name="history"),
        ("human", "{input}"),
    ])
    
    model = ChatOpenAI(model="gpt-4o-mini")
    
    def smart_trim_messages(messages, token_budget: int = 50000):
        """æ™ºèƒ½ä¿®å‰ªè¨Šæ¯ï¼Œä¿æŒé‡è¦å°è©±"""
        
        # ä½¿ç”¨å®˜æ–¹çš„ trim_messages åŠŸèƒ½
        trimmed = trim_messages(
            messages,
            token_counter=count_tokens_approximately,
            max_tokens=token_budget,
            strategy="last",  # ä¿ç•™æœ€å¾Œçš„è¨Šæ¯
            allow_partial=False,
        )
        
        return trimmed
    
    # è‡ªå®šç¾©éˆï¼ŒåŒ…å«æ™ºèƒ½ä¿®å‰ª
    def conversation_with_trim(input_data, history):
        # ä¿®å‰ªæ­·å²è¨Šæ¯
        trimmed_history = smart_trim_messages(history)
        
        # æ ¼å¼åŒ– prompt
        formatted = prompt.invoke({
            "input": input_data,
            "history": trimmed_history
        })
        
        return model.invoke(formatted)
    
    return conversation_with_trim

# ä½¿ç”¨æ™ºèƒ½ä¿®å‰ªçš„å°è©±ç³»çµ±
smart_conversation = create_smart_conversation_with_trimming()
```

## ç¸½çµ

Chat Models æ˜¯ç¾ä»£ AI æ‡‰ç”¨é–‹ç™¼çš„æ ¸å¿ƒçµ„ä»¶ï¼ŒLangChain 0.3+ æä¾›äº†æ›´å¼·å¤§å’Œå®Œæ•´çš„åŠŸèƒ½ï¼š

### ğŸ¯ æ ¸å¿ƒå„ªå‹¢

- ğŸ­ **å¤šè§’è‰²æ”¯æ´**ï¼šSystemMessageã€HumanMessageã€AIMessageã€AIMessageChunkã€ToolMessage å®Œæ•´è¦†è“‹
- ğŸ’¬ **åŸç”Ÿå°è©±ç®¡ç†**ï¼šè‡ªå‹•ç¶­è­·å¤šè¼ªå°è©±ä¸Šä¸‹æ–‡ï¼Œç„¡éœ€æ‰‹å‹•ç®¡ç†
- ğŸ”§ **ç¾ä»£åŒ–å·¥å…·æ•´åˆ**ï¼šåŸç”Ÿæ”¯æ´å·¥å…·å‘¼å«å’Œä¸²æµå›æ‡‰
- ğŸ”— **LCEL ç„¡ç¸«æ•´åˆ**ï¼šèˆ‡ LangChain Expression Language å®Œç¾çµåˆ
- ğŸ›¡ï¸ **çµ±ä¸€ä»‹é¢æŠ½è±¡**ï¼šRunnable ä»‹é¢æä¾›ä¸€è‡´çš„èª¿ç”¨é«”é©—

### ğŸ“ LangChain 0.3+ æœ€ä½³å¯¦è¸

1. **æ­£ç¢ºçš„åŒ¯å…¥è·¯å¾‘**ï¼š
   - âœ… `from langchain_core.messages import SystemMessage, HumanMessage, AIMessage`
   - âœ… `from langchain_core.prompts import ChatPromptTemplate`
   - âœ… `from langchain_core.tools import tool`

2. **ç¾ä»£åŒ–å‘¼å«æ–¹å¼**ï¼š
   - âœ… ä½¿ç”¨ `.invoke()`, `.ainvoke()`, `.stream()`, `.batch()`
   - âŒ é¿å…èˆŠç‰ˆ `chat(messages)` èªæ³•

3. **æ™ºèƒ½ Token ç®¡ç†**ï¼š
   - ä½¿ç”¨ `count_tokens_approximately()` é€²è¡Œç²¾ç¢ºè¨ˆæ•¸
   - æ¡ç”¨ `trim_messages()` é€²è¡Œæ™ºèƒ½ä¿®å‰ª
   - è¨­å®šåˆç†çš„ token é ç®—ï¼ˆç¾ä»£æ¨¡å‹æ”¯æ´æ›´å¤§ä¸Šä¸‹æ–‡ï¼‰

4. **å®‰å…¨æ€§è€ƒé‡**ï¼š
   - ğŸš« **çµ•ä¸ä½¿ç”¨ `eval()`** è™•ç†å·¥å…·è¼¸å…¥
   - âœ… ä½¿ç”¨ `ast.parse()` å’Œç™½åå–®é©—è­‰
   - âœ… å¯¦æ–½é©ç•¶çš„éŒ¯èª¤è™•ç†å’Œé‡è©¦æ©Ÿåˆ¶

5. **æ¨¡å‹é¸æ“‡ç­–ç•¥**ï¼š
   - ç°¡å–®ä»»å‹™ï¼š`gpt-4o-mini`ï¼ˆæ›¿ä»£ gpt-3.5-turboï¼‰
   - ä¸­ç­‰è¤‡é›œåº¦ï¼š`gpt-4o`
   - é«˜è¤‡é›œåº¦ï¼š`gpt-4` æˆ– o1 ç³»åˆ—

### ğŸ”„ é·ç§»æª¢æŸ¥æ¸…å–®

å¦‚æœä½ æ­£åœ¨å¾èˆŠç‰ˆæœ¬å‡ç´šï¼Œè«‹ç¢ºèªï¼š

- [ ] æ›´æ–°æ‰€æœ‰åŒ¯å…¥è·¯å¾‘ï¼ˆ`langchain.schema` â†’ `langchain_core.messages`ï¼‰
- [ ] æ¡ç”¨ç¾ä»£åŒ–çš„ ChatPromptTemplate èªæ³•ï¼ˆtuple æ ¼å¼ï¼‰
- [ ] æ›´æ–°å·¥å…·å®šç¾©ï¼ˆ`@tool` ä¾†è‡ª `langchain_core.tools`ï¼‰
- [ ] å¯¦æ–½å®‰å…¨çš„å·¥å…·åŸ·è¡Œé‚è¼¯
- [ ] ä½¿ç”¨å®˜æ–¹ token è¨ˆæ•¸å’Œå¿«å–æ©Ÿåˆ¶
- [ ] æ¸¬è©¦æ‰€æœ‰æ¨¡å‹æä¾›å•†çš„ SystemMessage æ”¯æ´

### ğŸš€ é€²éšæ‡‰ç”¨å»ºè­°

- **è¨˜æ†¶ç®¡ç†**ï¼šå„ªå…ˆä½¿ç”¨ `RunnableWithMessageHistory` è€Œéå‚³çµ± Memory
- **å·¥ä½œæµç·¨æ’**ï¼šè€ƒæ…®ä½¿ç”¨ LangGraph è™•ç†è¤‡é›œçš„å¤šæ­¥é©Ÿä»»å‹™
- **ç›£æ§å¯è§€æ¸¬æ€§**ï¼šæ•´åˆ LangSmith é€²è¡Œç”Ÿç”¢ç’°å¢ƒç›£æ§
- **çµæ§‹åŒ–è¼¸å‡º**ï¼šçµåˆ Pydantic è¼¸å‡ºè§£æå™¨ç¢ºä¿è³‡æ–™å“è³ª

---

::: tip ä¸‹ä¸€æ­¥
æŒæ¡äº† Chat Models çš„ç¾ä»£åŒ–ç”¨æ³•å¾Œï¼Œå»ºè­°æŒ‰ä»¥ä¸‹é †åºæ·±å…¥å­¸ç¿’ï¼š

1. [ç¬¬ä¸€å€‹æ‡‰ç”¨](/tutorials/first-app) - å‹•æ‰‹å»ºæ§‹å®Œæ•´çš„èŠå¤©æ‡‰ç”¨
2. [LCEL è¡¨é”å¼èªè¨€](/tutorials/lcel) - å­¸ç¿’ç¾ä»£éˆå¼çµ„åˆæ–¹å¼  
3. [çµæ§‹åŒ–è¼¸å‡ºè§£æ](/tutorials/output-parsers) - ç¢ºä¿ AI å›æ‡‰çš„è³‡æ–™å“è³ª
4. [è¨˜æ†¶æ©Ÿåˆ¶èˆ‡å°è©±ç®¡ç†](/tutorials/memory-systems) - å¯¦ç¾æ›´æ™ºèƒ½çš„å°è©±ç³»çµ±
5. [LangGraph å·¥ä½œæµæ¡†æ¶](/tutorials/langgraph) - å»ºæ§‹è¤‡é›œçš„å¤šä»£ç†ç³»çµ±
:::

::: warning âš ï¸ é‡è¦æé†’ï¼ˆå®‰å…¨æ€§å’Œåˆè¦æ€§ï¼‰

**å®‰å…¨æ€§**ï¼š
- **API é‡‘é‘°ç®¡ç†**ï¼šä½¿ç”¨ç’°å¢ƒè®Šæ•¸ï¼Œçµ•ä¸ç¡¬ç·¨ç¢¼é‡‘é‘°
- **è¼¸å…¥é©—è­‰**ï¼šå°æ‰€æœ‰ç”¨æˆ¶è¼¸å…¥é€²è¡Œé©ç•¶é©—è­‰å’Œéæ¿¾
- **å·¥å…·å®‰å…¨**ï¼šé¿å… `eval()` ç­‰å±éšªå‡½æ•¸ï¼Œå¯¦æ–½ç™½åå–®æ©Ÿåˆ¶

**åˆè¦æ€§**ï¼š
- **é€Ÿç‡é™åˆ¶**ï¼šéµå®ˆå„æ¨¡å‹æä¾›å•†çš„ API å‘¼å«é™åˆ¶  
- **å…§å®¹æ”¿ç­–**ï¼šç¢ºä¿è¼¸å…¥è¼¸å‡ºç¬¦åˆå„å¹³å°ä½¿ç”¨æ¢æ¬¾
- **éš±ç§ä¿è­·**ï¼šå¦¥å–„è™•ç†ç”¨æˆ¶æ•¸æ“šï¼Œéµå¾ªç›¸é—œéš±ç§æ³•è¦

**æˆæœ¬æ§åˆ¶**ï¼š
- **Token ç›£æ§**ï¼šå®šæœŸæª¢æŸ¥ API ä½¿ç”¨é‡å’Œè²»ç”¨
- **æ¨¡å‹é¸æ“‡**ï¼šæ ¹æ“šä»»å‹™è¤‡é›œåº¦é¸æ“‡æˆæœ¬æ•ˆç›Šæœ€ä½³çš„æ¨¡å‹
- **å¿«å–ç­–ç•¥**ï¼šåˆç†ä½¿ç”¨å¿«å–æ¸›å°‘ä¸å¿…è¦çš„ API å‘¼å«
:::